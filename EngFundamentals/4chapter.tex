\chapter{Linear Transformations and Operators}

\section{The Algebra of Linear Transformations}

\begin{theorem}
Let $V$ and $W$ be vector spaces over the field $F$.
Let $T$ and $U$ be two linear transformations from $V$ into $W$.
The function $(T+U)$ defined pointwise by
\begin{equation*}
(T + U) \left( \vecnot{v} \right) = T \vecnot{v} + U \vecnot{v}
\end{equation*}
is a linear transformation from $V$ into $W$.
Furthermore, if $s \in F$, the function $(sT)$ defined by
\begin{equation*}
(sT) \left( \vecnot{v} \right) = s \left( T \vecnot{v} \right)
\end{equation*}
is also a linear transformation from $V$ into $W$.
The set of all linear transformation from $V$ into $W$, together with the addition and scalar multiplication defined above, is a vector space over the field $F$.
\end{theorem}
\begin{proof}
Suppose that $T$ and $U$ are linear transformation from $V$ into $W$.
For $(T + U)$ defined above, we have
\begin{equation*}
\begin{split}
(T+U) \left( s \vecnot{v} + \vecnot{w} \right)
&= T \left( s \vecnot{v} + \vecnot{w} \right)
+ U \left( s \vecnot{v} + \vecnot{w} \right) \\
&= s \left( T \vecnot{v} \right) + T \vecnot{w}
+ s \left( U \vecnot{v} \right) + U \vecnot{w} \\
&= s \left( T \vecnot{v} + U \vecnot{v} \right)
+ \left( T \vecnot{w} + U \vecnot{w} \right) \\
&= s ( T + U ) \vecnot{v} + ( T + U ) \vecnot{w} ,
\end{split}
\end{equation*}
which shows that $(T+U)$ is a linear transformation.
Similarly, we have
\begin{equation*}
\begin{split}
(r T) \left( s \vecnot{v} + \vecnot{w} \right)
&= r \left( T \left( s \vecnot{v} + \vecnot{w} \right) \right) \\
&= r \left( s \left( T \vecnot{v} \right) + \left( T \vecnot{w} \right) \right) \\
&= r s \left( T \vecnot{v} \right) + r \left( T \vecnot{w} \right) \\
&= s \left( r \left( T \vecnot{v} \right) \right) + r T \left( \vecnot{w} \right) \\
&= s \left( \left( r T \right) \vecnot{v} \right) + \left( r T \right) \vecnot{w}
\end{split}
\end{equation*}
which shows that $(rT)$ is a linear transformation.

To verify that the set of linear transformations from $V$ into $W$ together with the operations defined above is a vector space, one must directly check the conditions of Definition~\ref{definition:VectorSpace}.
These are straightforward to verify, and we leave this exercise to the reader.
\end{proof}

We denote the space of linear transformations from $V$ into $W$ by $L(V,W)$.
Note that $L(V,W)$ is defined only when $V$ and $W$ are vector spaces over the same field.

\begin{fact}
Let $V$ be an $n$-dimensional vector space over the field $F$, and let $W$ be an $m$-dimensional vector space over $F$.
Then the space $L(V,W)$ is finite-dimensional and has dimension $mn$.
\end{fact}

\begin{theorem}
Let $V$, $W$, and $Z$ be vector spaces over a field $F$.
Let $T \in L(V,W)$ and $U \in L(W,Z)$.
Then the composed function $UT$ defined by $(UT) \left( \vecnot{v} \right) = U \left( T \left( \vecnot{v} \right) \right)$ is a linear transformation from $V$ into $Z$.
\end{theorem}
\begin{proof}
Let $\vecnot{v}_1, \vecnot{v}_2 \in V$ and $s \in F$.
Then, we have
\begin{equation*}
\begin{split}
(UT) \left( s \vecnot{v}_1 + \vecnot{v}_2 \right)
&= U \left( T \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) \right) \\
&= U \left( s T \vecnot{v}_1 + T \vecnot{v}_2 \right) \\
&= s U \left( T \vecnot{v}_1 \right) + U \left( T \vecnot{v}_2 \right) \\
&= s (UT) \left( \vecnot{v}_1 \right) + (UT) \left( \vecnot{v}_2 \right) ,
\end{split}
\end{equation*}
as desired.
\end{proof}

\begin{definition}
If $V$ is a vector space over the field $F$, a \defn{linear transform}{linear operator} on $V$ is a linear transformation from $V$ into $V$.
\end{definition}

\begin{definition}
An \defn{linear transform}{algebra} over a field $F$ is a vector space $V$ over $F$ that has a bilinear \defn{linear transform}{vector product} $``\cdot" \colon V \times V \to V$ satisfying
$(s\vecnot{u})\cdot(t\vecnot{v})=(st)(\vecnot{u}\cdot \vecnot{v})$ and
\[ (s\vecnot{u} + \vecnot{v})\cdot (t \vecnot{w} + \vecnot{x}) = st ( \vecnot{u}\cdot \vecnot{w}) + s (\vecnot{u}\cdot \vecnot{x}) + t (\vecnot{v}\cdot \vecnot{w}) + (\vecnot{v}\cdot \vecnot{x}), \]
for all $s,t\in F$ and $\vecnot{u},\vecnot{v},\vecnot{w},\vecnot{x} \in V$.
If $V$ is a Banach space and the norm of the vector product satisfies $\| \vecnot{u} \cdot \vecnot{v} \| \leq \|\vecnot{u}\| \vecnot{v}\|$, then it is called a \defn{linear transform}{Banach algebra}.
\end{definition}

\begin{example}
The set $L(V,V)$ of linear operators on $V$ forms an algebra when the vector product is defined by functional composition $UT(\vecnot{v})=U(T(\vecnot{v}))$.
If $V$ is a Banach space and $L(V,V)$ is equipped with the induced operator norm, then it forms a Banach algebra.
\end{example}

\begin{definition}
A linear transformation $T$ from $V$ into $W$ is called \defn{linear transform}{invertible} if there exists a function $U$ from $W$ to $V$ such that $UT$ is the identity function on $V$ and $TU$ is the identity function on $W$.
If $T$ is invertible, the function $U$ is unique and is denoted by $T^{-1}$.
Furthermore, $T$ is invertible if and only if
\begin{enumerate}
\item $T$ is one-to-one: $T \vecnot{v}_1 = T \vecnot{v}_2 \implies \vecnot{v}_1 = \vecnot{v}_2$
\item $T$ is onto: the range of $T$ is $W$.
\end{enumerate}
\end{definition}

\begin{example}
Consider the vector space $V$ of semi-infinite real sequences $\mathbb{R}^\omega$ where $\vecnot{v} = (v_1,v_2,v_3,\ldots)\in V$ with $v_n \in \mathbb{R}$ for $n\in \mathbb{N}$.
Let $L \colon V \rightarrow V$ be the left-shift linear transformation defined by
\[L \vecnot{v} = (v_2,v_3,v_4,\ldots) \]
and $R \colon V \rightarrow V$ be the right-shift linear transformation defined by
\[R \vecnot{v} = (0,v_1,v_2,\ldots). \]
Notice that $L$ is onto but not one-to-one and $R$ is one-to-one but not onto.
Therefore, neither transformation is invertible.
\end{example}

\begin{example}
Consider the normed vector space $V$ of semi-infinite real sequences $\mathbb{R}^\omega$ with the standard Schauder basis $\{\vecnot{e}_1,\vecnot{e}_2,\ldots\}$.
Let $T \colon V \rightarrow V$ be the linear transformation that satisfies $T \vecnot{e}_i = i^{-1} \vecnot{e}_i$ for $i=1,2,\ldots$.
Let the linear transformation $U \colon V \rightarrow V$ satisfy $U \vecnot{e}_i = i \vecnot{e}_i$ for $i=1,2,\ldots$.
It is easy to verify that $U = T^{-1}$ and $UT = TU = I$.
\end{example}
This example should actually bother you somewhat.
Since $T$ reduces vector components arbitrarily, its inverse must enlarge them arbitrarily.
Clearly, this is not a desirable property.
Later, we will introduce a norm for linear transforms which quantifies this problem.

\begin{theorem}
Let $V$ and $W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.
If $T$ is invertible, then the inverse function $T^{-1}$ is a linear transformation from $W$ onto $V$.
\end{theorem}
\begin{proof}
Let $\vecnot{w}_1$ and $\vecnot{w}_2$ be vectors in $W$ and let $s \in F$.
Define $\vecnot{v}_j = T^{-1} \vecnot{w}_j$, for $j =1,2$.
Since $T$ is a linear transformation, we have
\begin{equation*}
T \left( s \vecnot{v}_1 + \vecnot{v}_2 \right)
= s T \left( \vecnot{v}_1 \right) + T \left( \vecnot{v}_2 \right)
= s \vecnot{w}_1 + \vecnot{w}_2.
\end{equation*}
That is, $s \vecnot{v}_1 + \vecnot{v}_2$ is the unique vector in $V$ that maps to $s \vecnot{w}_1 + \vecnot{w}_2$ under $T$.
It follows that
\begin{equation*}
T^{-1} \left( s \vecnot{w}_1 + \vecnot{w}_2 \right)
= s \vecnot{v}_1 + \vecnot{v}_2
= s \left( T^{-1} \vecnot{w}_1 \right) + T^{-1} \vecnot{w}_2
\end{equation*}
and $T^{-1}$ is a linear transformation.
\end{proof}

A \defn{vector space}{homomorphism} is a mapping between algebraic structures which preserves all relevant structure.
An \defn{vector space}{isomorphism} is a homomorphism which is also invertible.
For vector spaces, the relevant structure is given by vector addition and scalar multiplication.
Since a linear transformation preserves both of these operation, it is also a \emph{vector space homomorphism}.
Likewise, an invertible linear transformation is a \emph{vector space isomorphism}.

\section{The Dual Space}

\begin{definition}
Let $V$ be a vector space.
The collection of all linear functionals on $V$, denoted $L(V,F)$, forms a vector space.
We also denote this space by $V^*$ and call it the \defn{vector space}{dual space} of $V$.
\end{definition}

The following theorem shows that, if $V$ is finite dimensional, then
\begin{equation*}
\dim V^* = \dim V.
\end{equation*}
In this case, one actually finds that $V$ is isomorphic to $V^*$.
Therefore, the two spaces can be identified with each other so that $V=V^*$ for finite dimensional $V$.

%If $\vecnot{v} \in V$, then we can define a functional $f_{\vecnot{v}}$ by
%\begin{equation*}
%f_{\vecnot{v}} \left( \vecnot{w} \right) = \left\langle \vecnot{w} | \vecnot{v} \right\rangle.
%\end{equation*}
%That is, the inner product with a fixed vector is a linear functional.
%Conversely, if $V$ is a Hilbert space, then any continuous linear functional can be expressed as an inner product.
%This result is known as the Riesz representation theorem.

\begin{theorem}
Let $V$ be a finite-dimensional vector space over the field $F$, and let $\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ be a basis for $V$.
There is a unique dual basis $\mathcal{B}^* = f_1, \ldots, f_n$ for $V^*$ such that $f_j \left( \vecnot{v}_i \right) = \delta_{ij}$.
For each linear functional on $V$, we have
\begin{equation*}
f = \sum_{i=1}^n f \left( \vecnot{v}_i \right) f_i
\end{equation*}
and for each vector $\vecnot{v}$ in $V$, we have
\begin{equation*}
\vecnot{v} = \sum_{i=1}^n f_i \left( \vecnot{v} \right) \vecnot{v}_i .
\end{equation*}
\end{theorem}
\begin{proof}
Let $\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ be a basis for $V$.
According to Theorem~\ref{theorem:UniqueLinearTransformation}, there is a unique linear functional $f_i$ on $V$ such that
\begin{equation*}
f_i \left( \vecnot{v}_j \right) = \delta_{ij} .
\end{equation*}
Thus, we obtain from $\mathcal{B}$ a set of $n$ distinct linear functionals $f_1, \ldots, f_n$ on $V$.
These functionals are linearly independent;
suppose that
\begin{equation*}
f = \sum_{i=1}^n s_i f_i ,
\end{equation*}
then
\begin{equation*}
f \left( \vecnot{v}_j \right) = \sum_{i=1}^n s_i f_i \left( \vecnot{v}_j \right)
= \sum_{i=1}^n s_i \delta_{ij} = s_j .
\end{equation*}
In particular, if $f$ is the zero functional, $f \left( \vecnot{v}_j \right) = 0$ for $j = 1, \ldots, n$ and hence the scalars $\{ s_j \}$ must all equal $0$.
It follows that the functionals $f_1, \ldots, f_n$ are linearly independent.
Since $\dim V^* = n$, we conclude that $\mathcal{B}^* = f_1, \ldots, f_n$ forms a basis for $V^*$, the \defn{vector space}{dual basis} of $\mathcal{B}$.

Next, we want to show that there is a unique basis which is dual to $\mathcal{B}$.
If $f$ is a linear functional on $V$, then $f$ is some linear combination of $f_1, \ldots, f_n$ with
\begin{equation*}
f = \sum_{i=1}^n s_i f_i .
\end{equation*}
Furthermore, by construction, we must have $s_j = f \left( \vecnot{v}_j \right)$ for $j = 1, \ldots, n$.
Similarly, if
\begin{equation*}
\vecnot{v} = \sum_{i=1}^n t_i \vecnot{v}_i .
\end{equation*}
is a vector in $V$, then
\begin{equation*}
f_j \left( \vecnot{v} \right) = \sum_{i=1}^n t_i f_j \left( \vecnot{v}_i \right)
= \sum_{i=1}^n t_i \delta_{ij} = t_j .
\end{equation*}
That is, the unique expression for $\vecnot{v}$ as a linear combination of $\vecnot{v}_1, \ldots, \vecnot{v}_n$ is
\begin{equation*}
\vecnot{v} = \sum_{i=1}^n f_i \left( \vecnot{v} \right) \vecnot{v}_i .
\end{equation*}
\end{proof}

One important use of the dual space is to define the transpose of a linear transform in a way that generalizes to infinite dimensional vector spaces.
Let $V,W$ be vector spaces over $F$ and $T \colon V \rightarrow W$ be a linear transform.
If $g \in W^*$ is a linear functional on W (i.e., $g \colon W \rightarrow F$), then $g(T\vecnot{v})\in V^*$ is a linear functional on $V$.
The \defn{linear transform}{transpose} of $T$ is the mapping $U \colon W^* \rightarrow V^*$ defined by $f(\vecnot{v})=g(T\vecnot{v})\in V^*$ for all $g\in W^*$.
If $V,W$ are finite-dimensional, then one can identify $V=V^*$ and $W=W^*$ via isomorphism and recover the standard transpose mapping $U \colon W \rightarrow V$ implied by the matrix transpose.

The details of this definition are not used in the remainder of these notes, but can be useful in understanding the subtleties of infinite dimensional spaces.
For infinite dimensional Hilbert spaces, we will see later that the definition again simplifies because one identify $V=V^*$ via isomorphism.
The interesting case that does not simplify is that of linear transforms between infinite dimensional Banach spaces.
 

%\begin{lemma}
%Let $V$ be a normed vector space and $V^*$ be its continuous dual space.
%If $f(\vecnot{v})=0$ for all $f \in \V^*$, then $\vecnot{v} = \vecnot{0}$.
%\end{lemma}
%\begin{proof}
%\end{proof}

\section{Operator Norms}

For any vector space of linear transforms, one can define a norm to get a normed vector space of linear transforms (e.g., consider the Frobenius norm of a matrix).
In constrast, an operator norm is defined for linear transforms between normed spaces and it is induced by the vector norms of the underlying spaces.
Intuitively, the induced operator norm is the largest factor by which a linear transform can increase the length of a vector.
This defines a simple ``worst-case" expansion for any linear transform.

\begin{definition}
Let $V$ and $W$ be two normed vector spaces and let $T \colon V \rightarrow W$ be a linear transformation.
The induced \defn{linear transform}{operator norm} of $T$ is defined to
\begin{equation*}
\left\| T \right\|
= \sup_{ \vecnot{v} \in V - \left\{ \vecnot{0} \right\} }
\frac{ \left\| T \vecnot{v} \right\| }{ \left\| \vecnot{v} \right\| }
= \sup_{\vecnot{v} \in V, \left\| \vecnot{v} \right\| = 1 }
\left\| T \vecnot{v} \right\| .
\end{equation*}
\end{definition}

A common question about the operator norm is, ``How do I know the two expressions give the same result?".
To see this, we can write
\begin{equation*}
\sup_{ \vecnot{v} \in V - \left\{ \vecnot{0} \right\} }
\frac{ \left\| T \vecnot{v} \right\| }{ \left\| \vecnot{v} \right\| }
= \sup_{ \vecnot{v} \in V - \left\{ \vecnot{0} \right\} }
\left\| T \frac{\vecnot{v}}{\| \vecnot{v} \|} \right\|
= \sup_{\vecnot{u} \in V, \left\| \vecnot{u} \right\| = 1 }
\left\| T \vecnot{u} \right\| .
\end{equation*}

Previously, we have seen that the set $L(V,W)$ of linear transformations from $V$ into $W$, with the standard addition and scalar multiplication, satisfies the conditions required to be a vector space.
Now, we have a norm for that vector space.
Interested readers should verify that the above definition satisfies the first two standard conditions required by a norm.
To verify the triangle inequality, we can write
\begin{align*}
\| T + U \| &= \sup_{\vecnot{v} \in V, \left\| \vecnot{v} \right\| = 1 }
\left\| (T+U) \vecnot{v} \right\| \\
&\leq  \sup_{\vecnot{v} \in V, \left\| \vecnot{v} \right\| = 1 }
\left(  \left\| T \vecnot{v} \right\| + \left\| U \vecnot{v} \right\| \right) \\
& \leq \sup_{\vecnot{v} \in V, \left\| \vecnot{v} \right\| = 1 }
\left\| T \vecnot{v} \right\| + \sup_{\vecnot{v} \in V, \left\| \vecnot{v} \right\| = 1 }
\left\| U \vecnot{v} \right\| \\
&= \|T\| + \|U\|.
\end{align*}

The induced operator norm also has another property that follows naturally from its definition.
Notice that 
\[ \| T \| = \sup_{ \vecnot{v} \in V - \left\{ \vecnot{0} \right\} }
\frac{ \left\| T \vecnot{v} \right\| }{ \left\| \vecnot{v} \right\| } \geq \frac{ \left\| T \vecnot{u} \right\| }{ \left\| \vecnot{u} \right\| } \]
for all non-zero $\vecnot{u} \in V$.
Checking the special case of $\vecnot{u}=\vecnot{0}$ separately, one can show the \textbf{induced operator-norm inequality} $\| T \vecnot{u} \| \leq \| T \| \| \vecnot{u} \|$ for all $\vecnot{u}\in V$.

For the space $L(V,V)$ of linear operators on $V$, a norm is called \textbf{submultiplicative} if $\| T U \| \leq \|T\| \|U\|$ for all $T,U \in L(V,V)$.
The induced operator-norm inequality shows that all induced operator norms are submultiplicative because
\begin{equation*}
\left\| U T \vecnot{v} \right\| \leq \left\| U \right\| \left\| T \vecnot{v} \right\| \leq \left\| U \right\| \left\| T \right\| \left\| \vecnot{v} \right\| .
\end{equation*}
This also defines a submultiplicative norm for the algebra of linear operators on $V$.

\subsection{Bounded Transformations}

\begin{definition}
If the norm of a linear transformation is finite, then the transformation is said to be \defn{linear transform}{bounded}.
\end{definition}

\begin{theorem}
A linear transformation $T \colon V \rightarrow W$ is bounded if and only if it is continuous.
\end{theorem}
\begin{proof}
Suppose that $T$ is bounded; that is, there exists $M$ such that  $\left\| T \vecnot{v} \right\| \leq M \left\| \vecnot{v} \right\|$ for all $\vecnot{v} \in V$.
Let $\vecnot{v}_1, \vecnot{v}_2, \ldots$ be a convergent sequence in $V$, then
\begin{equation*}
\left\| T \vecnot{v}_i - T \vecnot{v}_j \right\|
= \left\| T \left( \vecnot{v}_i - \vecnot{v}_j \right) \right\|
\leq  M \left\| \vecnot{v}_i - \vecnot{v}_j \right\| .
\end{equation*}
This implies that $T \vecnot{v}_1, T \vecnot{v}_2, \ldots$ is a convergent sequence in $W$, and $T$ is continuous.

Conversely, assume $T$ is continuous and notice that $T \vecnot{0} = \vecnot{0}$.
Therefore, for any $\epsilon > 0$, there is a $\delta>0$ such that $\| T \vecnot{v} \| < \epsilon$ for all $\| \vecnot{v} \| < \delta$.
%Then there is a $\delta > 0$ such that $\left\| T \vecnot{u} \right\| \leq 1$ for $\left\| \vecnot{u} \right\| \leq \delta$.
Since the norm of $\vecnot{u} = \frac{ \delta \vecnot{v} }{2 \left\| \vecnot{v} \right\| }$ is equal to $\delta/2$, we get
\begin{equation*}
\left\| T \vecnot{v} \right\|
= \left\| T \frac{ \delta \vecnot{v} }{2 \left\| \vecnot{v} \right\| } \right\|
\frac{ 2 \left\| \vecnot{v} \right\| }{ \delta }
< \frac{2 \epsilon}{ \delta } \left\| \vecnot{v} \right\|.
\end{equation*}
The value $M = \frac{2 \epsilon}{\delta}$ serves as an upper bound on $\|T\|$.
\end{proof}

Then, by showing that linear transformations over finite-dimensional spaces are continuous, one concludes that they are also bounded.
This is accomplished in the following theorem.

\begin{theorem}
Let $V$ and $W$ be normed vector spaces and let $T \colon V \rightarrow W$ be a linear transformation.
If $V$ is finite dimensional, then $T$ is continuous and bounded.
\end{theorem}

\begin{lemma}
Let $V$ be a finite-dimensional normed vector space, and let
\begin{equation*}
\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n
\end{equation*}
be a basis for $V$.
Then, for $\vecnot{v} \in V$, each coefficient $s_i$ in the expansion
\begin{equation*}
\vecnot{v} = s_1 \vecnot{v}_1 + \cdots + s_n \vecnot{v}_n
\end{equation*}
is a continuous linear function of $\vecnot{v}$.
Being continuous, it is also bounded, so there exists a constant $M$ such that $|s_i| \leq M \left\| \vecnot{v} \right\|$.
\end{lemma}
\begin{proof}[Proof of Lemma]
The linearity property is straightforward, its proof is omitted.
It will suffice to show that there is an $m > 0$ such that
\begin{equation} \label{equation:ContinuityFiniteDim}
m |s_i| \leq m \left( |s_1| + \cdots + |s_n| \right) \leq \left\| \vecnot{v} \right\| ,
\end{equation}
since \eqref{equation:ContinuityFiniteDim} implies that $|s_i| \leq m^{-1} \left\| \vecnot{v} \right\|$.
We first show that this holds for coefficients $\left\{ s_1, \ldots, s_n \right\}$ satisfying the condition $|s_1| + \cdots + |s_n| = 1$.
Let
\begin{equation*}
S = \left\{ (s_1, \ldots, s_n) \Big| \sum_{i=1}^n |s_i| = 1 \right\}.
\end{equation*}
This set is closed and bounded; it is therefore compact.
Define the function $f \colon S \rightarrow \RealNumbers$ by
\begin{equation*}
f(s_1, \ldots, s_n) = \left\| s_1 \vecnot{v}_1 + \cdots + s_n \vecnot{v}_n \right\| .
\end{equation*}
It can be shown that $f$ is continuous, and it is clear that $f > 0$ over $S$.
Let
\begin{equation*}
m = \min_{(s_1, \ldots, s_n) \in S} f(s_1, \ldots, s_n) .
\end{equation*}
Since $f$ is continuous and $S$ is compact, this minimum exists and is attained by some point $(s_1', \ldots, s_n') \in S$.
Note that $m > 0$ for otherwise $\vecnot{v}_1, \ldots, \vecnot{v}_n$ are linearly dependent, contradicting the fact that $\mathcal{B}$ is a basis.
Thus $m$ so defined satisfies \eqref{equation:ContinuityFiniteDim}.

For general sets of coefficients $\{ s_i \}$, let $c = |s_1| + \cdots + |s_n|$.
If $c = 0$, the result is trivial.
If $c > 0$, then write
\begin{equation*}
\begin{split}
\left\| s_1 \vecnot{v}_1 + \cdots + s_n \vecnot{v}_n \right\|
&= c \left\| \frac{s_1}{c} \vecnot{v}_1 + \cdots + \frac{s_n}{c} \vecnot{v}_n \right\| \\
&= c f \left( \frac{s_1}{c}, \ldots, \frac{s_n}{c} \right) \\
&\geq c m = m \left( |s_1| + \cdots + |s_n| \right) .
\end{split}
\end{equation*}
This is the desired result.
\end{proof}

We are now ready to prove the theorem.
\begin{proof}[Proof of Theorem]
Let $\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ be a basis for $V$.
Let $\vecnot{v} \in V$ be expressed in terms of this basis as
\begin{equation*}
\vecnot{v} = s_1 \vecnot{v}_1 + \cdots + s_n \vecnot{v}_n .
\end{equation*}
Let $C = \max_{1 \leq i \leq n} \left\| T \vecnot{v}_i \right\|$.
Then,
\begin{equation*}
\begin{split}
\left\| T \vecnot{v} \right\| &= \left\| T \left( s_1 \vecnot{v}_1 + \cdots + s_n \vecnot{v}_n \right) \right\| \\
& \leq |s_1| \left\| T \vecnot{v}_1 \right\| + \cdots + |s_n| \left\| T \vecnot{v}_n \right\| \\
&\leq C \left( |s_1| + \cdots + |s_n| \right) .
\end{split}
\end{equation*}
By the previous lemma, this implies that there exists an $M$ such that $|s_1| + \cdots + |s_n| \leq M \left\| \vecnot{v} \right\|$, so that
\begin{equation*}
\left\| T \vecnot{v} \right\| \leq C M \left\| \vecnot{v} \right\| .
\end{equation*}
\end{proof}



\subsection{The Neumann Expansion}

\begin{theorem}
Let $\| \cdot \|$ be a submultiplicative operator norm and $T \colon V \rightarrow V$ be a linear operator with $\left\| T \right\| < 1$.
Then, $(I-T)^{-1}$ exists and
\[ (I-T)^{-1} = \sum_{i=0}^{\infty} T^i. \]
\end{theorem}
\begin{proof}
First, we observe that the sequence
\[ A_n = \sum_{i=0}^{n-1} T^i.\]
is Cauchy.  This follows from the fact that, for $m<n$, we have
\[ \| A_n - A_m \| = \left\| \sum_{i=m}^{n-1} T^i \right\| \leq \sum_{i=m}^{n-1} \| T \|^i = \frac{\| T \|^m - \| T \|^n}{1 - \| T \|} \leq  \frac{\| T \|^m}{1 - \| T \|}. \]
Since this goes to zero as $m\rightarrow \infty$, we see that the limit $\lim_{n\rightarrow \infty} A_n$ exists.

Next, we observe that
\[ (I - T) \left( I + T + T^2 + \cdots + T^{n-1} \right) = I - T^n . \]
Since $\left\| T \right\| < 1$, we have $\lim_{k \rightarrow \infty} T^k = 0$ because $\left\| T^k \right\| \leq \left\| T \right\|^k \rightarrow 0$.
Taking the limit $n\rightarrow \infty$ of both sides gives
\[ (I - T) \sum_{i=0}^{\infty} T^i =  \lim_{n \rightarrow \infty} (I - T^n ) = I. \]
Likewise, reversing the order multiplication results in the same result.
This shows that $\sum_{i=0}^{\infty} T^i$ must be the inverse of $I -T$.
\end{proof}

If one only needs to show that $I - T$ is non-singular, then proof by contradiction is somewhat simpler.
Suppose $I - T$ is singular, then there exists a non-zero vector $\vecnot{v}$ such that $ \left( I - T \right) \vecnot{v} = \vecnot{0}$.
But, this implies that $\left\| \vecnot{v} \right\| = \left\| T \vecnot{v} \right\| \leq \left\| T \right\| \left\| \vecnot{v} \right\|$.
Since $\| \vecnot{v} \| \neq 0$, this gives the contradiction $\left\| T \right\| \geq 1$ and implies that $I-T$ is non-singular.

\subsection{Matrix Norms}

\begin{equation*}
\left\| A \right\|_{\infty} = \max_{\left\| \vecnot{v} \right\|_{\infty} = 1} \left\| A \vecnot{v} \right\|_{\infty} = \max_{i} \sum_{j} |a_{ij}|
\end{equation*}

\begin{equation*}
\left\| A \right\|_{1} = \max_{\left\| \vecnot{v} \right\|_{1} = 1} \left\| A \vecnot{v} \right\|_{1} = \max_{j} \sum_{i} |a_{ij}|
\end{equation*}

The 2-norm of a matrix can be found by solving
\begin{equation*}
\max_{\vecnot{v}^H \vecnot{v} = 1} \left\| A \vecnot{v} \right\|_{2}^{2}
= \vecnot{v}^H A^H A \vecnot{v} .
\end{equation*}
Using the Lagrange multiplier technique, one seeks to minimize
\begin{equation*}
J = \vecnot{v}^H A^H A \vecnot{v} - \lambda \vecnot{v}^H \vecnot{v} .
\end{equation*}
Taking the gradient with respect to $\vecnot{v}$ and equating the result to zero, we get
\begin{equation*}
A^H A \vecnot{v} = \lambda \vecnot{v} .
\end{equation*}
The corresponding $\vecnot{v}$ must be an eigenvector of the matrix $A^H A$.
Left multiplying this equation by $\vecnot{v}^H$ and using the fact that $\vecnot{v}^H \vecnot{v} = 1$, we obtain
\begin{equation*}
\vecnot{v}^H A^H A \vecnot{v} = \lambda \vecnot{v}^H \vecnot{v} = \lambda .
\end{equation*}
Since we are maximizing the left hand side of this equation, $\lambda$ must be the largest eigenvalue of $A^H A$.
For an $n \times n$ matrix $B$ with eigenvalues $\lambda_1, \ldots, \lambda_n$, the \defn{matrix}{spectral radius} $\rho (B)$ is defined by
\begin{equation*}
\rho (B) = \max_{i} | \lambda_i | .
\end{equation*}
The spectral radius of $B$ is the smallest radius of a circle centered at the origin that contains all the eigenvalues of $B$.
It follows that
\begin{equation*}
\| A \|_2 = \sqrt{ \rho (A^H A) } .
\end{equation*}
When $A$ is Hermitian, $\| A \|_2 = \rho(A)$.
The 2-norm is also called the \textbf{spectral norm}.

The \defn{matrix}{Frobenius norm} is given by
\begin{equation*}
\left\| A \right\|_F = \left( \sum_{i=1}^n \sum_{j=1}^n |a_{ij}|^2 \right)^{\frac{1}{2}} .
\end{equation*}
This norm is also called the \textbf{Euclidean norm}.
Note that $\left\| A \right\|_F^2 = \Trace (A^H A)$.


\section{Linear Functionals on Hilbert Spaces}

Let $V$ be an inner-product space, and let $\vecnot{v}$ be some fixed vector in $V$.
Define the function $f_{\vecnot{v}}$ from $V$ into $F$ by
\begin{equation*}
f_{\vecnot{v}} \left( \vecnot{w} \right)
= \left\langle \vecnot{w} | \vecnot{v} \right\rangle .
\end{equation*}
Clearly, $f_{\vecnot{v}}$ is a linear functional on $V$.
If $V$ is a Hilbert space, then every continuous linear functional on $V$ arises in this way from some vector $\vecnot{v}$.
This result is known as the \defn{inner-product space}{Riesz representation theorem}.

\begin{lemma} \label{lemma:DualSpaceSeparatesPoints}
If $\langle \vecnot{v} | \vecnot{w} \rangle = \langle \vecnot{u} | \vecnot{w} \rangle$ for all $\vecnot{w} \in V$, then $\vecnot{v} = \vecnot{u}$.
\end{lemma}
\begin{proof}
Then, $\langle \vecnot{v} - \vecnot{u} | \vecnot{w} \rangle = 0$ for all $\vecnot{w} \in V$.
Therefore, $\langle \vecnot{v} - \vecnot{u} | \vecnot{v} - \vecnot{u} \rangle = 0$ and this implies $\vecnot{v} - \vecnot{u} = \vecnot{0}$.
\end{proof}

\begin{theorem}[Riesz] \label{theorem:FunctionalInnerProduct}
Let $V$ be a Hilbert space and $f$ be a continuous linear functional on $V$.
Then, there exists a unique vector $\vecnot{v} \in V$ such that $f \left( \vecnot{w} \right) = \left\langle \vecnot{w} | \vecnot{v} \right\rangle$ for all $\vecnot{w} \in V$.
\end{theorem}
\begin{proof}
While the result holds in any Hilbert space, this proof assumes $V$ is separable for simplicity.
Therefore, we let $\vecnot{v}_1, \vecnot{v}_2, \ldots$ be a countable orthonormal basis for $V$.
We wish to find a candidate vector $\vecnot{v}$ for the inner product.

First, we note that $f$ is bounded and, as such, there exists $M$ such that $| f \left( \vecnot{x} \right) | \leq M \| \vecnot{x} \|$ for all $\vecnot{x} \in V$.
Let $\vecnot{x}_n = \sum_{i=1}^n \overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i$.
For any $n$, we have
\begin{equation*}
\begin{split}
M \left\| \vecnot{x}_n \right\| &\geq \left| f \left( \vecnot{x}_n \right) \right|
= \left| \sum_{i=1}^n \overline{ f \left( \vecnot{v}_i \right) } f(\vecnot{v}_i) \right|
= \sum_{i=1}^n \left| f \left( \vecnot{v}_i \right) \right|^2
= \sum_{i=1}^n f \left( \vecnot{v}_i \right)
\overline{ f \left( \vecnot{v}_i \right) } \\
&= \sum_{i=1}^n \left\langle \overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i
\Big| \overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i \right\rangle
= \sum_{i=1}^n \sum_{j=1}^n
\left\langle \overline{ f \left( \vecnot{v}_j \right) } \vecnot{v}_j
\Big| \overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i \right\rangle \\
&= \left\langle \sum_{j=1}^n
\overline{ f \left( \vecnot{v}_j \right) } \vecnot{v}_j
\bigg| \sum_{i=1}^n
\overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i \right\rangle
= \left\langle \vecnot{x}_n | \vecnot{x}_n \right\rangle
= \left\| \vecnot{x}_n \right\|^2 .
\end{split}
\end{equation*}
This implies that $\| \vecnot{x}_n \| \leq M$ for all $n$.
Hence, $\lim_{n \rightarrow \infty} \sum_{i=1}^n \left| f \left( \vecnot{v}_i \right) \right|^2$ is bounded and the vector
\begin{equation*}
\vecnot{v} = \sum_{i=1}^{\infty} \overline{ f \left( \vecnot{v}_i \right) } \vecnot{v}_i ,
\end{equation*}
is in $V$ because it is the limit point of a Cauchy sequence.
Let $f_{\vecnot{v}}$ be the functional defined by
\begin{equation*}
f_{\vecnot{v}} \left( \vecnot{w} \right) = \left\langle \vecnot{w} | \vecnot{v} \right\rangle .
\end{equation*}
By the Cauchy-Schwarz, we can verify that
\[ \| f_{\vecnot{v}} \| \triangleq \sup_{\vecnot{u}\in V - \{ \vecnot{0} \}} \frac{f_{\vecnot{v}} (\vecnot{u})}{\| \vecnot{u} \|} = \| \vecnot{v} \|. \]
Since $f$ is continuous, it follows that $\| f \| < \infty$ and $\| \vecnot{v} \| < \infty$.
%The Cauchy-Schwarz inequality shows that $\langle \vecnot{w} | \vecnot{v} \rangle \leq \| \vecnot{w} \| \| \vecnot{v} \|$ so that $f$ is continuous (and therefore linear) because $\| \vecnot{v} \| < \infty$.
Then, 
\begin{equation*}
f_{\vecnot{v}} \left( \vecnot{v}_j \right) = \left\langle \vecnot{v}_j \Big| 
\sum_{i=1}^\infty \overline{f \left( \vecnot{v}_i \right)} \vecnot{v}_i \right\rangle
= f \left( \vecnot{v}_j \right) .
\end{equation*}
Since this is true for each $\vecnot{v}_j$, it follows that $f = f_{\vecnot{v}}$.
Now, consider any $\vecnot{v}' \in V$ such that $\left\langle \vecnot{w} | \vecnot{v} \right\rangle = \left\langle \vecnot{w} | \vecnot{v}' \right\rangle$ for all $\vecnot{w} \in W$.
Applying Lemma~\ref{lemma:DualSpaceSeparatesPoints} shows that $\vecnot{v} = \vecnot{v}'$ and we conclude that $\vecnot{v}$ is unique.
\end{proof}

An important consequence of this theorem is that the continuous dual space $V^*$ of a Hilbert space $V$ is isometrically isomorphic to the original space $V$.
Let $R \colon V^* \rightarrow V$ be the implied Riesz mapping from continuous linear functionals on $V$ (i.e., $V^*$) to elements of $V$.
Then, $f(\vecnot{v}) = \langle \vecnot{v} | R( f ) \rangle$ for all $f \in V^*$.
The isomorphism can be shown by verifying that $R( s f_1 + f_2 ) = \overline{s} R(f_1) + R(f_2)$ and one finds that the mapping $R$ is conjugate linear.
The mapping is isometric because $\| f \| = \| R(f) \|$.
Based on this isomorphism, one can treat a Hilbert space as self-dual and assume without confusion that $V=V^*$.


\begin{theorem}
Let $V$ and $W$ be Hilbert spaces, and assume $T \colon V \rightarrow W$ is a continuous linear transformation.
Then, the \defn{inner-product space}{adjoint} is the unique linear transformation $T^*$ on $W$ such that
\begin{equation*}
\left\langle T \vecnot{v} | \vecnot{w} \right\rangle
= \left\langle \vecnot{v} | T^* \vecnot{w} \right\rangle
\end{equation*}
for all vectors $\vecnot{v} \in V$, $\vecnot{w} \in W$.
\end{theorem}
\begin{proof}
Let $\vecnot{w}$ be any vector in $W$.
Then $f(\vecnot{v}) = \left\langle T \vecnot{v} | \vecnot{w} \right\rangle$ is a continuous linear functional on $V$.
It follows from the Riesz representation theorem (Theorem~\ref{theorem:FunctionalInnerProduct}) that there exists a unique vector $\vecnot{v}' \in V$ such that $f(\vecnot{v}) = \left\langle T \vecnot{v} | \vecnot{w} \right\rangle = \left\langle \vecnot{v} | \vecnot{v}' \right\rangle$.
Of course, the vector $\vecnot{v}'$ depends on the choice of $\vecnot{w}$.
So, we define the adjoint mapping $T^* \colon W \rightarrow V$ to give the required $\vecnot{v}'$ for each $\vecnot{w}$.
In other words,
\begin{equation*}
\vecnot{v}' = T^* \vecnot{w} .
\end{equation*}
Next, we must verify that $T^*$ is a linear transformation.
Let $\vecnot{w}_1, \vecnot{w}_2$ be in $W$ and $s$ be a scalar.
For all $\vecnot{v} \in V$,
\begin{equation*}
\begin{split}
\left\langle \vecnot{v} | T^* \left( s \vecnot{w}_1 + \vecnot{w}_2 \right) \right\rangle
&= \left\langle T \vecnot{v} | \left( s \vecnot{w}_1 + \vecnot{w}_2 \right) \right\rangle \\
&= \bar{s} \left\langle T \vecnot{v} | \vecnot{w}_1 \right\rangle
+ \left\langle T \vecnot{v} | \vecnot{w}_2 \right\rangle \\
&= \bar{s} \left\langle \vecnot{v} | T^* \vecnot{w}_1 \right\rangle
+ \left\langle \vecnot{v} | T^* \vecnot{w}_2 \right\rangle \\
&= \left\langle \vecnot{v} | s T^* \vecnot{w}_1 \right\rangle
+ \left\langle \vecnot{v} | T^* \vecnot{w}_2 \right\rangle \\
&= \left\langle \vecnot{v} | s T^* \vecnot{w}_1 + T^* \vecnot{w}_2 \right\rangle .
\end{split}
\end{equation*}
Since this holds for all $\vecnot{v} \in V$, we gather from Lemma~\ref{lemma:DualSpaceSeparatesPoints} that $T^* \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) = s T^* \vecnot{v}_1 + T^* \vecnot{v}_2$.
Therefore, $T^*$ is linear.
The uniqueness of $T^*$ is inherited from Theorem~\ref{theorem:FunctionalInnerProduct} because, for each $\vecnot{w}\in W$, the vector $T^* \vecnot{w}$ is determined uniquely as the vector $\vecnot{v}'$ such that $\left\langle T \vecnot{v} | \vecnot{w} \right\rangle = \left\langle \vecnot{v} | \vecnot{v}' \right\rangle$ for all $\vecnot{v} \in V$.
\end{proof}

\begin{theorem}
Let $V$ be a finite-dimensional inner-product space and let
\begin{equation*}
\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n
\end{equation*}
be an orthonormal basis for $V$.
Let $T$ be a linear operator on $V$ and let $A$ be the matrix representation of $T$ in the ordered basis $\mathcal{B}$.
Then $A_{kj} = \left\langle T \vecnot{v}_j | \vecnot{v}_k \right\rangle$.
\end{theorem}
\begin{proof}
Since $\mathcal{B}$ is an orthonormal basis, we have
\begin{equation*}
\vecnot{v} = \sum_{k=1}^n \left\langle \vecnot{v} | \vecnot{v}_k \right\rangle \vecnot{v}_k .
\end{equation*}
The matrix $A$ is defined by
\begin{equation*}
T \vecnot{v}_j = \sum_{k=1}^n A_{kj} \vecnot{v}_k
\end{equation*}
and since
\begin{equation*}
T \vecnot{v}_j = \sum_{k=1}^n \left\langle T \vecnot{v}_j | \vecnot{v}_k \right\rangle \vecnot{v}_k ,
\end{equation*}
we conclude that $A_{kj} = \left\langle T \vecnot{v}_j | \vecnot{v}_k \right\rangle$.
\end{proof}

\begin{corollary}
Let $V$ be a finite-dimensional inner-product space, and let $T$ be a linear operator on $V$.
In any orthonormal basis for $V$, the matrix for $T^*$ is the conjugate transpose of the matrix of $T$.
\end{corollary}
\begin{proof}
Let $\mathcal{B} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ be an orthonormal basis for $V$, let $A = [T]_{\mathcal{B}}$ and $B = [T^*]_{\mathcal{B}}$.
According to the previous theorem,
\begin{align*}
A_{kj} &= \left\langle T \vecnot{v}_j | \vecnot{v}_k \right\rangle \\
B_{kj} &= \left\langle T^* \vecnot{v}_j | \vecnot{v}_k \right\rangle
\end{align*}
By the definition of $T^*$, we then have
\begin{equation*}
B_{kj} = \left\langle T^* \vecnot{v}_j | \vecnot{v}_k \right\rangle
= \overline{\left\langle \vecnot{v}_k | T^* \vecnot{v}_j \right\rangle}
= \overline{\left\langle T \vecnot{v}_k | \vecnot{v}_j \right\rangle}
= \overline{A_{jk}} .
\end{equation*}
\end{proof}

We note here that every linear operator on a finite-dimensional inner-product space $V$ has an adjoint on $V$.
However, in the infinite-dimensional case this is not necessarily true.
In any case, there exists at most one such operator $T^*$.


\section{Fundamental Subspaces}

There are four fundamental subspaces of a linear transformation $T \colon V \rightarrow W$ when $V$ and $W$ are Hilbert spaces.
We have already encountered two such spaces: The range of $T$ and the nullspace of $T$.
Recall that the range of a linear transformation $T$ is the set of all vectors $\vecnot{w} \in W$ such that $\vecnot{w} = T \vecnot{v}$ for some $\vecnot{v} \in V$.
The nullspace of $T$ consists of all vectors $\vecnot{v} \in V$ such that $T \vecnot{v} = \vecnot{0}$.

The other two fundamental subspaces of $T$ are the \textbf{range of the adjoint $T^*$}, denoted $R_{T^*}$ and the \textbf{nullspace of the adjoint $T^*$}, denoted $N_{T^*}$.
The various subspaces of the transformation $T \colon V \rightarrow W$ can be summarized as follows,
\begin{align*}
R_T &\subseteq W \\
N_T &\subseteq V \\
R_{T^*} &\subseteq V \\
N_{T^*} &\subseteq W .
\end{align*}

\begin{theorem}
Let $V$ and $W$ be Hilbert spaces and $T \colon V \rightarrow W$ be a bounded linear transformation from $V$ to $W$ such that $R_T$ and $R_{T^*}$ are both closed.
Then,
\begin{enumerate}
\item the range $R_T$ is the orthogonal complement of $N_{T^*}$, i.e., $\left[ R_T \right]^{\bot} = N_{T^*}$;
\item the nullspace $N_T$ is the orthogonal complement of $R_{T^*}$, i.e., $\left[ R_{T^*} \right]^{\bot} = N_T$ .
\end{enumerate}
Complementing these equalities, we get
\begin{align*}
\overline{R_T} &= R_T = \left[ N_{T^*} \right]^{\bot} \\
\overline{R_{T^*}} &= R_{T^*} = \left[ N_T \right]^{\bot} .
\end{align*}
\end{theorem}
\begin{proof}
Let $\vecnot{w} \in R_T$, then there exists $\vecnot{v} \in V$ such that $T \vecnot{v} = \vecnot{w}$.
Assume that $\vecnot{n} \in N_{T^*}$, then
\begin{equation*}
\left\langle \vecnot{w} | \vecnot{n} \right\rangle
= \left\langle T \vecnot{v} | \vecnot{n} \right\rangle
= \left\langle \vecnot{v} | T^* \vecnot{n} \right\rangle
= 0 .
\end{equation*}
That is, $\vecnot{w}$ and $\vecnot{n}$ are orthogonal vectors.
It follows that $N_{T^*} \subseteq \left[ R_T \right]^{\bot}$.
Now, let $\vecnot{w} \in \left[ R_T \right]^{\bot}$.
Then, for every $\vecnot{v} \in V$, we have
\begin{equation*}
\left\langle T \vecnot{v} | \vecnot{w} \right\rangle = 0.
\end{equation*}
This implies that $\left\langle \vecnot{v} | T^* \vecnot{w} \right\rangle = 0$, by the definition of the adjoint.
Since this is true for every $\vecnot{v} \in V$, we get $T^* \vecnot{w} = \vecnot{0}$, so $\vecnot{w} \in N_{T^*}$.
Then $\left[ R_T \right]^{\bot} \subseteq N_{T^*}$, which combined with our previous result yields $\left[ R_T \right]^{\bot} = N_{T^*}$.
Using a similar argument, one can show that $\left[ R_{T^*} \right]^{\bot} = N_T$.
\end{proof}


\section{Pseudoinverses}

\begin{theorem}
Let $V$ and $W$ be Hilbert spaces and $T$ be a bounded linear transformation from $V$ to $W$ where $R_T$ is closed.
The equation $T\vecnot{v} = \vecnot{w}$ has a solution if and only if $\left\langle \vecnot{w} | \vecnot{u} \right\rangle = 0$ for every vector $\vecnot{u} \in N_{T^*}$, i.e.,
\begin{equation*}
\vecnot{w} \in R_T \Leftrightarrow \vecnot{w} \perp N_{T^*} .
\end{equation*}
In matrix notation, $A \vecnot{v} = \vecnot{w}$ has a solution if and only if $\vecnot{u}^H \vecnot{w} = 0$ for every vector $\vecnot{u}$ such that $A^H \vecnot{u} = \vecnot{0}$.
\end{theorem}
\begin{proof}
Assume that $T \vecnot{v} = \vecnot{w}$, and let $\vecnot{u} \in N_{T^*}$.
Since $T$ is bounded, the adjoint $T^*$ exists and
\begin{equation*}
\left\langle \vecnot{w} | \vecnot{u} \right\rangle
= \left\langle T \vecnot{v} | \vecnot{u} \right\rangle
= \left\langle \vecnot{v} | T^* \vecnot{u} \right\rangle
= \left\langle \vecnot{v} | \vecnot{0} \right\rangle
= 0 .
\end{equation*}

To prove the reverse implication, suppose that $\left\langle \vecnot{w} | \vecnot{u} \right\rangle = 0$ when $\vecnot{u} \in N_{T^*}$ and $T \vecnot{v} = \vecnot{w}$ has no solution.
Since $\vecnot{w} \notin R_T$ and $R_T$ is closed, it follows that
\begin{equation*}
\vecnot{w}_o = \vecnot{w} - P_{R_T} \vecnot{w} = \vecnot{w} - \vecnot{w}_r
\neq \vecnot{0}.
\end{equation*}
%This implies that $\langle \vecnot{w} | \vecnot{w}_o \rangle = \left\langle T \vecnot{v} | \vecnot{w}_o \right\rangle = 0$ for all $\vecnot{v} \in V$, which implies that $\vecnot{w}_o \in N_{T^*}$.
But
\begin{equation*}
\left\langle \vecnot{w} | \vecnot{w}_o \right\rangle
= \left\langle \vecnot{w}_r + \vecnot{w}_o  | \vecnot{w}_o \right\rangle
= \left\langle \vecnot{w}_o | \vecnot{w}_o \right\rangle
> 0,
\end{equation*}
which contradicts the assumption that $\left\langle \vecnot{w} | \vecnot{u} \right\rangle = 0$ when $\vecnot{u} \in N_{T^*}$.
We must conclude that $T \vecnot{v} = \vecnot{w}$ has a solution.
\end{proof}

%\begin{remark}
%This result is related to Fredholm alternative because, for a given $\vecnot{w} \in W$ and $T\colon V \to W$ with closed range, exactly one of these statements is true:
%\begin{itemize}
%\item There exists $\vecnot{v} \in V$ such that $T \vecnot{v} = \vecnot{w}$, or
%\item There exists $\vecnot{u} \in W$ such that $T^* \vecnot{u} = \vecnot{0}$ and $\langle \vecnot{w} | \vecnot{u} \rangle \neq 0$.
%\end{itemize}
%\end{remark}

\begin{fact}
The solution to $T \vecnot{v} = \vecnot{w}$ (if it exists) is unique if and only if the only solution to $T \vecnot{v} = \vecnot{0}$ is $\vecnot{v} = \vecnot{0}$.
That is, if $N_T = \left\{ \vecnot{0} \right\}$.
\end{fact}

\subsection{Least Squares}

Let $T \colon V \rightarrow W$ be a bounded linear transformation.
If the equation $T \vecnot{v} = \vecnot{w}$ has no solution, then we can find a vector $\vecnot{v}$ that minimizes
\begin{equation*}
\left\| T \vecnot{v} - \vecnot{w} \right\|^2.
\end{equation*}

\begin{theorem}
The vector $\vecnot{v} \in V$ minimizes $\left\| T \vecnot{v} - \vecnot{w} \right\|$ if and only if
\begin{equation*}
T^* T \vecnot{v} = T^* \vecnot{w} .
\end{equation*}
\end{theorem}
\begin{proof}
Minimizing $\left\| \vecnot{w} - T \vecnot{v} \right\|$ is equivalent to minimizing $\left\| \vecnot{w} - \hat{\vecnot{w}} \right\|$, where $\hat{\vecnot{w}} = T\vecnot{v} \in R_T$.
By the projection theorem, we must have
\begin{equation*}
\vecnot{w} - \hat{\vecnot{w}} \in \left[ R_T \right]^{\bot} .
\end{equation*}
But this is equivalent to
\begin{equation*}
\vecnot{w} - \hat{\vecnot{w}} \in N_{T^*} .
\end{equation*}
That is, $T^* \left( \vecnot{w} - \hat{\vecnot{w}} \right) = \vecnot{0}$, or equivalently $T^* \vecnot{w} = T^* \hat{\vecnot{w}}$.
Conversely, if $T^*T \vecnot{v} = T^* \vecnot{w}$, then
\begin{equation*}
T^* \left( T \vecnot{v} - \vecnot{w} \right) = \vecnot{0},
\end{equation*}
so that $T \vecnot{v} - \vecnot{w} \in N_{T^*}$.
Hence, the error is orthogonal to the subspace $R_T$ and has minimal length by the projection theorem.
\end{proof}

\begin{corollary}
If $A$ is a matrix such that $A^H A$ is invertible, then the least-squares solution to $A \vecnot{v} = \vecnot{w}$ is
\begin{equation*}
\vecnot{v} = \left( A^H A \right)^{-1} A^H \vecnot{w}.
\end{equation*}
\end{corollary}

The matrix $\left( A^H A \right)^{-1} A^H$ is the left inverse of $A$ and is an example of a Moore-Penrose \defn{matrix}{pseudoinverse}.

\begin{theorem}
Suppose the vector $\hat{\vecnot{v}} \in V$ minimizes $\| \vecnot{v} \|$ over all $\vecnot{v}\in V$ satisfying $T \vecnot{v} = \vecnot{w}$.
Then, $\hat{\vecnot{v}} \in [N_T]^{\bot}$ and, if $R_{T^*}$ is closed, $\hat{\vecnot{v}} = T^* \vecnot{u}$ for some $\vecnot{u}\in W$.
\end{theorem}
\begin{proof}
Suppose $\hat{\vecnot{v}} \notin [N_T]^{\bot}$, then the orthogonal decomposition $V = [N_T]^{\bot} + N_T$ shows that the projection of $\hat{\vecnot{v}}$ onto $[N_T]^{\bot}$ has smaller norm but still satisfies $T \hat{\vecnot{v}} = \vecnot{w}$.
This gives a contradiction and shows that $\hat{\vecnot{v}} \in [N_T]^{\bot}$.
If $R_{T^*}$ is closed, then $R_{T^*} = [N_T]^{\bot}$ and $\hat{\vecnot{v}} = T^* \vecnot{u}$ for some $\vecnot{u}\in W$.
\end{proof}

\begin{corollary}
If $A$ is a matrix such that $A A^H$ is invertible, then the minimum-norm solution to $A \vecnot{v} = \vecnot{w}$ is
\begin{equation*}
\vecnot{v} = A^H \left( A A^H \right)^{-1} \vecnot{w}.
\end{equation*}
\end{corollary}
\begin{proof}
The theorem shows that $\vecnot{v} = A^H \vecnot{u}$ and $A \vecnot{v} = A A^H \vecnot{u} = \vecnot{w}$.
Since $A A^H$ is invertible, this gives $\vecnot{u} = (A A^H)^{-1} \vecnot{w}$ and computing $\vecnot{v}$ gives the desired result.
\end{proof}

The matrix $A^H \left( A A^H \right)^{-1}$ is the right inverse of $A$ and is another example of a Moore-Penrose \defn{matrix}{pseudoinverse}.

\begin{definition}
Let $T \colon V \rightarrow W$ be a bounded linear transformation, where $V$ and $W$ are Hilbert spaces, and $R_T$ is closed.
For each $\vecnot{w} \in W$, there is a unique vector $\hat{\vecnot{v}}$ of minimum norm in the set of vectors that minimize $\left\| T \vecnot{v} - \vecnot{w} \right\|$.
The \defn{linear transform}{pseudoinverse} $T^{\dagger}$ is the transformation mapping each $\vecnot{w}\in W$ to its unique $\hat{\vecnot{v}}$.
\end{definition}
