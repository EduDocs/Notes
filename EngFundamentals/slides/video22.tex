\documentclass[10pt,english,aspectratio=169]{beamer}
% Use notes or hide notes or show only notes or handout


\usetheme{default}

\usepackage{xstring}
\usepackage{pgfpages}
%\makeatletter
%\IfSubStr{\@classoptionslist}{handout}
%  {\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}
%  {}
%\makeatother

\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{stfloats}
\usepackage{bbm}
\usepackage{pdfpages}
\usepackage{framed}
\usepackage{tabularx}
\usepackage{scalerel}

\usepackage[most]{tcolorbox}
\tcbset{highlight math style={enhanced,
  colframe=white,colback=yellow!15,arc=8pt,boxrule=1pt,
  }}
  
\usepackage{tikz,pgf,pgfplots}
\usepackage{tikz-3dplot}
\usepackage{algorithm,algorithmic}
\usepgflibrary{shapes}
\usetikzlibrary{%
  arrows,%
  arrows.meta,
  backgrounds,
  shapes.misc,% wg. rounded rectangle
  shapes.arrows,%
  shapes,%
  calc,%
  chains,%
  matrix,%
  positioning,% wg. " of "
  scopes,%
  decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
  shadows,%
  backgrounds,%
  fit,%
  petri,%
  quotes
}

\tikzset{background rectangle/.style={
    fill=white,
  },
  use background/.style={    
    show background rectangle
  }
}

\setbeamersize{text margin left=10mm,text margin right=35mm}

\pgfplotsset{compat=1.12}

%\usetheme{Frankfurt}
%\usecolortheme{ldpc}
\useinnertheme{rounded}
\usecolortheme{whale}
\usecolortheme{orchid}

\newcommand{\ul}[1]{\underline{#1}}
\renewcommand{\Pr}{\mathbb{P}}

%% Setup slides and notes
\makeatletter
\IfSubStr{\@classoptionslist}{notes} { \IfSubStr{\@classoptionslist}{hide} {}{\IfSubStr{\@classoptionslist}{only} {}{\setbeameroption{show notes on second screen=right}}} }{}
\makeatother
%\setbeamertemplate{note page}{\pagecolor{yellow!5}\vfill\insertnote\vfill}

\newcommand{\getpdfpages}[2]{\begingroup
  \setbeamercolor{background canvas}{bg=}
  \addtocounter{framenumber}{1}
  \includepdf[pages={#1},%
  pagecommand={%
    \expandafter\def\expandafter\insertshorttitle\expandafter{%
      \insertshorttitle\hfill\insertframenumber\,/\,\inserttotalframenumber}}%
  ]{#2}
  \endgroup}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}

 \setbeamercolor{bibliography entry author}{fg=black}
 \setbeamercolor{bibliography entry title}{fg=black}
 \setbeamercolor{bibliography entry location}{fg=black}
 \setbeamercolor{bibliography entry note}{fg=black}
 
 \setbeamerfont{bibliography item}{size=\footnotesize}
 \setbeamerfont{bibliography entry author}{size=\footnotesize}
 \setbeamerfont{bibliography entry title}{size=\footnotesize}
 \setbeamerfont{bibliography entry location}{size=\footnotesize}
 \setbeamerfont{bibliography entry note}{size=\footnotesize}
 \setbeamertemplate{bibliography item}{\insertbiblabel}
 
\newlength\tikzwidth
\newlength\tikzheight


\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
%\newcommand{\expt}{\mbb{E}}
%\newcommand{\dd}{\mathrm{d}}
\input{macros}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\def\greencheck{{\color{green}\checkmark}}
\def\scalecheck{\resizebox{\widthof{\checkmark}*\ratio{\widthof{x}}{\widthof{\normalsize x}}}{!}{\checkmark}}
\def\xmark{\tikz [x=1.4ex,y=1.4ex,line width=.2ex, red] \draw (0,0) -- (1,1) (0,1) -- (1,0);}
\def\redx{{\color{red}\xmark}}

\renewcommand{\footnotesep}{-2pt}

\usetikzlibrary{3d}\makeatletter \tikzoption{canvas is xy plane at z}[]{%
   \def\tikz@plane@origin{\pgfpointxyz{0}{0}{#1}}%
   \def\tikz@plane@x{\pgfpointxyz{1}{0}{#1}}%
   \def\tikz@plane@y{\pgfpointxyz{0}{1}{#1}}%
   \tikz@canvas@is@plane } \makeatother

\begin{document}

\title{ECE 586: Vector Space Methods \\ Lecture 22: Alternating Projection}
\author{Henry D. Pfister \\ Duke University}
\date{}
%\date{August 20th, 2020}
%\maketitle

\setbeamertemplate{navigation symbols}{}

\begin{frame}[plain]
	\titlepage
	
	\note{
		\vspace{8mm}
		\begin{enumerate}
			\setlength\itemsep{3mm}
			\color{red}
			\item Welcome to the 11th video lecture for ECE 586, Vector Space Methods. \\[2mm]
			Today, we'll finish our discussion of subspaces and bases and then move on to linear transforms.
		\end{enumerate}
	}
\end{frame}

\addtocounter{framenumber}{-1}
\setbeamertemplate{navigation symbols}{\textcolor{blue}{\footnotesize \insertframenumber ~/ \inserttotalframenumber}}









\begin{frame}{Alternating Projection for Subspaces}

Let \textcolor{blue}{$P_{U}$ and $P_{W}$ be orthogonal projections onto closed subspaces $U$ and $W$} of a Hilbert space $V$. For an arbitrary $\vecnot v_{0}\in V$, what is the behavior of the vector sequence $\vecnot{v}_n$ generated by \textcolor{blue}{alternating projection}:
\begin{equation*}
\vecnot v_{n+1}=\begin{cases}
P_{U}\vecnot v_{n} & \text{if }n\text{ is even}\\
P_{W}\vecnot v_{n} & \text{if }n\text{ is odd}.
\end{cases}
\end{equation*}
Since $P_{U}\vecnot v=\vecnot v$ (resp. $P_{W}\vecnot v=\vecnot v$) if and only if $\vecnot v\in U$ (resp. $\vecnot v\in W$), it is easy to see that any vector $\vecnot v\in U\cap W$ is a fixed point of this recursion.

\vspace{3mm}

Letting $P_{U\cap W}$ denote the orthogonal projection onto $U\cap W$, one can show that the \textcolor{blue}{sequence $\vecnot v_{n}$ converges to $P_{U\cap W}\vecnot v_{0}$}.

\vspace{2mm}

\begin{theorem}
The sequence $\vecnot v_{n}$ converges to $P_{U\cap W}\vecnot v_{0}$, its projection onto $U\cap W$.
\end{theorem}

\begin{tikzpicture}[overlay,remember picture,shift={(11.8,0.5)},scale=0.6]
  \begin{axis}[font=\large,width=2.5in,height=2.5in,axis lines=middle,xmin=-3.5,xmax=3.5,ymin=-3.5,ymax=3.5,grid=both]
	\addplot[red,mark=none,domain=-3.5:3.5,very thick] {x};
	\addplot[blue,mark=none,domain=-3.5:3.5,very thick] {x/2};
	\node[label={180:{\textcolor{red}{$U$}}}] at (axis cs:2.95,2.95) {};
	\node[label={270:{\textcolor{blue}{$W$}}}] at (axis cs:2.75,1.375) {};
	\end{axis}
\end{tikzpicture}

\end{frame}

% Farkas Lemma Alternative?

\begin{frame}{4.6: Projection onto Hyperplane Subspaces}

\vspace{-5mm}

The orthogonal projection of $\vecnot v\in V$ onto a 1D subspace $W=\text{span}(\vecnot w)$ is \vspace{-0.5mm}
\[
P_{W}(\vecnot v)=\frac{\left\langle \vecnot v|\vecnot w\right\rangle }{\left\Vert \vecnot w\right\Vert ^{2}}\vecnot w.
\]
%\vspace{-0.5mm}
%\hrule
%\vspace{1.75mm}

\vspace{2mm}

\visible<2->{%
A subset of $V$ that \textcolor{blue}{satisfies a single linear equality} of the form $\left\langle \vecnot v|\vecnot w\right\rangle =0$
is a subspace $U \subset V$ with \textcolor{blue}{co-dimension one} (i.e., $\dim(U)=\dim(V)-1$).
Also, $U=W^\perp$ for a 1D subspace $W$ and $U$ is a \textcolor{blue}{hyperplane} containing $\vecnot{0}$. Thus, \vspace{0.5mm}
\[
P_{U}(\vecnot v)=P_{W^{\perp}}(\vecnot v)=\vecnot v-\frac{\left\langle \vecnot v|\vecnot w\right\rangle }{\left\Vert \vecnot w\right\Vert ^{2}}\vecnot w.
\]
%\vspace{-1mm}
%\hrule
%\vspace{1.75mm}
}

\tdplotsetmaincoords{105}{-30}
\begin{tikzpicture}[tdplot_main_coords,overlay,remember picture,shift={(10.9,-5.5)},scale=0.65]
%\begin{tikzpicture}[,font=\sffamily]
 \tdplotsetrotatedcoords{00}{30}{0}
 \begin{scope}[tdplot_rotated_coords]
  \begin{scope}[canvas is xy plane at z=0]
   \fill[red,fill opacity=0.3] (-2,-3) rectangle (2,3); 
   \draw[red,thick] (-2,0) -- (2,0);
   \draw[red,thick] (0,-3) -- (0,3);
   \path (-150:2) coordinate (H) (-1.5,0) coordinate(X);
   \pgflowlevelsynccm
   %\draw[very thick,-stealth,gray] (0,0) -- (-30:1.5);
  \end{scope} 
  \draw[stealth-] (H) -- ++ (-1,0,0.2) node[pos=1.6]{\textcolor{red}{$U=W^\perp$}};
  %\draw[stealth-] (X) -- ++ (0,1,0.2) node[pos=1.3]{$X$};
  \draw[very thick,-stealth] (0,0,0) coordinate (O) -- (0,0,3) node[right]{$\vecnot{w}$};
 \end{scope}
 \pgfmathsetmacro{\Radius}{1.5}
 \draw[-stealth]  (O)-- (2.5*\Radius,0,0) node[pos=1.15] {$x_1$};
 \draw[-stealth] (O) -- (0,3.5*\Radius,0) node[pos=1.15] {$x_2$};
 \draw[-stealth] (O) -- (0,0,2.5*\Radius) node[pos=1.05] {$x_3$};
\end{tikzpicture} 

\end{frame}


\begin{frame}{4.6: Projection onto Hyperplanes}

\vspace{-5mm}

\visible<1->{%
The linear equation $\langle\vecnot v|\vecnot w\rangle=c$ defines \textcolor{blue}{a shifted subspace $U+\vecnot v_{0}$} (for any $\vecnot v_{0} \in V$ satisfying $\langle\vecnot v_{0}|\vecnot w\rangle=c$) with co-dimension one (i.e., a \textcolor{blue}{hyperplane}): \vspace{-0.5mm}
\[
\langle\vecnot v|\vecnot w\rangle=\langle\vecnot u+\vecnot v_{0}|\vecnot w\rangle=\langle\vecnot u|\vecnot w\rangle+\langle\vecnot v_{0}|\vecnot w\rangle=0+c=c.
\]
%\vspace{-2.5mm}
%\hrule
%\vspace{1.75mm}
}

\vspace{-2mm}

\visible<2->{%
One can project onto $U+\vecnot{v_{0}}$ by shifting, projecting, and shifting back: \vspace{-0.75mm}
\begin{align*}
P_{U+\vecnot v_{0}}(\vecnot v)&=\left((\vecnot v-\vecnot v_{0})-\frac{\left\langle \vecnot v-\vecnot v_{0}|\vecnot w\right\rangle }{\left\Vert \vecnot w\right\Vert ^{2}}\vecnot w\right)+\vecnot v_{0}\\
&=\vecnot v-\frac{\left\langle \vecnot v|\vecnot w\right\rangle -c}{\left\Vert \vecnot w\right\Vert ^{2}}\vecnot w,
\end{align*}
}
\vspace{-10mm}

\tdplotsetmaincoords{105}{-30}
\begin{tikzpicture}[tdplot_main_coords,overlay,remember picture,shift={(12.1,-3.5)},scale=0.65]
%\begin{tikzpicture}[,font=\sffamily]
 \tdplotsetrotatedcoords{00}{30}{0}
 \begin{scope}[tdplot_rotated_coords]
  \begin{scope}[canvas is xy plane at z=1.4]
   \fill[red,fill opacity=0.3] (-2,-3) rectangle (2,3); 
   \draw[red,thick] (-2,0) -- (2,0);
   \draw[red,thick] (0,-3) -- (0,3);
   \path (-150:2) coordinate (H) (-1.5,0) coordinate(X);
   \pgflowlevelsynccm
   %\draw[very thick,-stealth,gray] (0,0) -- (-30:1.5);
  \end{scope} 
  \draw[stealth-] (H) -- ++ (-1,0,0.2) node[pos=1.6]{\textcolor{red}{$U\!+\!\frac{1}{2}\vecnot{w}$}};
  %\draw[stealth-] (X) -- ++ (0,1,0.2) node[pos=1.3]{$X$};
  \draw[very thick,-stealth] (0,0,0) coordinate (O) -- (0,0,3) node[right]{$\vecnot{w}$};
 \end{scope}
 \pgfmathsetmacro{\Radius}{1.5}
 \draw[-stealth]  (O)-- (2.5*\Radius,0,0) node[pos=1.15] {$x_1$};
 \draw[-stealth] (O) -- (0,3.5*\Radius,0) node[pos=1.15] {$x_2$};
 \draw[-stealth] (O) -- (0,0,2.5*\Radius) node[pos=1.05] {$x_3$};
\end{tikzpicture} 

\end{frame}


\begin{frame}{4.6: Solving Linear Equations via Alternating Projection}

\vspace{-10mm}

Let $A\in\mathbb{R}^{m\times n}$ and $\vecnot b\in\mathbb{R}^{m}$ be define a set of $m$ linear equations in $n$ variables \textcolor{blue}{with at least one solution}.

\vspace{3mm}

The goal is to use alternating projection find a solution $\vecnot x^{*}$ such that $A\vecnot x^{*}=\vecnot b$. If $\vecnot b=\vecnot 0$, then the set of solutions is a subspace equal to the null space of $A$,
\[
\mathcal{N}(A)=\left\{ \vecnot x\in\mathbb{R}^{n}\,|\,A\vecnot x=\vecnot 0\right\} =\bigcap_{i=1}^{m}\left\{ \vecnot x\in\mathbb{R}^{n}\,|\,{\textstyle \sum_{j=0}^{n}}a_{i,j}x_{j}=b_i = 0\right\} .
\]
The result follows because $\mathcal{N}(A)$ is the intersection of $m$ hyperplane subspaces (i.e., subspaces of dimension $n-1$). But, what if $\vecnot b\neq\vecnot 0$?

\begin{tikzpicture}[overlay,remember picture,shift={(11.0,-1.75)},scale=0.75]
  \begin{axis}[font=\large,width=2.5in,height=2.5in,axis lines=middle,xmin=-3.5,xmax=3.5,ymin=-3.5,ymax=3.5,grid=both]
	\addplot[red,mark=none,domain=-3.5:3.5,very thick] {x/2};
	\addplot[blue,mark=none,domain=-3.5:3.5,very thick] {1.5*x};
	\end{axis}
\end{tikzpicture}

\end{frame}

\begin{frame}{4.6: Kaczmarz's Algorithm}

The idea is to \textcolor{blue}{iteratively project a candidate vector onto linear equality constraints}. For a matrix $A\in\mathbb{R}^{m\times n}$ and vector $\vecnot b\in\mathbb{R}^{m}$, the algorithm starts from $\vecnot{v}_0 = \vecnot{0}$ and defines $\vecnot v_{i+1}$ to be the projection of $\vecnot v_{i}$ onto the set \vspace{-0.5mm}
\[
W_{i}=\left\{ \vecnot v\in\mathbb{R}^{n}\,\middle|\:\sum_{k=1}^{n}a_{\sigma(i),k}v_{k}=b_{\sigma(i)}\right\} ,
\]
where $\sigma(i)=(i\bmod m)+1$.
\vspace{3mm}

\hrule

\vspace{3mm}

Using the previously derived projection formula, this gives  \vspace{-0.5mm}
\begin{equation*}
\vecnot v_{i+1}= (1-s) \vecnot{v}_i + s \, P_{W_{\sigma(i)}} (\vecnot{v}_i) = \vecnot v_{i}- s \frac{\langle\vecnot v_{i}|\vecnot a_{\sigma(i)}\rangle-b_{\sigma(i)}}{\text{\ensuremath{\|}}\vecnot a_{\sigma(i)}\|^{2}}\vecnot a_{\sigma(i)},
\end{equation*}
where \textcolor{blue}{$s\in(0,1]$ is the step-size} and $\vecnot a_{j}$ is the $j$-th row of the matrix $A$. 

\vspace{5mm}
Note: The true projection uses $s=1$ but $s<1$ may work better if $\vecnot{b} \notin \mathcal{R}(A)$. 
\end{frame}

\begin{frame}{4.6: Alternating Projection onto Convex Sets}

Let $C_{1},C_{2},\ldots,C_{m}$ be closed convex subsets in a Hilbert space $V$.
The \textcolor{blue}{alternating projection algorithm finds a point in their intersection}. Starting from any $\vecnot v_{0}\in V$, the alternating projection algorithm computes 
\begin{align*}
\vecnot v_{i+1} & = (1-s) \vecnot{v}_i + s\, P_{C_{\sigma(i)}}(\vecnot v_{i}),
\end{align*}
where $\sigma(i)=(i\bmod m)+1$ and $s\in(0,1]$.

\vspace{4mm}

\begin{block}{Remark}
The intersection of convex sets is convex, so $C \triangleq \cap_{i=1}^{m}C_{i}$ is convex set.
Ideally, alternating projection would give $\vecnot{v}_i \to P_C (\vecnot{v}_0)$ but it does not :-(
\end{block}

\begin{theorem}[Bregman]
For finite-dimensional $V$, there is some $\vecnot v\in\cap_{i=1}^{m}C_{i}$ such that $\vecnot{v}_i \to \vecnot{v}$.
\end{theorem}

\vspace{-1.1in}
\hspace*{4.6in}
\includegraphics[width=1.25in]{figures/ch4_pocs}
%\vspace{0.5in}

\end{frame}

\begin{frame}{4.6: Orthogonal Projection onto Half Spaces}

\vspace{2mm}

For $\vecnot{w} \in V$, let $H = \{ \vecnot{v} \in V \,|\, \langle\vecnot v|\vecnot w\rangle\geq c \}$ be a closed convex \textcolor{blue}{half space}.

\begin{itemize}
\item For $\vecnot v\in H$, the projection satisfies $P_{H}(\vecnot v)=\vecnot v$ 
\item For $\vecnot v\notin H$, the projection satisfies $P_{H}(\vecnot v)=P_{U+\vecnot v_{0}}(\vecnot v)$ because \\ the closest point in $H$ achieves the inequality with equality
\end{itemize}

\vspace{2mm}

It follows that \vspace{-2.5mm}
\begin{equation*}
P_{H}(\vecnot v)=\begin{cases}
\vecnot v & \text{if }\langle\vecnot v|\vecnot w\rangle\geq c\\
\vecnot v-\frac{\left\langle \vecnot v|\vecnot w\right\rangle -c}{\left\Vert \vecnot w\right\Vert ^{2}}\vecnot w & \text{if }\langle\vecnot v|\vecnot w\rangle<c
\end{cases}\label{eq:proj_ineq}
\end{equation*}

\vspace{1mm}
\hrule
\vspace{4mm}
Thus, alternating projection can find a feasible vector $\vecnot{x}\in \mathbb{R}^3$ satisfying \vspace{-1mm}
{\small
\begin{align*}
2x_{1}-x_{2}+x_{3} & \geq-1\\
x_{1}+2x_{3} & \geq2\\
-7x_{1}+4x_{2}-6x_{3} & \geq1\\
-3x_{1}+x_{2}-2x_{3} & \geq0
\end{align*}
}
\vspace{-3mm}

\vspace{-1.1in}
\hspace*{3.7in}
\includegraphics[width=2.2in]{figures/ch4_halfspace}
%\vspace{0.5in}

\end{frame}

\begin{frame} \frametitle{Next Steps}

\begin{itemize}
\setlength\itemsep{5mm}
\item To continue studying after this video -- \vspace{2mm}

\begin{itemize}
 \setlength\itemsep{3mm}
 
 \item Try the required reading:  Course Notes EF 4.6
 
 \item Look at the Mini-Project Handout on Alternating Projection

 \item Also, look at the related problems in Assignments 8 and 9
\end{itemize}
\end{itemize}

\note{
	\vspace{8mm}
	\begin{enumerate}
		\setlength\itemsep{3mm}
		\color{red}
		\item Here are some options to continue learning this material. (read) \\ [2mm]  That's it for today.  So, I'll see you next time.
	\end{enumerate}
}

\end{frame}

\end{document}


\begin{frame}{Unconstrained Linear Optimization}

\begin{minipage}{0.45\textwidth}
Consider the unconstrained linear optimization problem:
\[ \min_{\vecnot{x} \in \mathbb{R}^n} \vecnot{c}^T \vecnot{x} = \begin{cases} 0 & \text{if } \vecnot{c} = \vecnot{0} \\ -\infty & \text{otherwise}. \end{cases} \]  

Figure shows labeled level sets of $\vecnot{c}^T \vecnot{x}$ for $n=2$ and $\color{red} \vecnot{c}=(1,2)$.

\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{tikzpicture}[scale=0.45]

    \pgfmathtruncatemacro{\xa}{-6}
    \pgfmathtruncatemacro{\xb}{6}
    \pgfmathtruncatemacro{\ya}{-6}
    \pgfmathtruncatemacro{\yb}{6}
    \draw[gray!25, thin, step=1] (\xa,\ya) grid (\xb,\yb);
    \draw[very thick,-latex] (\xa,0) -- (\xb,0) node[above] {$x_1$};
    \draw[very thick,-latex] (0,\ya) -- (0,\yb) node[above] {$x_2$};

    \foreach \x in {\xa,...,\xb} \draw (\x,0.05) -- (\x,-0.05) node[below] {\ifthenelse{\x=0}{}{\tiny\x}};
    \foreach \y in {\ya,...,\yb} \draw (-0.05,\y) -- (0.05,\y) node[right=-0.5mm] {\ifthenelse{\y=0}{}{\tiny\y}};

    \pgfmathtruncatemacro{\vx}{1}
    \pgfmathtruncatemacro{\vy}{2}
    \draw[red,thick,-latex] (0,0) -- node[below right ,pos=0.5] {$\vecnot{c}$} (\vx,\vy);
	\begin{scope}
        \clip(\xa,\ya) rectangle (\xb,\yb);
		\foreach \i in {-4,...,4}
		    \draw[red,dashed] (\vx*\i+5*\vy,\vy*\i-5*\vx) -- node [red,pos=0.515,above=0.75mm] {\small\pgfmathparse{(\vx*\vx+\vy*\vy)*\i}\pgfmathprintnumber{\pgfmathresult}} (\vx*\i-5*\vy,\vy*\i+5*\vx);
	\end{scope}
		    
\end{tikzpicture}
\end{minipage}

\end{frame}

\begin{frame}{Constrained Linear Optimization}

\begin{minipage}{0.45\textwidth}
Now, consider the \textcolor{blue}{constrained} linear optimization problem:
\[ \min_{\vecnot{x} \in \mathcal{D}} \vecnot{c}^T \vecnot{x}. \]  

Figure shows $\color{green!40!black} \mathcal{D}$ with level sets of $\vecnot{c}^T \vecnot{x}$ for $n=2$ and $\color{red} \vecnot{c}=(1,2)$.

\vspace{3mm}

\visible<2->{ $\circ\,$~Optimal point $\color{blue} \vecnot{x}^*$ can be found using gradient descent from any initial feasible point.}

\vspace{2mm}

\visible<3->{ $\circ\,$~Negative gradient canceled by constraint normal to give feasible descent direction.}

\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{tikzpicture}[scale=0.45]
    \pgfmathtruncatemacro{\xa}{-6}
    \pgfmathtruncatemacro{\xb}{6}
    \pgfmathtruncatemacro{\ya}{-6}
    \pgfmathtruncatemacro{\yb}{6}
    \draw[gray!25, thin, step=1] (\xa,\ya) grid (\xb,\yb);
    \draw[very thick,-latex] (\xa,0) -- (\xb,0) node[above] {$x_1$};
    \draw[very thick,->] (0,\ya) -- (0,\yb) node[above] {$x_2$};

    \foreach \x in {\xa,...,\xb} \draw (\x,0.05) -- (\x,-0.05) node[below] {\ifthenelse{\x=0}{}{\tiny\x}};
    \foreach \y in {\ya,...,\yb} \draw (-0.05,\y) -- (0.05,\y) node[right=-0.5mm] {\ifthenelse{\y=0}{}{\tiny\y}};

    \pgfmathtruncatemacro{\vx}{1}
    \pgfmathtruncatemacro{\vy}{2}
    \draw[red,thick,-latex] (0,0) -- node[below right ,pos=0.5] {$\vecnot{c}$} (\vx,\vy);
	\begin{scope}
        \clip(\xa,\ya) rectangle (\xb,\yb);
		\foreach \i in {-4,...,4}
		    \draw[red,dashed] (\vx*\i+5*\vy,\vy*\i-5*\vx) -- node [red,pos=0.515,above=0.75mm] {\pgfmathparse{(\vx*\vx+\vy*\vy)*\i}\pgfmathprintnumber{\pgfmathresult}} (\vx*\i-5*\vy,\vy*\i+5*\vx);
	\end{scope}

	\fill[green!50!black,opacity=0.25] (15/3,10/3) -- (1/3,-6/3) -- (13/3,-15/3) -- cycle;
	\draw (4.2,1.5) node[green!40!black] {$\mathcal{D}$};
	\only<2->{%
	  \draw (13/3,-15/3) node[circle,fill=blue,inner sep=1pt,node contents={}];
	  \draw (13/3,-15/3) node[below,blue] {$\vecnot{x}^*$};
	}
    \only<3->{%
      \draw[red,thick,-latex] (3,-4) -- (3-\vx/2,-4-\vy/2);
      \draw[green!40!black,thick,->] (3,-4) -- (3+3/5,-4+3/4);
      \draw[blue,thick,-latex] (3,-4) -- (3+2/3,-4-2/4);
    }
		
\end{tikzpicture}
\end{minipage}

\end{frame}


\begin{frame}{5.4: Constrained Non-Linear Optimization}

\iffalse
Lagrangian optimization is an indispensable tool in engineering and physics that allows one to solve constrained non-linear optimization problems.

\vspace{4mm}
\hrule
\vspace{4mm}
\fi

Consider a constrained non-linear optimization problem over $\mathcal{D} \subseteq \mathbb{R}^n$  in the following \textcolor{blue}{standard form}.
Let $f_i \colon \mathcal{D} \rightarrow \mathbb{R}$ and $h_j \colon \mathcal{D} \rightarrow \mathbb{R}$ be real functionals for $i=0,1,\ldots,m$ and $j=1,2,\ldots,p$.
Then, we write
\begin{align*}
\mathrm{minimize} \quad & f_0 (\vecnot{x}) \\
\mathrm{subject\,to} \quad & f_i (\vecnot{x}) \leq 0, \quad i=1,2,\ldots,m \\
& h_j (\vecnot{x}) = 0, \quad j=1,2,\ldots,p.
\end{align*}

\hrule

\begin{itemize}
\item<2-> the function $f_0$ is called the \textcolor{blue}{objective function}
\item<3-> the functions $f_1,\ldots,f_m$ define \textcolor{blue}{inequality constraints}
\item<4-> the functions $h_1,\ldots,h_p$ define \textcolor{blue}{equality constraints}
\item<5-> \textcolor{blue}{feasible} points in $\mathcal{F} \triangleq \{ \vecnot{x}\in \mathcal{D} \, | \,  f_i (\vecnot{x}) \leq 0, i\in [m], h_j (\vecnot{x}) = 0, j\in[p] \}$ satisfy all constraints and the problem is feasible if $\mathcal{F} \neq \emptyset$.
\item<6-> the \textcolor{blue}{optimal value} is $p^* \triangleq \inf \left\{ f_0 (\vecnot{x}) \, | \, \vecnot{x} \in \mathcal{F} \right\}$
\item<7-> problem called \textcolor{blue}{convex} if all $f_i$ convex and all $h_j$ affine: $h_j (\vecnot{x}) \!=\! \vecnot{a}_j^T \vecnot{x} \!-\! b_j \!\!\!$ 
\end{itemize}

\end{frame}

\begin{frame}{General Example}

\vspace{-2mm}

  \begin{center}
    \includegraphics[width=65mm]{opt_fig}
  \end{center}

\vspace{-2mm}

Contour plot of $f_0 (x_1,x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 - x_1 x_2 /2$ whose minimum occurs at $(4/3,4/3)$ (i.e., center of blue ellipse).  The red line shows the inequality constraint $f_1 (x_1,x_2)= 1.85 + (x_1 - 2.25)^2 / 2 - x_2 \leq 0$

\end{frame}

\begin{frame}{Linear Programs}

The optimization of a linear function with arbitrary affine equality and \\ inequality constraints is called a \textcolor{blue}{linear program (LP)}.

\vspace{3mm}

LPs have many equivalent forms because:
\begin{itemize}
\item $x_1 \!=\! 0$ is the same as $(x_1 \leq 0) \wedge (x_1\geq 0)$
\item $x_1 \leq 0$ is the same as $(x_1 + x_2 = 0) \wedge x_2 \geq 0$ for slack variable $x_2$
\item negation swaps: $\min \& \max$ for objective and $\geq \& \leq$ for constraints
\end{itemize}  

\begin{definition}
Any LP can be transformed into one of the standard $\min$ forms: \\[1mm]
\hrule \vspace{0mm}
\begin{minipage}{0.49\textwidth}
\vspace{-2mm}
\begin{align*}
\mathrm{minimize} \quad & \vecnot{c}^T \vecnot{x} \\
\mathrm{subject\,to} \quad & A \vecnot{x} = \vecnot{b} \\
& \vecnot{x} \succeq \vecnot{0}
\end{align*}
\end{minipage}
\vrule
\begin{minipage}{0.49\textwidth}
\vspace{-2mm}
\begin{align*}
\mathrm{minimize} \quad & \vecnot{c}^T \vecnot{x} \\
\mathrm{subject\,to} \quad & A \vecnot{x} \succeq \vecnot{b} \\
& \vecnot{x} \succeq \vecnot{0}
\end{align*}
\end{minipage}
\end{definition}

\end{frame}


\begin{frame}{Langrangian Formulation}

Used to transform from constrained to unconstrained optimization

\begin{definition}
For the standard optimization, the \textcolor{blue}{Lagrangian} $L \colon \mathcal{D} \times \mathbb{R}^m \times \mathbb{R}^p \rightarrow \mathbb{R}$ is \vspace{-2mm}
\[ L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) = f_0(\vecnot{x}) + \sum_{i=1}^m \lambda_i f_i(\vecnot{x}) + \sum_{j=1}^p \nu_j h_j(\vecnot{x}), \vspace{-1.5mm} \]
where the \textcolor{blue}{Lagrange multipliers} $\lambda_i$ and $\nu_j$ define penalties associated with violating the $i$-th inequality and $j$-th equality constraints, respectively.
\end{definition}

\begin{theorem}[Karush-Kuhn-Tucker]<2->
Assume the functions $f_i$ and $h_j$ are continuously differentiable and let $A = \{ i\in [m] \, | \, f_i (\vecnot{x}^*)=0 \}$ be the set of active constraints at $\vecnot{x}^*$.
Then, $\vecnot{x}^*$ is locally optimal only if $\vecnot{\lambda}^* \geq 0$ and $\vecnot{\nu}^*$ exist such that \vspace{-1.5mm}
\begin{align*}
\nabla_\vecnot{x} L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) = \nabla f_0 (\vecnot{x}^*) + \sum_{i\in A} \lambda_i^* \nabla f_i (\vecnot{x}^*) + \sum_{j=1}^p \nu_j^* \nabla h_j (\vecnot{x}^*) &= \vecnot{0} \vspace{-1mm}
\end{align*}
\end{theorem}

% Add example pic with vector additions

\end{frame}

\iffalse
\begin{frame}{Lagrangian Example}

For the pictured example, the Lagrangian is given by
\[ L(\vecnot{x},\lambda) = \underbrace{(x_1 \!-\! 1)^2 + (x_2 \!-\! 1)^2 - \frac{x_1 x_2}{2}}_{f_0(\vecnot{x})} + \lambda \Bigg( \underbrace{1.85 + \frac{(x_1 - 2.25)^2}{2} \!-\! x_2}_{f_1 (\vecnot{x})} \Bigg) \]

\centering
\begin{tikzpicture}
  \begin{axis}[width=80mm,view={60}{30},grid=major,ymin=1.0,ymax=2.5,xmin=1.0,xmax=2.5,xlabel=$x_1$,ylabel=$x_2$]
    \addplot3 [surf,domain=1:2.5] {(x-1)^2 + (y-1)^2 - x*y/2 + 0*(1.85+(x-2.25)^2/2-y)};
    %\addlegendentry{$\lambda=0$}
    
    %\addplot3 [surf,domain=1:2.5] {(x-1)^2 + (y-1)^2 - x*y/2 + 1*(1.85+(x-2.25)^2/2-y)};
    %\addlegendentry{$\lambda=1$}

    \addplot3 [variable=x,mesh,domain=1.0:2.5,red,very thick] ({x},{(1.85+((x-2.25)^2)/2)},{(x-1)^2 + ((1.85+((x-2.25)^2)/2)-1)^2 - x*(1.85+((x-2.25)^2)/2)/2});
    %\addlegendentry{$constraint$}
\end{axis}
\end{tikzpicture}
\end{frame}


\fi

\begin{frame}{Langrangian Duality}

\begin{definition}
The \textcolor{blue}{Lagrangian dual} function is defined to be \vspace{-1.5mm}
\[ g(\vecnot{\lambda},\vecnot{\nu}) \triangleq \inf_{\vecnot{x}\in \mathcal{D}} L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) = \inf_{\vecnot{x}\in \mathcal{D}} \Bigg( f_0(\vecnot{x}) + \sum_{i=1}^m \lambda_i f_i(\vecnot{x}) + \sum_{j=1}^p \nu_j h_j(\vecnot{x}) \Bigg). \]
\end{definition}


%The pointwise infimum of linear functions is concave because
%\[ \inf_{\vecnot{x}\in \mathcal{D}} %L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu})  \inf]

\begin{lemma}<2->
The Lagrangian dual function is concave and the dual problem \vspace{-2.5mm}
\begin{align*}
\mathrm{maximize} \quad & g(\vecnot{\lambda},\vecnot{\nu}) \\
\mathrm{subject\,to} \quad & \vecnot{\lambda} \geq 0 \vspace{-1mm}
\end{align*}
has a unique max value $d^* \leq p^*$.
This property is known as \textcolor{blue}{weak duality}.
\end{lemma}

\begin{definition}<3->
If $d^* = p^*$, then one says that \textcolor{blue}{strong duality} holds for the problem.
\end{definition}

\visible<3->{Proof of lemma on whiteboard}

\end{frame}

\begin{frame}{Weak Duality Proof}

Lagrangian dual is concave because pointwise infimum of affine functions:\vspace{-1mm}
\begin{align*}
g(\alpha\vecnot{\lambda}+&(1-\alpha)\vecnot{\lambda}',\alpha\vecnot{\nu}+(1-\alpha)\vecnot{\nu}') \\
&= \inf_{\vecnot{x}\in \mathcal{D}} L(\vecnot{x},\alpha\vecnot{\lambda}+(1-\alpha)\vecnot{\lambda}',\alpha\vecnot{\nu}+(1-\alpha) \vecnot{\nu}') \\
&= \inf_{\vecnot{x}\in \mathcal{D}} \big( \alpha L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) + (1-\alpha) L(\vecnot{x},\vecnot{\lambda}',\vecnot{\nu}') \big) \\
&\geq \inf_{\vecnot{x}\in \mathcal{D}} \alpha L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) + \inf_{\vecnot{x}'\in \mathcal{D}} (1-\alpha) L(\vecnot{x}',\vecnot{\lambda}',\vecnot{\nu}') \\
&= \alpha g(\vecnot{\lambda},\vecnot{\nu}) + (1-\alpha) g(\vecnot{\lambda}',\vecnot{\nu}'). \vspace{-1mm}
\end{align*}
Concavity implies unique maximum value $d^*$ upper bounded by \vspace{-2mm}
\begin{align*}
g(\vecnot{\lambda},\vecnot{\nu})
&= \inf_{\vecnot{x}\in \mathcal{D}} L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu})
\stackrel{(a)}{\leq} \inf_{\vecnot{x}\in \mathcal{F}} L(\vecnot{x},\vecnot{\lambda},\vecnot{\nu}) \\
&\stackrel{(b)}{=} p^* + \sum_{i=1}^m \lambda_i f_i (\vecnot{x})
\stackrel{(c)}{\leq} p^*, \vspace{-3mm}
\end{align*}
where $(a)$ is implied by $\mathcal{F} \subseteq \mathcal{D}$, $(b)$ follows from $h_j(\vecnot{x}) = 0$ for $\vecnot{x}\in \mathcal{F}$, and $(c)$ holds by combining $f_i(\vecnot{x}) \leq 0$ for $\vecnot{x}\in \mathcal{F}$ and $\lambda_i \geq 0$.

\end{frame}

\backupbegin

%\begin{frame}
%\frametitle{Backup Slides}
%\begin{itemize}
%\item Slide numbers not included in denominator!
%\end{itemize}
%\end{frame}

%\begin{frame}[allowframebreaks]
%\frametitle{References}
%\bibliographystyle{alpha}
%\footnotesize
%\bibliography{IEEEabrv,WCLabrv,WCLbib,WCLnewbib}
%\end{frame}

\backupend

\end{document}
