\documentclass[10pt,english,aspectratio=169]{beamer}
% Use notes or hide notes or show only notes or handout


\usetheme{default}

\usepackage{xstring}
\usepackage{pgfpages}
%\makeatletter
%\IfSubStr{\@classoptionslist}{handout}
%  {\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}
%  {}
%\makeatother

\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{stfloats}
\usepackage{bbm}
\usepackage{pdfpages}
\usepackage{framed}
\usepackage{tabularx}
\usepackage{scalerel}

\usepackage[most]{tcolorbox}
\tcbset{highlight math style={enhanced,
  colframe=white,colback=yellow!15,arc=8pt,boxrule=1pt,
  }}
  
\usepackage{tikz,pgf,pgfplots}
\usepackage{algorithm,algorithmic}
\usepgflibrary{shapes}
\usetikzlibrary{%
  arrows,%
  arrows.meta,
  backgrounds,
  shapes.misc,% wg. rounded rectangle
  shapes.arrows,%
  shapes,%
  calc,%
  chains,%
  matrix,%
  positioning,% wg. " of "
  scopes,%
  decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
  shadows,%
  backgrounds,%
  fit,%
  petri,%
  quotes
}

\tikzset{background rectangle/.style={
    fill=white,
  },
  use background/.style={    
    show background rectangle
  }
}

\setbeamersize{text margin left=10mm,text margin right=35mm}

\pgfplotsset{compat=1.12}

%\usetheme{Frankfurt}
%\usecolortheme{ldpc}
\useinnertheme{rounded}
\usecolortheme{whale}
\usecolortheme{orchid}

\newcommand{\ul}[1]{\underline{#1}}
\renewcommand{\Pr}{\mathbb{P}}

%% Setup slides and notes
\makeatletter
\IfSubStr{\@classoptionslist}{notes} { \IfSubStr{\@classoptionslist}{hide} {}{\IfSubStr{\@classoptionslist}{only} {}{\setbeameroption{show notes on second screen=right}}} }{}
\makeatother
%\setbeamertemplate{note page}{\pagecolor{yellow!5}\vfill\insertnote\vfill}

\newcommand{\getpdfpages}[2]{\begingroup
  \setbeamercolor{background canvas}{bg=}
  \addtocounter{framenumber}{1}
  \includepdf[pages={#1},%
  pagecommand={%
    \expandafter\def\expandafter\insertshorttitle\expandafter{%
      \insertshorttitle\hfill\insertframenumber\,/\,\inserttotalframenumber}}%
  ]{#2}
  \endgroup}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}

 \setbeamercolor{bibliography entry author}{fg=black}
 \setbeamercolor{bibliography entry title}{fg=black}
 \setbeamercolor{bibliography entry location}{fg=black}
 \setbeamercolor{bibliography entry note}{fg=black}
 
 \setbeamerfont{bibliography item}{size=\footnotesize}
 \setbeamerfont{bibliography entry author}{size=\footnotesize}
 \setbeamerfont{bibliography entry title}{size=\footnotesize}
 \setbeamerfont{bibliography entry location}{size=\footnotesize}
 \setbeamerfont{bibliography entry note}{size=\footnotesize}
 \setbeamertemplate{bibliography item}{\insertbiblabel}
 
\newlength\tikzwidth
\newlength\tikzheight


\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
%\newcommand{\expt}{\mbb{E}}
%\newcommand{\dd}{\mathrm{d}}
\input{macros}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\def\greencheck{{\color{green}\checkmark}}
\def\scalecheck{\resizebox{\widthof{\checkmark}*\ratio{\widthof{x}}{\widthof{\normalsize x}}}{!}{\checkmark}}
\def\xmark{\tikz [x=1.4ex,y=1.4ex,line width=.2ex, red] \draw (0,0) -- (1,1) (0,1) -- (1,0);}
\def\redx{{\color{red}\xmark}}

\renewcommand{\footnotesep}{-2pt}


\begin{document}

\title{ECE 586: Vector Space Methods \\ Lecture 18: Best Approximation Formulas}
\author{Henry D. Pfister \\ Duke University}
\date{}
%\date{August 20th, 2020}
%\maketitle

\setbeamertemplate{navigation symbols}{}

\begin{frame}[plain]
	\titlepage
	
	\note{
		\vspace{8mm}
		\begin{enumerate}
			\setlength\itemsep{3mm}
			\color{red}
			\item Welcome to the 11th video lecture for ECE 586, Vector Space Methods. \\[2mm]
			Today, we'll finish our discussion of subspaces and bases and then move on to linear transforms.
		\end{enumerate}
	}
\end{frame}

\addtocounter{framenumber}{-1}
\setbeamertemplate{navigation symbols}{\textcolor{blue}{\footnotesize \insertframenumber ~/ \inserttotalframenumber}}




\begin{frame}{4.2: Normal Equations}

Let $W$ be a subspace of a Hilbert space $V$ that is spanned by the linearly independent (but not orthogonal) set of vectors  $\vecnot{w}_1, \ldots, \vecnot{w}_n \in V$.

\vspace{2mm}

The projection theorem shows that $\hat{\vecnot{v}} \in W$ is the best approximation of $\vecnot{v} \in V$ if and only if $(\vecnot{v} - \hat{\vecnot{v}}) \bot \vecnot{w}_j$ for $j=1,\ldots,n$.
This implies that \vspace{-1mm}
\begin{equation*}
\inner{ \vecnot{v} - \hat{\vecnot{v}} }{ \vecnot{w}_j }
= \inner{ \vecnot{v} - \sum_{i=1}^n s_i \vecnot{w}_i }{ \vecnot{w}_j }
= 0 \vspace{-1mm}
\end{equation*}
or, equivalently, the \textcolor{blue}{normal equations} \vspace{-1mm}
\begin{equation*}
\sum_{i=1}^n s_i \inner{ \vecnot{w}_i }{ \vecnot{w}_j }
= \inner{ \vecnot{v} }{ \vecnot{w}_j }. \vspace{-1mm}
\end{equation*}

The gives a system of $n$ linear equations in $n$ unknowns defined by \vspace{-1mm}
\begin{equation*}
\underbrace{\left[ \begin{array}{cccc}
\inner{ \vecnot{w}_1 }{ \vecnot{w}_1 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_1 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_1 } \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_2 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_2 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_2 } \\
\vdots & \vdots & \ddots & \vdots \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_n }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_n } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_n }
\end{array} \right]}_G
\underbrace{\left[ \begin{array}{c}
s_1 \\ s_2 \\ \vdots \\ s_n \end{array} \right]}_\vecnot{s}
= \underbrace{\left[ \begin{array}{c}
\inner{ \vecnot{v} }{ \vecnot{w}_1 } \\
\inner{ \vecnot{v} }{ \vecnot{w}_2 } \\ \vdots \\
\inner{ \vecnot{v} }{ \vecnot{w}_n } \end{array} \right]}_\vecnot{t} .
\end{equation*}

\end{frame}

\begin{frame}{4.2: The Gramian}

\vspace{-1.5mm}

\begin{definition}<1->
For $\vecnot{w}_1,\ldots,\vecnot{w}_n$, the $n \times n$ \textcolor{blue}{Gramian matrix} is defined to be \vspace{-2mm}
\begin{equation*}
G = \left[ \begin{array}{cccc}
\inner{ \vecnot{w}_1 }{ \vecnot{w}_1 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_1 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_1 } \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_2 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_2 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_2 } \\
\vdots & \vdots & \ddots & \vdots \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_n }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_n } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_n }
\end{array} \right] \vspace{-1mm}
\end{equation*}
Since $g_{ij} = \smash{\inner{ \vecnot{w}_j }{ \vecnot{w}_i }}$, we see $G$ is Hermitian symmetric (i.e. $G^H = G$).
\end{definition}

\vspace{-1mm}

\begin{definition}<2->
A matrix $M\in F^{n \times n}$ is \textcolor{blue}{positive-semidefinite} if $M=M^H$ and $\vecnot{v}^H M \vecnot{v} \geq 0$ for all $\vecnot{v} \in F^n - \left\{ \vecnot{0} \right\}$.
If the inequality is strict, then it is \textcolor{blue}{positive-definite}.
\end{definition}

\vspace{-1mm}

\begin{theorem}<2->
A Gramian matrix $G$ is always positive-semidefinite.
It is positive-definite if and only if the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is linearly independent.
\end{theorem}
\vspace{-1mm}

\visible<2->{Proof in live session.}

\end{frame}

\begin{frame}{4.3: Least-Squares Solution of a Linear System}

\visible<1->{%
For $V = F^m$, let $A \in F^{m \times n}$ be a matrix whose $i$-th column is $\vecnot{w}_i \in V$.
Then, a vector $\hat{\vecnot{v}} \in W = \text{colspace}(A)$ can be written as \vspace{-1mm}
\[ \hat{\vecnot{v}} = A \vecnot{s} = \sum_{i=1}^n s_i \vecnot{w}_i. \vspace{-1mm} \]

Also, the best approximation of $\vecnot{v}$ by vectors in $W$ is found by solving
\[ \min_{\hat{\vecnot{v}}\in W} \| \vecnot{v} - \hat{ \vecnot{v}} \| = \min_\vecnot{s} \| \vecnot{v} - A \vecnot{s} \| . \]
}

\vspace{-2mm}

\visible<2->{%
For the induced norm, any solution must satisfy the normal equations
\[ \inner{ \vecnot{v} - \hat{\vecnot{v}} }{ \vecnot{w}_j }
= \inner{ \vecnot{v} - A \vecnot{s} }{ \vecnot{w}_j }
= 0, \quad j \in [n]. \]
}

\vspace{-4mm}

\visible<3->{%
For the standard inner product, these equations can be expressed as
\begin{equation*}
\vecnot{0} = \left[ \begin{array}{c} \vecnot{w}_1^H \\ \vdots \\ \vecnot{w}_n^H \end{array} \right] \left( \vecnot{v} - A \vecnot{s} \right) = A^H \vecnot{v} - A^H A \vecnot{s} = \vecnot{t} - G \vecnot{s},
\end{equation*}
where \textcolor{blue}{$G = A^H A$ is the Gramian} and \textcolor{blue}{$\vecnot{t}$ is the cross-correlation vector.}}
\end{frame}

\begin{frame}{4.3.2: Pseudo-Inverse and Projection}

\visible<1->{%
When the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ are linearly independent, the Gramian matrix is positive definite and hence invertible.
Thus, the optimal solution for the least-squares problem is given by
\begin{equation*}
\vecnot{s} = G^{-1} \vecnot{t} = \left( A^H A \right)^{-1} A^H \vecnot{v},
\end{equation*}
where the matrix \textcolor{blue}{$\left( A^H A \right)^{-1} A^H$ is the pseudoinverse} of $A$ in this case.}

\vspace{7mm}

\visible<2->{%
Using this, the best approximation of $\vecnot{v} \in V$ by vectors in $W$ is equal to
\begin{equation*}
\hat{\vecnot{v}} = A \vecnot{s} = A \left( A^H A \right)^{-1} A^H \vecnot{v} .
\end{equation*}
The matrix \textcolor{blue}{$P = A \left( A^H A \right)^{-1} A^H$ is the projection matrix} for the range of $A$.
It defines an orthogonal projection onto the range of $A$ (i.e., the subspace spanned by the columns of $A$).}

\end{frame}

\begin{frame}{4.3.3: Weighted Least-Squares Solution of a Linear System}

For the standard Euclidean norm $\| \vecnot{v} \|_E = \sqrt{\vecnot{v}^H \vecnot{v}}$ and any invertible $B$, consider the weighted least-squares problem
\[ \min_{\hat{\vecnot{v}}\in W} \| B(\vecnot{v} - \hat{ \vecnot{v}}) \|_E = \min_\vecnot{s} \| B( \vecnot{v} - A \vecnot{s}) \|_E \]
But, $\| B \vecnot{v} \|_E$ equals the induced norm $\| \vecnot{v} \|$ for the weighted inner product
\[ \inner{ \vecnot{u} }{ \vecnot{v} }  \triangleq \vecnot{v}^H B^H B \vecnot{u}. \]

For the weighted inner product, the normal equations look the same
\[ \inner{ \vecnot{v} - \hat{\vecnot{v}} }{ \vecnot{w}_j }
= \inner{ \vecnot{v} - A \vecnot{s} }{ \vecnot{w}_j }
= 0, \quad j \in [n]. \]
but they solve a different problem and they reduce to
\begin{equation*}
\vecnot{0} = \left[ \begin{array}{c} \vecnot{w}_1^H \\ \vdots \\ \vecnot{w}_n^H \end{array} \right] B^H B \left( \vecnot{v} - A \vecnot{s} \right) = A^H B^H B \vecnot{v} - A^H B^H B A \vecnot{s}
\end{equation*}

\end{frame}

\iffalse
\begin{frame}{4.3.4: Expression for Minimum Approximation Error}

Let $\hat{\vecnot{v}} \in W$ be the best approximation of $\vecnot{v}$ by vectors in $W$. Then,
\begin{equation*}
\vecnot{v} = \hat{\vecnot{v}} + \vecnot{e},
\end{equation*}
where $\vecnot{e} \in W^{\bot}$ is the minimum achievable error.
The squared norm of the minimum error is given implicitly by
\begin{equation*}
\left\| \vecnot{v} \right\|^2
= \left\| \hat{\vecnot{v}} + \vecnot{e} \right\|^2
= \inner{ \hat{\vecnot{v}} + \vecnot{e} }{ \hat{\vecnot{v}} + \vecnot{e} }
= \inner{ \hat{\vecnot{v}} }{ \hat{\vecnot{v}} }
+ \inner{ \vecnot{e} }{ \vecnot{e} }
= \left\| \hat{\vecnot{v}} \right\|^2 + \left\| \vecnot{e} \right\|^2 .
\end{equation*}
For the weighted problem, let $H=B^H B$ and write
\begin{equation*}
\begin{split}
\left\| \vecnot{e} \right\|^2
&= \left\| \vecnot{v} \right\|^2
- \left\| \hat{\vecnot{v}} \right\|^2
= \vecnot{v}^H H \vecnot{v} - \hat{\vecnot{v}}^H H \hat{\vecnot{v}} \\
&= \vecnot{v}^H H \vecnot{v} - \vecnot{s}^H A^H H A \vecnot{s} \\
&= \vecnot{v}^H H \vecnot{v}
- \vecnot{v}^H H A \left( A^H H A \right)^{-1} A^H H \vecnot{v} \\
&= \vecnot{v}^H
\left( H -  H A \left( A^H H A \right)^{-1} A^H H \right)
\vecnot{v}.
\end{split}
\end{equation*}
\end{frame}
\fi

\begin{frame}{4.4.2: Linear Minimum Mean-Squared Error Estimation}

Let $Y, X_1, \ldots, X_n$ be zero-mean random variables.
Linear minimum mean-squared error (LMMSE) estimation finds $s_1, \ldots, s_n$ such that
\[ \hat{Y} = s_1 X_1 + \cdots + s_n X_n \]
minimizes the mean squared-error $\Expect[|Y-\hat{Y}|^2]$.
Using the inner product
\begin{equation*}
\inner{ X }{ Y } = \Expect \left[ X \overline{Y} \right],
\end{equation*}
the normal equations for the LMMSE estimate $\hat{Y}$ are $G \vecnot{s} = \vecnot{t}$, where
\begin{equation*}
G = \left[ \begin{array}{cccc}
\Expect \left[ X_1 \overline{X}_1 \right]
& \Expect \left[ X_2 \overline{X}_1 \right] & \cdots
& \Expect \left[ X_n \overline{X}_1 \right] \\
\Expect \left[ X_1 \overline{X}_2 \right]
& \Expect \left[ X_2 \overline{X}_2 \right] & \cdots
& \Expect \left[ X_n \overline{X}_2 \right] \\
\vdots & \vdots & \ddots & \vdots \\
\Expect \left[ X_1 \overline{X}_n \right]
& \Expect \left[ X_2 \overline{X}_n \right] & \cdots
& \Expect \left[ X_n \overline{X}_n \right] \\
\end{array} \right], \quad \vecnot{t} = \left[ \begin{array}{c}
\Expect \left[ Y \overline{X}_1 \right] \\
\Expect \left[ Y \overline{X}_2 \right] \\ \vdots \\
\Expect \left[ Y \overline{X}_n \right] \end{array} \right].
\end{equation*}

If $G$ is invertible, then $\vecnot{s} = G^{-1} \vecnot{t}$ implies $\Expect [|\hat{Y}|^2] = \vecnot{s}^H G \vecnot{s} = \vecnot{s}^H \vecnot{t}$ and
\begin{equation*}
%\left\| Y - \hat{Y} \right\|^2 =
\left\| Y \right\|^2 - \left\| \hat{Y} \right\|^2 
= \Expect \left[ |Y|^2 \right]
- \Expect \left[ |\hat{Y}|^2 \right] = \Expect \left[ |Y|^2 \right] - \vecnot{s}^H \vecnot{t}.
\end{equation*}

\end{frame}

\begin{frame}{4.5.1: Dual Approximation and Minimum-Norm Solutions}

An underdetermined system of linear equations has an infinite number of solutions.
It often makes sense to prefer the \textcolor{blue}{minimum-norm solution}.

\vspace{1.5mm}

Let $V$ be a Hilbert space and $\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n$ be a basis for subspace $W$.
For any $\vecnot{v} \in V$, the best approximation of $\vecnot{v}$ in $W$ can be found by solving
\begin{equation} \label{eqn:DualNormalEquations}
\left[ \begin{array}{cccc}
\inner{ \vecnot{w}_1 }{ \vecnot{w}_1 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_1 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_1 } \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_2 }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_2 } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_2 } \\
\vdots & \vdots & \ddots & \vdots \\
\inner{ \vecnot{w}_1 }{ \vecnot{w}_n }
& \inner{ \vecnot{w}_2 }{ \vecnot{w}_n } & \cdots
& \inner{ \vecnot{w}_n }{ \vecnot{w}_n }
\end{array} \right]
\left[ \begin{array}{c}
s_1 \\ s_2 \\ \vdots \\ s_n \end{array} \right]
= \left[ \begin{array}{c}
\inner{ \vecnot{v} }{ \vecnot{w}_1 } \\
\inner{ \vecnot{v} }{ \vecnot{w}_2 } \\ \vdots \\
\inner{ \vecnot{v} }{ \vecnot{w}_n } \end{array} \right] .
\end{equation}

\vspace{-1mm}

\begin{theorem}
Let $V$ be a Hilbert space and $\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n$ be a basis for $W\subseteq V$.
The \textcolor{blue}{dual approximation} problem is to find the minimum-norm vector $\vecnot{w}\in V$ satisfying $\inner{ \vecnot{w} }{ \vecnot{w}_i } = c_i$ for $i=1,\ldots,n$.
Then, the solution $\vecnot{w}$ satisfies \vspace{-3mm}
\[ \vecnot{w} = \sum_{i=1}^n s_i \vecnot{w}_i \in W, \vspace{-2.75mm} \]
where $s_1,s_2,\ldots,s_n$ can be found by solving \eqref{eqn:DualNormalEquations} with $\inner{ \vecnot{v} }{ \vecnot{w}_i } = c_i$.
\end{theorem}

\end{frame}

\begin{frame}{4.5.2: Minimum-Norm Solutions}

\visible<1->{%
For $A \in \ComplexNumbers^{m\times n}$ with $m<n$ and $\vecnot{v} \in \ComplexNumbers^m$, consider the underdetermined linear system $A \vecnot{s} = \vecnot{v}$.
Then, the dual approximation theorem can be applied to solve the minimum-norm problem
\[ \min_{\vecnot{s} : A\vecnot{s} = \vecnot{v}} \| \vecnot{s} \| . \]
}

\vspace{1mm}

\visible<2->{%
To see this as a dual approximation, we can rewrite the constraint $A \vecnot{s} = \vecnot{v}$ as $B^H \vecnot{s} = \vecnot{v}$ where $B = A^H$.
Then, the theorem concludes that the minimum-norm solution lies in  the column space of $B=A^H$.}

\vspace{4mm}

\visible<3->{%
Using $\vecnot{s} \in \mathcal{R}(A^H)$, there is a $\vecnot{t}$ such that $\hat{\vecnot{s}} = A^H \vecnot{t}$ and the constraint gives $A (A^H \vecnot{t}) = \vecnot{v}$.
If the rows of $A$ are linearly independent, then the columns of $B=A^H$ are linearly independent and $(B^H B)^{-1} = (A A^H)^{-1}$ exists.

\vspace{4mm}

Thus, the solution $\hat{\vecnot{s}}$ can be obtained in closed form and is given by \vspace{-1mm}
\[ \color{blue} \hat{\vecnot{s}} = A^H \left( A A^H \right)^{-1} \vecnot{v}. \]
}

\end{frame}

\begin{frame} \frametitle{Next Steps}

\begin{itemize}
\setlength\itemsep{5mm}
\item To continue studying after this video -- \vspace{2mm}

\begin{itemize}
 \setlength\itemsep{3mm}
 \item Try the required reading: Course Notes EF 4.2 - 4.3.4
 \item Or the recommended reading: LADR 6C
 \item Also, look at the problems in Assignment 7
\end{itemize}
\end{itemize}

\note{
	\vspace{8mm}
	\begin{enumerate}
		\setlength\itemsep{3mm}
		\color{red}
		\item Here are some options to continue learning this material. (read) \\ [2mm]  That's it for today.  So, I'll see you next time.
	\end{enumerate}
}

\end{frame}


\end{document}


