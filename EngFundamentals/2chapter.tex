\chapter{Linear Algebra}


\section{Fields}

Consider a set $F$ of objects and two operations on the elements of $F$, addition and multiplication.
For every pair of elements $s, t \in F$ then $(s + t) \in F$.
For every pair of elements $s, t \in F$ then $st \in F$.
Suppose that these two operations satisfy
\begin{enumerate}
\item addition is commutative: $s + t = t + s \; \forall s, t \in F$
\item addition is associative: $r + (s + t) = (r + s) + t \; \forall r, s, t \in F$
\item to each $s \in F$ there exists a unique element $(-s) \in F$ such that $s + (-s) = 0$
\item multiplication is commutative: $st = ts \; \forall s,t \in F$
\item multiplication is associative: $r(st) = (rs)t \; \forall r, s, t \in F$
\item there is a unique non-zero element $1 \in F$ such that $s1 = s \; \forall s \in F$
\item to each $s \in F - 0$ there exists a unique element $s^{-1} \in F$ such that $s s^{-1} = 1$
\item multiplication distributes over addition: $r (s + t) = rs + rt \; \forall r, s, t \in F$.
\end{enumerate}
Then, the set $F$ together with these two operations is a \textbf{field}.
\index{field}

\begin{example}
The real numbers with the usual operations of addition and multiplication form a field.
The complex numbers with these two operations also form a field.
\end{example}

\begin{example}
The set of integers with addition and multiplication is not a field.
\end{example}

\begin{problem} \label{problem:RationalNumbers}
Is the set of rational numbers a subfield of the real numbers?
\end{problem}

\begin{example}
Is the set of all real numbers of the form $s + t \sqrt{2}$, where $s$ and $t$ are rational, a subfield of the complex numbers?

The set $F = \left\{ s + t \sqrt{2} : s, t \in \RationalNumbers \right\}$ together with the standard addition and multiplication is a field.
Let $s, t, u, v \in \RationalNumbers$,
\begin{gather*}
s + t \sqrt{2} + u + v \sqrt{2} = (s+u) + (t+v) \sqrt{2} \in F \\
\left( s + t \sqrt{2} \right) \left( u + v \sqrt{2} \right) = (su + 2 tv) + (sv + tu) \sqrt{2} \in F \\
\left( s + t \sqrt{2} \right)^{-1} = \frac{ s - t \sqrt{2} }{ s^2 + 2 t^2 }
= \frac{ s }{ s^2 + 2 t^2 } - \frac{ t }{ s^2 + 2 t^2 } \sqrt{2} \in F
\end{gather*}
Again, the remaining properties are straightforward to prove.
The field $s + t \sqrt{2}$, where $s$ and $t$ are rational, is a subfield of the complex numbers.
\end{example}


\section{Matrices}

Let $F$ be a field and consider the problem of finding $n$ scalars $x_1, \ldots, x_n$ which satisfy the conditions
\begin{equation} \label{equation:SystemOfEquations}
\begin{array}{ccccccccc}
a_{11} x_1 & + & a_{12} x_2 & + & \cdots & + & a_{1n} x_n & = & y_1 \\
a_{21} x_1 & + & a_{22} x_2 & + & \cdots & + & a_{2n} x_n & = & y_2 \\
\vdots & & \vdots  & & & & \vdots & & \vdots \\
a_{m1} x_1 & + & a_{m2} x_2 & + & \cdots & + & a_{mn} x_n & = & y_m
\end{array}
\end{equation}
where $\{ y_i : 1 \leq i \leq n \} \subset F$ and $\{ a_{ij} : 1 \leq i \leq m, 1 \leq j \leq n \} \subset F$.
These conditions form a \emph{system of $m$ linear equations in $n$ unknowns}.
A shorthand notation for~\eqref{equation:SystemOfEquations} is the matrix equation
\begin{equation*}
A \vecnot{x} = \vecnot{y},
\end{equation*}
where $A$ is the matrix representation given by
\begin{equation*}
A = \left[ \begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{array} \right]
\end{equation*}
and $\vecnot{x}$, $\vecnot{y}$ denote
\begin{align*}
\vecnot{x} &= (x_1, \ldots, x_n)^T \\
\vecnot{y} &= (y_1, \ldots, y_m)^T .
\end{align*}

\begin{definition}
Let $A$ be an $m \times n$ matrix over $F$ and let $B$ be an $n \times p$ matrix over $F$.
The \defn{matrix}{matrix product} $AB$ is the $m \times p$ matrix $C$ whose $i,j$ entry is
\begin{equation*}
c_{ij} = \sum_{r = 1}^n a_{ir} b_{rj}.
\end{equation*}
\end{definition}

\begin{definition}
Let $A$ be an $n \times n$ matrix over $F$.
An $n \times n$ matrix $B$ is called the \defn{matrix}{inverse} of $A$ if
\begin{equation*}
AB = BA = I.
\end{equation*}
If $A$ is invertible then its inverse will be denoted by $A^{-1}$.
\end{definition}

\begin{lemma}
\label{lemma:widematrix_has_nullvector}
Let $A$ be an $m \times n$ matrix over $F$ with $m<n$.
Then, there exists a length-$n$ column vector $\vecnot{x}$ (over $F$) such that $A \vecnot{x} = \vecnot{0}$.
\end{lemma}
\begin{proof}
This result follows easily from row reduction and the proof is left as an exercise for the reader.
\end{proof}

\section{Vector Spaces}

\index{vector space}
\begin{definition} \label{definition:VectorSpace}
A \textbf{vector space} consists of the following,
\begin{enumerate}
\item a field $F$ of scalars
\item a set $V$ of objects, called vectors
\item an operation called vector addition, which associates with each pair of vectors $\vecnot{v}, \vecnot{w} \in V$ a vector $\vecnot{v} + \vecnot{w} \in V$ such that
\begin{enumerate}
\item addition is commutative: $\vecnot{v} + \vecnot{w} = \vecnot{w} + \vecnot{v}$
\item addition is associative: $\vecnot{u} + (\vecnot{v} + \vecnot{w}) = (\vecnot{u} + \vecnot{v}) + \vecnot{w}$
\item there is a unique vector $\vecnot{0} \in V$ such that $\vecnot{v} + \vecnot{0} = \vecnot{v}$, $\forall \vecnot{v} \in V$
\item to each $\vecnot{v} \in V$ there is a unique vector $- \vecnot{v} \in V$ such that $\vecnot{v} + (- \vecnot{v}) = \vecnot{0}$
\end{enumerate}
\item an operation called scalar multiplication, which associates with each $s \in F$ and $\vecnot{v} \in V$ a vector $s \vecnot{v} \in V$ such that
\begin{enumerate}
\item $1 \vecnot{v} = \vecnot{v}$, $\forall \vecnot{v} \in V$
\item $(s_1 s_2) \vecnot{v} = s_1 ( s_2 \vecnot{v} )$
\item $s ( \vecnot{v} + \vecnot{w} ) = s \vecnot{v} + s \vecnot{w}$
\item $(s_1 + s_2) \vecnot{v} = s_1 \vecnot{v} + s_2 \vecnot{v}$.
\end{enumerate}
\end{enumerate}
\end{definition}

\begin{example}
Let $F$ be a field, and let $V$ be the set of all $n$-tuples $\vecnot{v} = (v_1, \ldots, v_n)$ of scalar $v_i \in F$.
If $\vecnot{w} = (w_1, \ldots, w_n)$ with $w_i \in F$, the sum of $\vecnot{v}$ and $\vecnot{w}$ is defined by
\begin{equation*}
\vecnot{v} + \vecnot{w} = (v_1 + w_1, \ldots, v_n + w_n).
\end{equation*}
The product of a scalar $s \in F$ and vector $\vecnot{v}$ is defined by
\begin{equation*}
s \vecnot{v} = (s v_1, \ldots, s v_n).
\end{equation*}
The set of $n$-tuples, denoted by $F^n$, together with the vector addition and scalar product defined above forms a vector space.
\end{example}

\begin{example}
Let $X$ be a non-empty set and let $F$ be a field.
Consider the set $V$ of all functions from $X$ into $F$.
The sum of two vectors $f,g \in V$ is the function from $X$ into $F$ defined by
\begin{equation*}
(f + g)(x) = f(x) + g(x) \quad \forall x \in X.
\end{equation*}
The product of scalar $s \in F$ and the function $f \in V$ is the function $sf$ defined by
\begin{equation*}
(sf)(x) = s f(x) \; \forall x \in X.
\end{equation*}
\end{example}

\begin{definition}
A vector $\vecnot{w} \in V$ is said to be a \defn{vector space}{linear combination} of the vectors $\vecnot{v}_1, \ldots, \vecnot{v}_n \in V$ provided that there exist scalars $s_1, \ldots, s_n \in F$ such that
\begin{equation*}
\vecnot{w} = \sum_{i=1}^n s_i \vecnot{v}_i.
\end{equation*}
\end{definition}

\begin{definition}
Consider a complex $n \times n$ matrix $A$ with elements $a_{ij}$.
The \defn{matrix}{Hermitian transpose} $B = A^H$ of $A$ has elements defined by $b_{ij} = \overline{a_{ji}}$ where $\overline{a}$ denotes the complex conjugate of $a$.
\end{definition}.

\subsection{Subspaces}

\begin{definition}
Let $V$ be a vector space over $F$.
A \defn{vector space}{subspace} of $V$ is a subset $W \subset V$ which is itself a vector space over $F$.
\end{definition}

\begin{fact}
A non-empty subset $W \subset V$ is a subspace of $V$ if and only if for every pair of vector $\vecnot{w}_1, \vecnot{w}_2 \in W$ and every scalar $s \in F$ the vector $s \vecnot{w}_1 + \vecnot{w}_2$ is again in $W$.
\end{fact}
If $V$ is a vector space then the intersection of any collection of subspaces of $V$ is a subspace of $V$.

\begin{example}
Let $A$ be an $m \times n$ matrix over $F$.
The set of all $n \times 1$ column vectors $V$ such that
\begin{equation*}
\vecnot{v} \in V \implies A \vecnot{v} = \vecnot{0}
\end{equation*}
is a subspace of $F^{n \times 1}$.
\end{example}

\begin{definition}
Let $U$ be a set of vectors in a vector space $V$.
The \defn{vector space}{span} of $U$, denoted $\Span(U)$, is defined to be the set of all finite linear combinations of vectors in $U$.
%The \defn{vector space}{subspace spanned} by $U$ is defined to be the intersection $W$ of all subspaces of $V$ which contain $U$.
\end{definition}
The subspace spanned by $U$ can also be defined equivalently as the intersection of all subspaces of $V$ which contain $U$.
It is easy to see that all finite linear combinations of vectors in $U$ must be in all subspaces containing $U$.
One can show the converse by considering its contrapositive: There exists a finite linear combination of vectors in $U$ and a subspace containing $U$ such that the vector is not in the subspace.
Of course, this contradicts the definition of a vector space.

\begin{definition}
Let $V$ be a vector space and $U,W$ be subspaces.
If $U,W$ are disjoint (i.e., $U\cap W = \{ \vecnot{0} \}$), their \defn{vector space}{direct sum} $U \oplus W$ is defined by
\[ U \oplus W \triangleq \{\vecnot{u}+\vecnot{w} | \vecnot{u} \in U, \vecnot{w} \in W \}. \]
\end{definition}

An important property of a direct sum is that any vector $\vecnot{v} \in U \oplus W$ has a unique decomposition $\vecnot{v} = \vecnot{u} + \vecnot{w}$ where $\vecnot{u} \in U$ and $\vecnot{w} \in W$.
 
\subsection{Bases and Dimension}
\label{section:BasesAndDimension}

The dimension of a vector space is defined using the concept of a basis for the space.

\begin{definition}
Let $V$ be a vector space over $F$.
A subset $U \subset V$ is \defn{vector space}{linearly dependent} if there exist distinct vectors $\vecnot{u}_1, \ldots, \vecnot{u}_n \in U$ and scalars $s_1, \ldots, s_n \in F$, not all of which are $0$, such that
\begin{equation*}
\sum_{i=1}^n s_i \vecnot{u}_i = 0.
\end{equation*}
A set which is not linearly dependent is called \defn{vector space}{linearly independent}.
\end{definition}

A few important consequences follow immediately from this definition.
Any subset of a linearly independent set is also linearly independent.
Any set which contains the $\vecnot{0}$ vector is linearly dependent.
A set $U \subset V$ is linearly independent if and only if each finite subset of $U$ is linearly independent.

\iffalse

\begin{lemma}
Let $V$ be a vector space over $F$ and suppose a subset $U \subseteq V \setminus \{ \vecnot{0} \}$ is linearly dependent.
Then, there are at least two distinct vectors $\vecnot{u} \in U$ such that (i) $\vecnot{u} \in \Span \left(U \setminus  \{\vecnot{u} \}\right)$ and (ii) $\Span \left( U \setminus \{\vecnot{u} \}\right) = \Span(U)$.
\end{lemma}
\begin{proof}
%If $\vecnot{0} \in U$, then this is trivially true by choosing $\vecnot{u}_1 = \vecnot{0}$.
%Therefore, we assume wolog that $\vecnot{0} \notin U$.
Since $U$ is linearly dependent, there exist distinct vectors $\vecnot{u}_1, \ldots, \vecnot{u}_n \in U$ and scalars $s_1, \ldots, s_n \in F$, not all of which are $0$, such that
\begin{equation*}
\sum_{i=1}^n s_i \vecnot{u}_i = 0.
\end{equation*}
Since none of the $\vecnot{u}_i$ are $\vecnot{0}$, this implies that at least two coefficients (e.g., $s_j,s_k$) are non-zero.
This allows us to solve for either $\vecnot{u}_j$ or $\vecnot{u}_k$ in terms of the other vectors in $U$.
Therefore, they both satisfy (i).
%Therefore, we have
%\begin{equation*}
%\vecnot{u}_j = -\frac{s_2}{s_1} \vecnot{u}_2 - \frac{s_3}{s_1} \vecnot{u}_3 - \ldots -\frac{s_n}{s_1} \vecnot{u}_n, 
%\end{equation*}
%which clearly implies (i).
Result (ii) for each vector follows from (i) for the same vector because, for example, any linear combination including $\vecnot{u}_j$ can be rewritten in terms of vectors in $U \setminus  \{\vecnot{u}_j \}$.
\end{proof}

\fi

\begin{definition}
Let $V$ be a vector space over $F$.
Let $\mathcal{B} = \left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$ be a subset of linearly independent vectors from $V$, such that every $\vecnot{v} \in V$ has a unique decomposition as a finite linear combination of vectors from $\mathcal{B}$.
Then, the set $\mathcal{B}$ is a \defn{vector space}{Hamel basis} for $V$.
The space $V$ is \defn{vector space}{finite-dimensional} if it has a finite basis.
\end{definition}

\begin{theorem}
Every vector space has a Hamel basis.
\end{theorem}
\begin{proof}
Let $X$ be the set of linearly independent subsets of $V$.
Furthermore, for $x, y \in X$ consider the strict partial order defined by proper inclusion.
By the maximum principle, if $x$ is an element of $X$, then there exists a maximal simply ordered subset $Z$ of $X$ containing $x$.
This element is a Hamel basis for $V$.
\end{proof}

\begin{example}
Let $F$ be a field and let $U \subset F^n$ be the subset consisting of the vectors $\vecnot{e}_1, \ldots, \vecnot{e}_n$ defined by
\begin{equation*}
\begin{array}{ccc}
\vecnot{e}_1 & = & (1, 0, \ldots, 0) \\
\vecnot{e}_2 & = & (0, 1, \ldots, 0) \\
\vdots & = & \vdots \\
\vecnot{e}_n & = & (0, 0, \ldots, 1).
\end{array}
\end{equation*}
For any $\vecnot{v} = (v_1, \ldots, v_n) \in F^n$, we have
\begin{equation} \label{equation:StandardBasisSpan}
\vecnot{v} = \sum_{i=1}^n v_i \vecnot{e}_i.
\end{equation}
Thus, the collection $U = \left\{ \vecnot{e}_1, \ldots, \vecnot{e}_n \right\}$ spans $F^n$.
Since $\vecnot{v} = \vecnot{0}$ in~\eqref{equation:StandardBasisSpan} if and only if $v_1 = \cdots = v_n = 0$, $U$ is linearly independent.
Accordingly, the set $U$ is a basis for $F^{n \times 1}$.
This basis is termed the \defn{vector space}{standard basis} of $F^n$.
\end{example}

\begin{example}
Let $A$ be an invertible matrix over $F$.
The columns of $A$, denoted by $A_1, \ldots, A_n$, form a basis for the space of column vectors $F^{n \times 1}$.
This can be seen as follows.
If $\vecnot{v} = (v_1, \ldots, v_n)^T$ is a column vector, then
\begin{equation*}
A \vecnot{v} = \sum_{i=1}^n v_i A_i.
\end{equation*}
Since $A$ is invertible,
\begin{equation*}
A \vecnot{v} = \vecnot{0} \implies I \vecnot{v} = A^{-1} \vecnot{0} \implies \vecnot{v} = \vecnot{0}.
\end{equation*}
That is, $\{ A_1, \ldots, A_n \}$ is a linearly independent set.
For any column vector $\vecnot{w} \in F^n$, let $\vecnot{v} = A^{-1} \vecnot{w}$.
It follows that $\vecnot{w} = A \vecnot{v}$ and, as a consequence, $\{ A_1, \ldots, A_n \}$ is a basis for $F^n$.
\end{example}

\begin{theorem}
Let $V$ be a vector space which is spanned by a finite set of vectors $W = \left\{ \vecnot{w}_1, \ldots, \vecnot{w}_n \right\}$.
Then, any linearly independent set of vectors in $V$ is finite and contains no more than $n$ elements.
\end{theorem}
\begin{proof}
Assume that $U = \left\{ \vecnot{u}_1, \ldots, \vecnot{u}_m \right\} \subset V$ with $m > n$.
Since $W$ spans $V$, there exists scalars $a_{ij}$ such that
\begin{equation*}
\vecnot{u}_j = \sum_{i=1}^n a_{ij} \vecnot{w}_i.
\end{equation*}
For any $m$ scalars $s_1, \ldots, s_m$ we have
\begin{equation*}
\sum_{j=1}^m s_j \vecnot{u}_j
= \sum_{j=1}^m s_j \sum_{i=1}^n a_{ij} \vecnot{w}_i
= \sum_{j=1}^m \sum_{i=1}^n ( a_{ij} s_j ) \vecnot{w}_i
= \sum_{i=1}^n \left( \sum_{j=1}^m a_{ij} s_j \right) \vecnot{w}_i . 
\end{equation*}
Collecting the $a_{ij}$ coefficients into an $n$ by $m$ matrix $A$ shows that
\begin{equation*}
\left[ \begin{array}{c} t_1 \\ \vdots \\ t_n \end{array} \right] = A \left[ \begin{array}{c} s_1 \\ \vdots \\ s_m \end{array} \right].
\end{equation*}
Since $n<m$, Lemma~\ref{lemma:widematrix_has_nullvector} implies that there exist scalars $s_1, \ldots, s_n$, not all $0$, such that $t_1 = t_2 = \cdots = t_m = 0$.
For these scalars, $\sum_{j=1}^m s_j \vecnot{u}_j = 0$.
That is, the set $U$ is linearly dependent.
\end{proof}

If $V$ is a finite-dimensional vector space, then any two bases of $V$ have the same number of elements.
Therefore, the dimension of a finite-dimensional vector space is uniquely defined.
While this may seem intuitively obvious to many readers, the previous theorem shows that this intuition from $\mathbb{R}^n$ does not break down for other fields and vector spaces.

\begin{definition}
The \defn{vector space}{dimension} of a finite-dimensional vector space is defined as the number of elements in any basis for $V$.
We denote the dimension of a finite-dimensional vector space $V$ by $\dim(V)$.
\end{definition}

The zero subspace of a vector space $V$ is the subspace spanned by the vector $\vecnot{0}$.
Since the set $\left\{ \vecnot{0} \right\}$ is linearly dependent and not a basis, we assign a dimension $0$ to the zero subspace.
Alternatively, it can be argue that the empty set $\emptyset$ spans $\left\{ \vecnot{0} \right\}$ because the intersection of all the subspaces containing the empty set is $\left\{ 0 \right\}$.
Though this is only a minor point.

\begin{theorem}
Let $A$ be an $n \times n$ matrix over $F$ and suppose that the columns of $A$, denoted by $A_1, \ldots, A_n$, form a linearly independent set of vectors in $F^{n \times 1}$.
Then $A$ is invertible.
\end{theorem}
\begin{proof}
Suppose that $W$ is the subspace of $F^{n \times 1}$ spanned by $A_1, \ldots, A_n$.
Since $A_1, \ldots, A_n$ are linearly independent, $\dim(W) = n = \dim(F^{n \times 1})$.
It follows that $W = V$ and, as a consequence, there exist scalars $b_{ij} \in F$ such that
\begin{equation*}
\vecnot{e}_j = \sum_{i=1}^n b_{ij} A_i, \quad 1 \leq j \leq n
\end{equation*}
where $\left\{ \vecnot{e}_1, \ldots, \vecnot{e}_n \right\}$ is the standard basis for $F^{n \times 1}$.
Then, for the matrix $B$ with entries $b_{ij}$, we have
\begin{equation*}
AB = I.
\end{equation*}
Note also that if the rows of $A$ form a linearly independent set of vectors in $F^{1 \times n}$ then $A$ is invertible.
\end{proof}


\subsection{Coordinate System}

Let $\left\{ \vecnot{v}_1, \ldots, \vecnot{v}_n \right\}$ be a basis for the $n$-dimensional vector space $V$.
Every vector $\vecnot{w} \in V$ can be expressed uniquely as
\begin{equation*}
\vecnot{w} = \sum_{i=1}^n s_i \vecnot{v}_i.
\end{equation*}
While standard vector and matrix notation requires that the basis elements be ordered, a set is an unordered collection of objects.
Ordering this set (e.g., $\vecnot{v}_1, \ldots, \vecnot{v}_n$) allows the first element in the coordinate vector to be associated with the first vector in our basis and so on.


\begin{definition}
If $V$ is a finite-dimensional vector space, an \defn{vector space}{ordered basis} for $V$ is a finite sequence of vectors which is linearly independent and spans $V$.
\end{definition}

In particular, if the sequence $\vecnot{v}_1, \ldots, \vecnot{v}_n$ is an ordered basis for $V$, then the set $\left\{ \vecnot{v}_1, \ldots, \vecnot{v}_n \right\}$ is a basis for $V$.
The ordered basis $\mathcal{B}$ is the set $\left\{ \vecnot{v}_1, \ldots, \vecnot{v}_n \right\}$, together with the specific ordering of the vectors.
Based on this ordered basis, a vector $\vecnot{w} \in V$ can be unambiguously represented as an $n$-tuple,
\begin{equation*}
\vecnot{w} = (s_1, \ldots, s_n) = \sum_{i=1}^n s_i \vecnot{v}_i.
\end{equation*}
Equivalently, vector $\vecnot{w}$ can be described using the \emph{coordinate matrix of $\vecnot{w}$ relative to the ordered basis $\mathcal{B}$}:
\begin{equation*}
\vecnot{w} = \left[ \begin{array}{c} s_1 \\ \vdots \\ s_n \end{array} \right].
\end{equation*}
The dependence of this coordinate matrix on $\mathcal{B}$ can be specified explicitly using the notation $\left[ \vecnot{w} \right]_{\mathcal{B}}$.
This will be particularly important when multiple coordinates systems are involved.

\begin{example}
The canonical example of an ordered basis is the standard basis for $F^n$ introduced in Section~\ref{section:BasesAndDimension}.
Note that the standard basis contains a natural ordering: $\vecnot{e}_1, \ldots, \vecnot{e}_n$.
Vectors in $F^n$ can therefore be unambiguously expressed as $n$-tuples.
\end{example}

\iffalse
Let $V$ be a finite-dimensional vector space and assume that
\begin{align*}
\mathcal{A} &= \vecnot{v}_1, \ldots, \vecnot{v}_n \\
\mathcal{B} &= \vecnot{w}_1, \ldots, \vecnot{w}_n
\end{align*}
are two ordered bases for $V$.
There are unique scalars $p_{ij}$ such that
\begin{equation*}
\vecnot{w}_j = \sum_{i=1}^n p_{ij} \vecnot{v}_i \quad 1 \leq j \leq n.
\end{equation*}
Let $\vecnot{u} \in V$ and
\begin{equation*}
\left[ \vecnot{u} \right]_{\mathcal{B}} = \left[ \begin{array}{c} t_1 \\ \vdots \\ t_n \end{array} \right].
\end{equation*}
Then, we can write
\begin{equation*}
\vecnot{u} = \sum_{j=1}^n t_j \vecnot{w}_j
= \sum_{j=1}^n t_j \sum_{i=1}^n p_{ij} \vecnot{v}_i
= \sum_{j=1}^n \sum_{i=1}^n ( p_{ij} t_j ) \vecnot{v}_i
= \sum_{i=1}^n \left( \sum_{j=1}^n p_{ij} t_j \right) \vecnot{v}_i.
\end{equation*}
Since the coordinates $s_1, \ldots, s_n$ of $\vecnot{u}$ in the ordered basis $\mathcal{A}$ are uniquely determined, it follows that
\begin{equation*}
s_i = \sum_{j=1}^n p_{ij} t_j \quad 1 \leq i \leq n.
\end{equation*}
Let $P$ be the $n \times n$ matrix with entries $p_{ij}$, then $P$ is invertible and
\begin{align*}
\left[ \vecnot{u} \right]_{\mathcal{A}} &= P \left[ \vecnot{u} \right]_{\mathcal{B}} \\
\left[ \vecnot{u} \right]_{\mathcal{B}} &= P^{-1} \left[ \vecnot{u} \right]_{\mathcal{A}}
\end{align*}
for every $\vecnot{u} \in V$.
Furthermore, the columns of $P$ are given by
\begin{equation*}
P_j = \left[ \vecnot{w}_j \right]_{\mathcal{A}} \quad 1 \leq j \leq n.
\end{equation*}
\fi

\begin{problem} \label{problem:OrderBasisFromMatrix}
Suppose that $\mathcal{A} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ is an ordered basis for $V$.
Let $P$ be an $n \times n$ invertible matrix.
Show that there exists an ordered basis $\mathcal{B} = \vecnot{w}_1, \ldots, \vecnot{w}_n$ for $V$ such that
\begin{align*}
\left[ \vecnot{u} \right]_{\mathcal{A}} &= P \left[ \vecnot{u} \right]_{\mathcal{B}} \\
\left[ \vecnot{u} \right]_{\mathcal{B}} &= P^{-1} \left[ \vecnot{u} \right]_{\mathcal{A}}
\end{align*}
for every $\vecnot{u} \in V$.
\end{problem}
\textbf{S~\ref{problem:OrderBasisFromMatrix}}.
Consider the ordered basis $\mathcal{A} = \vecnot{v}_1, \ldots, \vecnot{v}_n$ and let $Q = P^{-1}$.
For all $\vecnot{u} \in V$, we have $\vecnot{u} = \sum_{i=1}^n s_i \vecnot{v}_i$, where
\begin{equation*}
\left[ \vecnot{u} \right]_{\mathcal{A}} = \left[ \begin{array}{c} s_1 \\ \vdots \\ s_n \end{array} \right].
\end{equation*}
If we define
\begin{equation*}
\vecnot{w}_i = \sum_{k = 1}^n p_{ki} \vecnot{v}_k,
\end{equation*}
and
\begin{equation*}
t_i = \sum_{i=1} q_{ij} s_j,
\end{equation*}
then we find that
\begin{equation*}
\begin{split}
\sum_{i = 1}^n t_i \vecnot{w}_i
&= \sum_{i = 1}^n \sum_{j=1}^n q_{ij} s_j \vecnot{w}_i
= \sum_{i = 1}^n \sum_{j=1}^n q_{ij} s_j \sum_{k = 1}^n p_{ki} \vecnot{v}_k \\
& = \sum_{j=1}^n s_j \sum_{k=1}^n \vecnot{v}_k \sum_{i = 1}^n p_{ki} q_{ij}
= \sum_{j=1}^n s_j \sum_{k=1}^n \vecnot{v}_k \delta_{jk} \\
& = \sum_{j=1}^n s_j \vecnot{v}_j
= \vecnot{u}.
\end{split}
\end{equation*}
This shows that $\mathcal{B} = \vecnot{w}_1, \ldots, \vecnot{w}_n$ is an ordered basis for $V$ and
\begin{equation*}
\left[ \vecnot{u} \right]_{\mathcal{B}} = \left[ \begin{array}{c} t_1 \\ \vdots \\ t_n \end{array} \right].
\end{equation*}
The definition of $t_i$ also shows that $\left[ \vecnot{u} \right]_{\mathcal{B}} = P^{-1} \left[ \vecnot{u} \right]_{\mathcal{A}}$ and therefore $\left[ \vecnot{u} \right]_{\mathcal{A}} = P \left[ \vecnot{u} \right]_{\mathcal{B}}$.



\section{Norms}
Let $V$ be a vector space over the real numbers or the complex numbers.

A \defn{vector space}{norm} on vector space $V$ is a real-valued function $\left\| \cdot \right\| : V \rightarrow \RealNumbers$ that satisfies the following properties.
\begin{enumerate}
\item $\left\| \vecnot{v} \right\| \geq 0 \quad \forall \vecnot{v} \in V$;
equality holds if and only if $\vecnot{v} = \vecnot{0}$
\item $\left\| s \vecnot{v} \right\| = |s| \left\| \vecnot{v} \right\| \quad \forall \vecnot{v} \in V, s \in F$
\item $\left\| \vecnot{v} + \vecnot{w} \right\| \leq
\left\| \vecnot{v} \right\| + \left\| \vecnot{w} \right\| \quad \forall \vecnot{v}, \vecnot{w} \in V$.
\end{enumerate}

The concept of a norm is closely related to the concept of a metric.
For instance, a metric can be defined in terms of a norm.
Let $\left\| \vecnot{v} \right\|$ be a norm on vector space $V$, then
\begin{equation*}
d \left( \vecnot{v}, \vecnot{w} \right)
= \left\| \vecnot{v} - \vecnot{w} \right\|
\end{equation*}
is the metric induced by the norm.

Normed vector spaces are very useful because they have all the properties of a vector space and all the benefits of a topology generated by the norm.
Therefore, one can discuss limits and convergence in a meaningful way.

\begin{example}
Consider vectors in $\RealNumbers^n$ with the euclidean metric
\begin{equation*}
d \left( \vecnot{v}, \vecnot{w} \right)
= \sqrt{ (v_1 - w_1)^2 + \cdots + (v_n - w_n)^2 }.
\end{equation*}
Recall that the standard bounded metric introduced in Problem~\ref{problem:StandardBoundedMetric} is given by
\begin{equation*}
\bar{d} \left( \vecnot{v}, \vecnot{w} \right)
= \min \left\{ d \left( \vecnot{v}, \vecnot{w} \right), 1 \right\}.
\end{equation*}
Define the function $f : \RealNumbers^n \rightarrow \RealNumbers$ by
\begin{equation*}
f \left( \vecnot{v} \right) = \bar{d} \left( \vecnot{v}, \vecnot{0} \right).
\end{equation*}
Is the function $f$ a norm?

By the properties of a metric, we have
\begin{enumerate}
\item $\bar{d} \left( \vecnot{v}, \vecnot{0} \right) \geq 0 \quad \forall \vecnot{v} \in V$; equality holds if and only if $\vecnot{v} = \vecnot{0}$
\item $\bar{d} \left( \vecnot{v}, \vecnot{0} \right) + \bar{d} \left( \vecnot{w}, \vecnot{0} \right) = \bar{d} \left( \vecnot{v}, \vecnot{0} \right) + \bar{d} \left( \vecnot{0}, \vecnot{w} \right) \geq \bar{d} \left( \vecnot{v}, \vecnot{w} \right) \quad \forall \vecnot{v}, \vecnot{w} \in V$.
\end{enumerate}
However, $\bar{d} \left( s \vecnot{v}, \vecnot{0} \right)$ is not necessarily equal to $s \bar{d} \left( \vecnot{v}, \vecnot{0} \right)$.
For instance,
$\bar{d} \left( 2 \vecnot{e}_1, \vecnot{0} \right) = 1 < 2 \bar{d} \left( \vecnot{e}_1, \vecnot{0} \right)$.
Thus, the function $f : \RealNumbers^n \rightarrow \RealNumbers$ defined by
\begin{equation*}
f \left( \vecnot{v} \right) = \bar{d} \left( \vecnot{v}, \vecnot{0} \right).
\end{equation*}
is not a norm.
\end{example}

\begin{example}
The following functions are examples of norms for $F^n$,
\begin{enumerate}
\item the $l_1$ norm: $\left\| \vecnot{v} \right\|_1 = \sum_{i=1}^n |x_i|$
\item the $l_p$ norm: $\left\| \vecnot{v} \right\|_p = \left( \sum_{i=1}^n |x_i|^p \right)^{\frac{1}{p}}, \quad p \in (1,\infty)$
\item the $l_{\infty}$ norm: $\left\| \vecnot{v} \right\|_{\infty} = \max_{1,\ldots, n} \{ |x_i| \}$.
\end{enumerate}
\end{example}

\begin{example}
Similarly, norms can be defined for the vector space of functions from $[a, b]$ to $\RealNumbers$ (or $\ComplexNumbers$) with
\begin{enumerate}
\item the $L_1$ norm: $\left\| f(t) \right\|_1 = \int_a^b |f(t)| dt$
\item the $L_p$ norm: $\left\| f(t) \right\|_p = \left( \int_a^b |f(t)|^p dt \right)^{\frac{1}{p}}, \quad p \in (1,\infty)$
\item the $L_{\infty}$ norm: $\left\| f(t) \right\|_{\infty} = \esssup_{[a,b]} \{ | f(t) | \}$.
\end{enumerate}
\end{example}

\index{integral}
In this example, the integral notation refers to the \defn{integral}{Lebesgue integral} (rather than the \defn{integral}{Riemann integral}).
The reason for this is that many important spaces include functions that are not Riemann integrable.
The Lebesgue integral is defined using measure theory and is often used in advanced probability courses.
Since there are many non-zero Lebesgue-integrable functions whose integral is zero, this definition has a subtlety.
The norm of a function is zero if and only if it is zero \defn{integral}{almost everywhere} (abbreviated a.e.).
Therefore, two functions are \emph{equal almost everywhere} if the norm of their difference is zero. 
Strictly speaking, a vector space of ``functions" with the $L_p$ norm actually has elements that are equivalence classes of functions defined by \emph{equality almost everywhere}.

\begin{definition}
A vector $\vecnot{v} \in V$ is said to be \defn{vector space}{normalized} if $\left\| \vecnot{v} \right\| = 1$.
Any vector can be normalized, except the zero vector:
\begin{equation}
\vecnot{u} = \frac{\vecnot{v}}{\left\| \vecnot{v} \right\|}
\end{equation}
has norm $\left\| \vecnot{u} \right\| = 1$.
A normalized vector is also referred to as a \defn{vector space}{unit vector}.
\end{definition}

\begin{definition}
A complete normed vector space is called a \defn{vector space}{Banach space}.
\end{definition}

Banach spaces are the standard setting for many problems because completeness is a powerful tool for solving problems.

\begin{example}
The vector spaces $\mathbb{R}^n$ (or $\mathbb{C}^n$) with any well-defined norm are Banach spaces.
\end{example}

\begin{example}
The vector space of all continuous functions from $[a,b]$ to $\mathbb{R}$ is a Banach space under the supremum norm
\[ \left\| f(t) \right\| = \sup_{t\in [a,b]} f(t). \]
\end{example}

\begin{definition}
A Banach space $V$ has a \defn{Banach space}{Schauder basis}, $\vecnot{v}_1, \vecnot{v}_2, \ldots$, if every $\vecnot{v} \in V$ can be written uniquely as
\[ \vecnot{v} = \sum_{i\in \mathbb{N}} s_i \vecnot{v}_i. \]
\end{definition}

\begin{example}
Let $V = \mathbb{R}^\omega$ be the vector space of semi-infinite real sequences.
The \defn{Banach space}{standard Schauder basis} is the countably infinite extension $\{\vecnot{e}_1 ,\vecnot{e}_2, \ldots \}$ of the standard basis.
\end{example}

\begin{definition}
A \defn{Banach space}{closed subspace} of a Banach space is a subspace that is a closed set in the topology generated by the norm.
\end{definition}

\begin{theorem}
All finite dimensional subspaces of a Banach space are closed.
\end{theorem}
\begin{proof}
This proof requires material from later in the notes, but is given here for completeness.
Let $\vecnot{w}_1,\vecnot{w}_2,\ldots,\vecnot{w}_n$ be a basis for a finite dimensional subspace $W$ of a Banach space $V$ over $F$.
Let $U = F^n$ be the standard Banach space, which is closed by definition, and consider the mapping $f: U\rightarrow W$ defined by
\[ f(\vecnot{s}) = \sum_{i=1}^n s_i \vecnot{w}_i. \]
It is easy to verify that this linear mapping is non-singular and onto.
Therefore, it has a linear inverse mapping $g = f^{-1}$ that must be continuous (i.e., bounded) because $U,W$ are finite dimensional.
Since $g$ is continuous, we find that $W = g^{-1}(U) = f(U)$ is closed because $U$ is closed.
\end{proof}

\begin{example}
Let $V = L_p ( [a,b] )$, for $1\leq p <\infty$, be the set of real Lebesgue-integrable functions on $[a,b]$.
We say that $f \in V$ is continuous if the equivalence class generated by equality almost everywhere contains a continuous function.
It is easy to verify that the subset $W \subset V$ of continuous functions is a subspace.
It is not closed, however, because sequences in $W$ may converge to discontinuous functions.
More generally, the span of any infinite set of linearly independent vectors only includes finite linear combinations and is therefore not closed.
%In fact, the set of continuous function is dense in $L_p ([a,b])$ so that $\Closure{W} = V$.
\end{example}

\section{Functionals and Optimization}

Functions mapping elements of a vector space (over $F$) down to the scalar field $F$ play a very special role in the analysis of vector spaces.

\begin{definition}
Let $V$ be a vector space over $F$.
Then, a \defn{vector space}{functional} on $V$ is a function $f : V \rightarrow F$ that maps $V$ to $F$.
\end{definition}

Later, we will see that linear functionals (i.e., functionals that are linear) are used to define many important concepts.
For unconstrained optimization, however, linear functionals are not interesting because they are either zero or achieve all values in $F$.
Instead, this section focuses on a class of functionals (called convex) which have well-defined minimum values.

\begin{definition}
Let $(X,\|\cdot\|)$ be a normed vector space.
Then, a functional $f: X \rightarrow \RealNumbers$ achieves a \defn{functions}{local minimum value} at $\vecnot{x}_0 \in X$ if there is an $\epsilon > 0$ such that, for all $\vecnot{x}\in X$ satisfying $\| \vecnot{x} - \vecnot{x}_0 \| < \epsilon$, we have  $f(\vecnot{x}) \geq f(\vecnot{x}_0)$.
If the bound holds for all $x\in X$ instead, then the local minimum is also a \defn{functions}{global minimum value}.
\end{definition}

\begin{definition}
Let $V$ be a vector space over the real numbers.
The subset $A \subseteq V$ is called a \defn{vector space}{convex set} if, for all $\vecnot{a}_1,\vecnot{a}_2 \in A$ and $\lambda\in[0,1]$, we have $\lambda \vecnot{a}_1 + (1-\lambda) \vecnot{a}_2 \in A$.
The set is \textbf{strictly convex} if, for all $\vecnot{a}_1,\vecnot{a}_2 \in \Closure{A}$ and $\lambda\in(0,1)$, we have $\lambda \vecnot{a}_1 + (1-\lambda) \vecnot{a}_2 \in \Interior{A}$.
\end{definition}

\begin{definition}
A Banach space $X$ is called \defn{Banach space}{strictly convex} if the unit ball,  given by $\{ x\in X | \, \| x \| \leq 1 \}$, is a strictly convex set.
An equivalent condition is that equality in the triangle inequality (i.e., $\| \vecnot{x} + \vecnot{y} \| = \| \vecnot{x} \| + \| \vecnot{y} \|$) for non-zero vectors implies that $ \vecnot{x} = s \vecnot{y} $ for some $s\in F$.
\end{definition}

\begin{definition}
Let $V$ be a vector space, $A \subseteq V$ be a convex set, and $f: V \rightarrow \RealNumbers$ be a functional.
Then, the functional $f$ is called \defn{functions}{convex} on $A$ if, for all $\vecnot{a}_1,\vecnot{a}_2 \in A$ and $\lambda\in(0,1)$, we have
\[ f( \lambda \vecnot{a}_1 + (1-\lambda) \vecnot{a}_2 ) \leq \lambda f( \vecnot{a}_1 ) + (1-\lambda) f ( \vecnot{a}_2 ). \]
The functional is \textbf{strictly convex} if equality occurs only when $\vecnot{a}_1 = \vecnot{a}_2$.
\end{definition}

\begin{example}
Let $(X,\|\cdot\|)$ be a normed vector space.
Then, the norm $\| \cdot \| : X \rightarrow \mathbb{R}$ is a convex functional on $X$.
Try proving this.
\end{example}

\begin{theorem}
Let $(X,\|\cdot\|)$ be a normed vector space, $A \subseteq X$ be a convex set, and $f: X \rightarrow \RealNumbers$ be a convex functional on $A$.
Then, any local minimum value of $f$ on $A$ is a global minimum value on $A$.
If the functional is strictly convex on $A$ and achieves a local minimum value on $A$, then there is a unique point $\vecnot{x}_0 \in A$ that achieves the global minimum value on $A$.
\end{theorem}
\begin{proof}
Let $\vecnot{x}_0 \in A$ a point where the functional achieves a local minimum value.
Proving by contradiction, we suppose that there is another point $\vecnot{x}_1 \in A$ such that $f(\vecnot{x}_1) <f(\vecnot{x}_0)$.
From the definition of a local minimum value, we find an $\epsilon > 0$ such that $f(\vecnot{x}) \geq f(\vecnot{x}_0)$ for all $\vecnot{x} \in A$ satisfying $\| \vecnot{x} - \vecnot{x}_0 \| < \epsilon$.
Choosing $\lambda < \frac{\epsilon}{\smash{\| \vecnot{x}_0 - \vecnot{x}_1 \|}}$ in $(0,1)$ and $\vecnot{x} = (1-\lambda) \vecnot{x}_0 + \lambda \vecnot{x}_1$ implies that $\| \vecnot{x} - \vecnot{x}_0 \| < \epsilon $
while the convexity of $f$ implies that
\[ f( \vecnot{x} ) = f \left( (1-\lambda) \vecnot{x}_0 + \lambda \vecnot{x}_1 \right) \leq (1-\lambda) f(\vecnot{x}_0) + \lambda f(\vecnot{x}_1) < f(\vecnot{x}_0). \]
This contradicts the definition of a local minimum value and implies that $f(\vecnot{x}_0)$ is a global minimum value on $A$.
If $f$ is strictly convex and $f(\vecnot{x}_1) = f(\vecnot{x}_0)$, then we suppose that $\vecnot{x}_0 \neq \vecnot{x}_1$.
In this case, strict convexity implies that
\[ f \left( (1-\lambda) \vecnot{x}_0 + \lambda \vecnot{x}_1 \right) < (1-\lambda) f(\vecnot{x}_0) + \lambda f(\vecnot{x}_1) = f(\vecnot{x}_0). \]
This contradicts the fact that $f(\vecnot{x}_0)$ is a global minimum value on $A$ and implies that $\vecnot{x}_0 = \vecnot{x}_1$ is unique.
\end{proof}

\section{Inner Products}

\begin{definition} \label{definition:InnerProduct}
Let $F$ be the field of real numbers or the field of complex numbers, and assume $V$ is a vector space over $F$.
An \defn{inner-product space}{inner product} on $V$ is a function which assigns to each ordered pair of vectors $\vecnot{v}, \vecnot{w} \in V$ a scalar $\left\langle \vecnot{v} | \vecnot{w} \right\rangle \in F$ in such a way that for all $\vecnot{u}, \vecnot{v}, \vecnot{w} \in V$ and any scalar $s \in F$
\begin{enumerate}
\item $\left\langle \vecnot{u} + \vecnot{v} | \vecnot{w} \right\rangle
= \left\langle \vecnot{u} | \vecnot{w} \right\rangle
+ \left\langle \vecnot{v} | \vecnot{w} \right\rangle$
\item $\left\langle s \vecnot{v} | \vecnot{w} \right\rangle
= s \left\langle \vecnot{v} | \vecnot{w} \right\rangle$
\item $\left\langle \vecnot{v} | \vecnot{w} \right\rangle
= \overline{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle }$, where the overbar denotes complex conjugation;
\item $\left\langle \vecnot{v} | \vecnot{v} \right\rangle \geq 0$ with equality iff $\vecnot{v} = \vecnot{0}$.
\end{enumerate}
\end{definition}

Note that the conditions of Definition~\ref{definition:InnerProduct} imply that
\begin{equation*}
\left\langle \vecnot{u} | s \vecnot{v} + \vecnot{w} \right\rangle
= \overline{s} \left\langle \vecnot{u} | \vecnot{v} \right\rangle
+ \left\langle \vecnot{u} | \vecnot{w} \right\rangle .
\end{equation*}

\index{inner-product space}
\begin{definition}
A real or complex vector space equipped with an inner product is called an \textbf{inner-product space}.
\end{definition}

\begin{example} \label{example:StandardInnerProduct}
Consider the inner product on $F^n$ defined by
\begin{equation*}
\left\langle \vecnot{v} | \vecnot{w} \right\rangle
= \left\langle (v_1, \ldots, v_n) | (w_1, \ldots, w_n) \right\rangle
= \sum_{j=1}^n v_j \overline{w}_j.
\end{equation*}
This inner product is called the \defn{inner-product space}{standard inner product}.
When $F = \RealNumbers$, the standard inner product can also be written as
\begin{equation*}
\left\langle \vecnot{v} | \vecnot{w} \right\rangle
= \sum_{j=1}^n v_j w_j.
\end{equation*}
In this context it is often called the dot product, denoted by $\vecnot{v} \cdot \vecnot{w}$.
In either case, it can also be written in terms of the Hermitian transpose as $ \left\langle \vecnot{v} | \vecnot{w} \right\rangle = \vecnot{w}^H \vecnot{v} $.

\end{example}

\begin{problem} \label{problem:InnerProductForm}
For $\vecnot{v} = (v_1, v_2)$ and $\vecnot{w} = (w_1, w_2)$  in $\RealNumbers^2$, show that
\begin{equation*}
\left\langle \vecnot{v} | \vecnot{w} \right\rangle
= v_1 w_1 - v_2 w_1 - v_1 w_2 + 4 v_2 w_2
\end{equation*}
is an inner product.
\end{problem}
\noindent \textbf{S~\ref{problem:InnerProductForm}}.
For all $\vecnot{u}, \vecnot{v}, \vecnot{w} \in V$ and all scalars $s$
\begin{equation*}
\begin{split}
\left\langle \vecnot{u} + \vecnot{v} | \vecnot{w} \right\rangle
&= (u_1 + v_1) w_1 - (u_2 + v_2) w_1 - (u_1 + v_1) w_2 + 4 (u_2 + v_2) w_2 \\
&= u_1 w_1 - u_2 w_1 - u_1 w_2 + 4 u_2 w_2
+ v_1 w_1 - v_2 w_1 - v_1 w_2 + 4 v_2 w_2 \\
&= \left\langle \vecnot{u} | \vecnot{w} \right\rangle
+ \left\langle \vecnot{v} | \vecnot{w} \right\rangle.
\end{split}
\end{equation*}
Also, we have
\begin{equation*}
\left\langle s \vecnot{v} | \vecnot{w} \right\rangle
= s v_1 w_1 - s v_2 w_1 - s v_1 w_2 + 4 s v_2 w_2
= s \left\langle \vecnot{v} | \vecnot{w} \right\rangle.
\end{equation*}
Since $V = \RealNumbers^2$, we have $\left\langle \vecnot{v} | \vecnot{w} \right\rangle = \overline{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle }$.
Furthermore,
\begin{equation*}
\left\langle \vecnot{v} | \vecnot{v} \right\rangle
= v_1^2 - 2 v_1 v_2 + 4 v_2^2
= ( v_1 - v_2 )^2 + 3 v_2^2
\geq 0
\quad \text{with equality iff } \vecnot{v} = \vecnot{0}.
\end{equation*}
That is, $\left\langle \vecnot{v} | \vecnot{v} \right\rangle$ is an inner product.


\begin{example}
Let $V$ be the vector space of all continuous complex-valued functions on the unit interval $[0,1]$.
Then
\begin{equation*}
\left\langle f | g \right\rangle
= \int_0^1 f(t) \overline{g(t)} dt
\end{equation*}
is an inner product.
\end{example}

\begin{example}
Let $V$ and $W$ be two vector spaces over $F$ and suppose that $\langle \cdot | \cdot \rangle$ is an inner product on $W$.
If $T$ is a non-singular linear transformation from $V$ into $W$, then the equation
\begin{equation*}
p_T \left( \vecnot{v}_1, \vecnot{v}_2 \right)
= \left\langle T \vecnot{v}_1 | T \vecnot{v}_2 \right\rangle
\end{equation*}
defines an inner product $p_T$ on $V$.
\end{example}

\begin{theorem}
Let $V$ be a finite-dimensional space, and suppose that
\begin{equation*}
\mathcal{B} = \vecnot{w}_1, \ldots, \vecnot{w}_n
\end{equation*}
is an ordered basis for $V$.
Any inner product on $V$ is completely determined by the values
\begin{equation*}
h_{ji} = \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle
\end{equation*}
it assumes on pairs of vectors in $\mathcal{B}$.
\end{theorem}
\begin{proof}
If $\vecnot{u} = \sum_{i} s_i \vecnot{w}_i$ and $\vecnot{v} = \sum_{j} t_j \vecnot{w}_j$, then
\begin{equation*}
\begin{split}
\left\langle \vecnot{u} | \vecnot{v} \right\rangle
&= \left\langle \sum_{i} s_i \vecnot{w}_i \Big| \vecnot{v} \right\rangle
= \sum_{i} s_i \left\langle \vecnot{w}_i | \vecnot{v} \right\rangle \\
&= \sum_{i} s_i \left\langle \vecnot{w}_i \Big| \sum_{j} t_j \vecnot{w}_j \right\rangle
= \sum_{i} \sum_{j} s_i \overline{t}_j \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle \\
&= \sum_{i} \sum_{j} \overline{t}_j h_{ji} s_i
= \left[ \vecnot{v} \right]_{\mathcal{B}}^H H \left[ \vecnot{u} \right]_{\mathcal{B}}
\end{split}
\end{equation*}
where $\left[ \vecnot{u} \right]_{\mathcal{B}}$ and $\left[ \vecnot{v} \right]_{\mathcal{B}}$ are the coordinate matrices of $\vecnot{u}$, $\vecnot{v}$ in the ordered basis $\mathcal{B}$.
The matrix $H$ is called the \emph{matrix of the inner product in the ordered basis $\mathcal{B}$}.
\end{proof}

It is easily verified that $H$ is a Hermitian matrix, i.e., $H = H^H$.
Furthermore, $H$ must satisfy the additional condition
\begin{equation} \label{equation:PositiveDefinite}
\vecnot{w}^H H \vecnot{w} > 0, \quad \forall \vecnot{w} \neq \vecnot{0}.
\end{equation}
In particular, $H$ must be invertible.

Conversely if $H$ is an $n \times n$ Hermitian matrix over $F$ which satisfies~\eqref{equation:PositiveDefinite}, then $H$ is the matrix in the ordered basis $\mathcal{B}$ of an inner product on $V$.
This inner product is given by
\begin{equation*}
\left\langle \vecnot{u} | \vecnot{v} \right\rangle
= \left[ \vecnot{v} \right]_{\mathcal{B}}^H H \left[ \vecnot{u} \right]_{\mathcal{B}}.
\end{equation*}

\begin{problem}
Let $V$ be a vector space over $F$.
Show that the sum of two inner products on $V$ is an inner product on $V$.
Show that a positive multiple of an inner product is also an inner product.
\end{problem}

\begin{example}
Consider any set $W$ of real-valued random variables, defined on a common probability space, that have finite 2nd moments.
It turns out that $V = \Span (W)$ is a vector space over $\RealNumbers$.
In fact, one can define the inner product
\begin{equation*}
\left\langle X | Y \right\rangle = \Expect \left[ XY \right] ,
\end{equation*}
for any $X,Y \in V$.
Using the induced norm, this inner product provides the topology of mean-square convergence and two random variables $X,Y\in V$ are considered equal if $\| X-Y \|^2 = E \left[ |X-Y|^2 \right] = 0$ (or equivalently $\Pr (X \neq Y ) = 0$).
\end{example}


\subsection{Induced Norms}

A finite-dimensional real inner-product space is often referred to as a \defn{inner-product space}{Euclidean space}.
A complex inner-product space is sometimes called a unitary space.

\begin{definition}
Let $V$ be an inner-product space with inner product $\langle \cdot | \cdot \rangle$.
This inner product can be used to define a norm, called the \defn{inner-product space}{induced norm},
\begin{equation*}
\left\| \vecnot{v} \right\| = \left\langle \vecnot{v} | \vecnot{v} \right\rangle^{\frac{1}{2}}
\end{equation*}
for every $\vecnot{v} \in V$.
\end{definition}

\begin{theorem}
If $V$ is an inner-product space and $\| \cdot \|$ is its associated induced norm, then for any $\vecnot{v}, \vecnot{w} \in V$ and any scalar $s$
\begin{enumerate}
\item $\left\| s \vecnot{v} \right\| = |s| \left\| \vecnot{v} \right\|$
\item $\left\| \vecnot{v} \right\| > 0$ for $\vecnot{v} \neq \vecnot{0}$
\item $\left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right| \leq \left\| \vecnot{v} \right\| \left\| \vecnot{w} \right\|$ with equality iff $\vecnot{v} = \vecnot{0}$, $\vecnot{w}=\vecnot{0}$, or $\vecnot{v} = s \vecnot{w}$
\item $\left\| \vecnot{v} + \vecnot{w} \right\| \leq \left\| \vecnot{v} \right\| + \left\| \vecnot{w} \right\|$ with equality iff $\vecnot{v} = \vecnot{0}$, $\vecnot{w}=\vecnot{0}$, or $\vecnot{v} = s \vecnot{w}$.
\end{enumerate}
\end{theorem}
\begin{proof}
The first two items follow immediately from the definitions involved.
The third inequality, $\left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right| \leq \left\| \vecnot{v} \right\| \left\| \vecnot{w} \right\|$, is called the \defn{inner-product space}{Cauchy-Schwarz inequality}.
When $\vecnot{v} = \vecnot{0}$, then clearly $\left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right| = \left\| \vecnot{v} \right\| \left\| \vecnot{w} \right\| =0$.
Assume $\vecnot{v} \neq \vecnot{0}$ and let
\begin{equation*}
\vecnot{u} = \vecnot{w} - \frac{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle}{ \left\| \vecnot{v} \right\|^2 } \vecnot{v}.
\end{equation*}
Then $\left\langle \vecnot{u} | \vecnot{v} \right\rangle = 0$ and
\begin{equation*}
\begin{split}
0 &\leq \left\| \vecnot{u} \right\|^2
= \left\langle \vecnot{w} - \frac{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle}{ \left\| \vecnot{v} \right\|^2 } \vecnot{v} \Big|
\vecnot{w} - \frac{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle}{ \left\| \vecnot{v} \right\|^2 } \vecnot{v} \right\rangle \\
&= \left\langle \vecnot{w} | \vecnot{w} \right\rangle
- \frac{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle
\left\langle \vecnot{v} | \vecnot{w} \right\rangle }
{ \left\| \vecnot{v} \right\|^2 }
= \left\| \vecnot{w} \right\|^2
- \frac{ \left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right|^2 }
{ \left\| \vecnot{v} \right\|^2 } .
\end{split}
\end{equation*}
Hence
$\left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right|^2 \leq \left\| \vecnot{v} \right\|^2 \left\| \vecnot{w} \right\|^2$.
Notice that equality occurs iff $\vecnot{u}=\vecnot{0}$, or equivalently iff $\vecnot{w}=\vecnot{0}$ or $\vecnot{v} = s \vecnot{w}$.

Using this result, we get
\begin{equation*}
\begin{split}
\left\| \vecnot{v} + \vecnot{w} \right\|^2
&= \left\| \vecnot{v} \right\|^2 + \left\langle \vecnot{v} | \vecnot{w} \right\rangle + \left\langle \vecnot{w} | \vecnot{v} \right\rangle + \left\| \vecnot{w} \right\|^2 \\
&= \left\| \vecnot{v} \right\|^2 + 2 \Real \left\langle \vecnot{v} | \vecnot{w} \right\rangle + \left\| \vecnot{w} \right\|^2 \\
&\leq \left\| \vecnot{v} \right\|^2 + 2 \left\| \vecnot{v} \right\| \left\| \vecnot{w} \right\| + \left\| \vecnot{w} \right\|^2 ,
\end{split}
\end{equation*}
with equality iff Cauchy-Schwarz holds with equality.
Thus, $\left\| \vecnot{v} + \vecnot{w} \right\| \leq \left\| \vecnot{v} \right\| + \left\| \vecnot{w} \right\|$ with equality iff $\vecnot{v} = \vecnot{0}$, $\vecnot{w}=\vecnot{0}$, or $\vecnot{v} = s \vecnot{w}$ (i.e., $\vecnot{v}$ and $\vecnot{w}$ are linearly dependent).
\end{proof}

%Its proof shows that if $\vecnot{v}$ is non-zero, then $\left| \left\langle \vecnot{v} | \vecnot{w} \right\rangle \right| < \left\| \vecnot{v} \right\| \left\| \vecnot{w} \right\|$ unless
%\begin{equation*}
%\vecnot{w} = \frac{ \left\langle \vecnot{w} | \vecnot{v} \right\rangle }
%{ \left\| \vecnot{v} \right\| } \vecnot{v} .
%\end{equation*}
%That is, equality holds if and only if $\vecnot{v}$ and $\vecnot{w}$ are linearly dependent.

\begin{theorem}
\label{theorem:InnerProductContinuous}
Consider the vector space $\RealNumbers^n$ with the standard inner product of Example~\ref{example:StandardInnerProduct}.
The function $f: V \rightarrow F$ defined by $f \left( \vecnot{w} \right) = \left\langle \vecnot{w} | \vecnot{v} \right\rangle$ is continuous.
\end{theorem}
\begin{proof}
Let $\vecnot{w}_1, \vecnot{w}_2, \ldots$ be a sequence in $V$ converging to $\vecnot{w}$.
Then,
\begin{equation*}
\left| \left\langle \vecnot{w}_n | \vecnot{v} \right\rangle
- \left\langle \vecnot{w} | \vecnot{v} \right\rangle \right|
= \left| \left\langle \vecnot{w}_n - \vecnot{w} | \vecnot{v} \right\rangle \right|
\leq \left\| \vecnot{w}_n - \vecnot{w} \right\| \left\| \vecnot{v} \right\|.
\end{equation*}
Since $\left\| \vecnot{w}_n - \vecnot{w} \right\| \rightarrow 0$, the convergence of $\left\langle \vecnot{w}_n, \vecnot{v} \right\rangle$ is established.
\end{proof}


\section{Orthogonal Vectors and Subspaces}

\begin{definition}
Let $\vecnot{v}$ and $\vecnot{w}$ be vectors in an inner-product space $V$.
Then $\vecnot{v}$ is \defn{inner-product space}{orthogonal} to $\vecnot{w}$ (denoted $\vecnot{v} \bot \vecnot{w}$) if $\left\langle \vecnot{v} | \vecnot{w} \right\rangle = 0$.
Since this relation is reflexive and $\vecnot{w}$ is also orthogonal to $\vecnot{v}$, we simply say that $\vecnot{v}$ and $\vecnot{w}$ are orthogonal.
\end{definition}

\begin{definition}
Let $V$ be an inner-product space and $U,W$ be subspaces.
Then, the subspace $U$ is an \defn{inner-product space}{orthogonal} to the subspace $W$ (denoted $U \bot W$) if $\vecnot{u} \bot \vecnot{w}$ for all $\vecnot{u}\in U$ and $\vecnot{w}\in W$.
\end{definition}

\begin{definition}
A collection $W$ of vectors in $V$ is an \defn{inner-product space}{orthogonal set} if all pairs of distinct vectors in $W$ are orthogonal.
\end{definition}

\begin{example}
The standard basis of $\RealNumbers^n$ is an orthonormal set with respect to the standard inner product.
\end{example}

\begin{example}
Let $V$ be the vector space (over $\ComplexNumbers$) of continuous complex-valued functions on the interval $0 \leq x \leq 1$ with the inner product
\begin{equation*}
\left\langle f | g \right\rangle = \int_0^1 f(x) \overline{g(x)} dx.
\end{equation*}
Let $f_n(x) = \sqrt{2} \cos 2 \pi n x$ and $g_n (x) = \sqrt{2} \sin 2 \pi n x$.
Then $\{ 1, f_1, g_1, f_2, g_2, \ldots \}$ is a countably infinite orthonormal set that is a Schauder basis for this vector space.
\end{example}

\begin{theorem}
An orthogonal set of non-zero vectors is linearly independent.
\end{theorem}
\begin{proof}
Let $W$ be an orthogonal set of non-zero vectors in a given inner-product space $V$.
Suppose $\vecnot{w}_1, \ldots, \vecnot{w}_n$ are distinct vectors in $W$ and consider
\begin{equation*}
\vecnot{v} = s_1 \vecnot{w}_1 + \cdots + s_n \vecnot{w}_n.
\end{equation*}
The inner product $\left\langle \vecnot{v} | \vecnot{w}_i \right\rangle$ is given by
\begin{equation*}
\begin{split}
\left\langle \vecnot{v} | \vecnot{w}_i \right\rangle
&= \left\langle \sum_j s_j \vecnot{w}_j | \vecnot{w}_i \right\rangle
= \sum_j s_j \left\langle \vecnot{w}_j | \vecnot{w}_i \right\rangle
= s_i \left\langle \vecnot{w}_i | \vecnot{w}_i \right\rangle .
\end{split}
\end{equation*}
Since $\left\langle \vecnot{w}_i | \vecnot{w}_i \right\rangle \neq 0$, it follows that
\begin{equation*}
s_i = \frac{ \left\langle \vecnot{v} | \vecnot{w}_i \right\rangle }
{ \left\| \vecnot{w}_i \right\|^2 }
\quad 1 \leq i \leq n.
\end{equation*}
In particular, if $\vecnot{v} = 0$ then $s_j = 0$ for $1 \leq j \leq n$ and the vectors in $W$ are linearly independent.
\end{proof}

\begin{corollary}
If $\vecnot{v} \in V$ is a linear combination of an orthogonal sequence of distinct, non-zero vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$, then $\vecnot{v}$ is the particular linear combination
\begin{equation*}
\vecnot{v} = \sum_{i = 1}^n \frac{ \left\langle \vecnot{v} | \vecnot{w}_i \right\rangle } { \left\| \vecnot{w}_i \right\|^2 } \vecnot{w}_i.
\end{equation*}
\end{corollary}

\begin{theorem}
Let $V$ be an inner-product space and assume $\vecnot{v}_1, \ldots, \vecnot{v}_n$ are linearly independent vectors in $V$.
Then it is possible to construct an orthogonal sequence of vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n \in V$ such that for each $k = 1, \ldots, n$ the set
\begin{equation*}
\left\{ \vecnot{w}_1, \ldots, \vecnot{w}_k \right\}
\end{equation*}
is a basis for the subspace spanned by $\vecnot{v}_1, \ldots, \vecnot{v}_k$.
\end{theorem}
\begin{proof}
First let $\vecnot{w}_1 = \vecnot{v}_1$.
Define the remaining vectors inductively as follows.
Suppose the vectors
\begin{equation*}
\vecnot{w}_1, \ldots, \vecnot{w}_m \quad (1 \leq m < n)
\end{equation*}
have been chosen so that for every $k$
\begin{equation*}
\left\{ \vecnot{w}_1, \ldots, \vecnot{w}_k \right\} \quad 1 \leq k \leq m
\end{equation*}
is an orthogonal basis for the subspace spanned by $\vecnot{v}_1, \ldots, \vecnot{v}_k$.
Let
\begin{equation*}
\vecnot{w}_{m+1} = \vecnot{v}_{m+1} - \sum_{i=1}^m \frac{ \left\langle \vecnot{v}_{m+1} | \vecnot{w}_i \right\rangle } { \left\| \vecnot{w}_i \right\|^2 } \vecnot{w}_i.
\end{equation*}
Then $\vecnot{w}_{m+1} \neq 0$, for otherwise $\vecnot{v}_{m+1}$ is a linear combination of $\vecnot{w}_1, \ldots, \vecnot{w}_m$ and hence a linear combination of $\vecnot{v}_1, \ldots, \vecnot{v}_m$.
Furthermore, for $j \in 1, \ldots, m$
\begin{equation*}
\begin{split}
\left\langle \vecnot{w}_{m+1} | \vecnot{w}_j \right\rangle
&= \left\langle \vecnot{v}_{m+1} | \vecnot{w}_j \right\rangle
- \sum_{i=1}^m \frac{ \left\langle \vecnot{v}_{m+1} | \vecnot{w}_i \right\rangle } { \left\| \vecnot{w}_i \right\|^2 }
\left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle \\
&= \left\langle \vecnot{v}_{m+1} | \vecnot{w}_j \right\rangle
- \frac{ \left\langle \vecnot{v}_{m+1} | \vecnot{w}_j \right\rangle } { \left\| \vecnot{w}_j \right\|^2 }
\left\langle \vecnot{w}_j | \vecnot{w}_j \right\rangle \\
&= 0.
\end{split}
\end{equation*}
Clearly, $\{ \vecnot{w}_1, \ldots, \vecnot{w}_{m+1} \}$ is an orthogonal set consisting of $m+1$ non-zero vectors in the subspace spanned by $\vecnot{v}_1, \ldots, \vecnot{v}_{m+1}$.
Since the dimension of the latter subspace is $m+1$, this set is a basis for the subspace.
\end{proof}

The inductive construction of the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is known as the \defn{inner-product space}{Gram-Schmidt orthogonalization} process.

\begin{corollary}
\label{cor:orthonormal_basis}
Every finite-dimensional inner-product space has a basis of orthonormal vectors.
\end{corollary}
\begin{proof}
Let $V$ be a finite-dimensional inner-product space.
Suppose that $\vecnot{v}_1, \ldots, \vecnot{v}_n$ is a basis for $V$.
Apply the Gram-Schmidt process to obtain a basis of orthogonal vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$.
Then, a basis of orthonormal vectors is given by
\begin{equation*}
\vecnot{u}_1 = \frac{ \vecnot{w}_1 }{ \left\| \vecnot{w}_1 \right\| }, \ldots,
\vecnot{u}_n = \frac{ \vecnot{w}_n }{ \left\| \vecnot{w}_n \right\| }.
\end{equation*}
\end{proof}

\begin{example}
Consider the vectors
\begin{align*}
\vecnot{v}_1 &= (2,2,1) \\
\vecnot{v}_2 &= (3,6,0) \\
\vecnot{v}_3 &= (6,3,9)
\end{align*}
in $\RealNumbers^3$ equipped with the standard inner product.
Apply the Gram-Schmidt process to $\vecnot{v}_1, \vecnot{v}_2, \vecnot{v}_3$ to obtain an orthogonal basis.

Applying the Gram-Schmidt process to $\vecnot{v}_1, \vecnot{v}_2, \vecnot{v}_3$, we get
\begin{align*}
\vecnot{w}_1 &= (2,2,1) \\
\vecnot{w}_2 &= (3,6,0)
- \frac{ \left\langle (3,6,0) | (2,2,1) \right\rangle }{ 9 } (2,2,1) \\
&= (3,6,0) - 2 (2,2,1) = (-1,2,-2) \\
\vecnot{w}_3 &= (6,3,9)
- \frac{ \left\langle (6,3,9) | (2,2,1) \right\rangle }{ 9 } (2,2,1)
- \frac{ \left\langle (6,3,9) | (-1,2,-2) \right\rangle }{ 9 } (-1,2,-2) \\
&= (6,3,9) - 3 (2,2,1) + 2 (-1,2,-2) = (-2,1,2) .
\end{align*}
It is easily verified that $\vecnot{w}_1, \vecnot{w}_2, \vecnot{w}_3$ is an orthogonal set of vectors.
\end{example}

\begin{definition}
Let $V$ be an inner-product space and $W$ be any set of vectors in $V$.
The \defn{inner-product space}{orthogonal complement} of $W$ denoted by $W^{\bot}$ is the set of all vectors in $V$ that are orthogonal to every vector in $W$ or
\begin{equation*}
W^{\bot} = \left\{ \vecnot{v} \in V \big| \langle \vecnot{v} | \vecnot{w} \rangle = 0 \; \forall \; \vecnot{w}\in W \right\}. 
\end{equation*}
\end{definition}

\begin{problem} \label{problem:WbotSubspace}
Let $W$ be any subset of vector space $V$.
Show that $W^{\bot}$ is a closed subspace of $V$ and that any vector in the subspace spanned by $W$ is orthogonal to any vector in $W^{\bot}$.
\end{problem}
\noindent
\textbf{S~\ref{problem:WbotSubspace}}.
Let $\vecnot{m}_1, \vecnot{m}_2 \in W^{\bot}$ and $s \in F$.
For any vector $\vecnot{w} \in W$, we have
\begin{equation*}
\left\langle \vecnot{m}_1 | \vecnot{w} \right\rangle
= \left\langle \vecnot{m}_2 | \vecnot{w} \right\rangle
= 0.
\end{equation*}
This implies
\begin{equation*}
\left\langle s \vecnot{m}_1 + \vecnot{m}_2 | \vecnot{w} \right\rangle
= s \left\langle \vecnot{m}_1 | \vecnot{w} \right\rangle
+ \left\langle \vecnot{m}_2 | \vecnot{w} \right\rangle
= 0.
\end{equation*}
That is, $s \vecnot{m}_1 + \vecnot{m}_2 \in W^{\bot}$.
Hence, $W^{\bot}$ is a subspace of $V$.

To see that $W^{\bot}$ is closed, we let $\vecnot{m}$ be any point in the closure of $W^{\bot}$ and $\vecnot{m}_1,\vecnot{m}_2,\ldots \in W^{\bot}$ be a sequence that converges to $\vecnot{m}$.
The continuity of the inner product, from Theorem~\ref{theorem:InnerProductContinuous}, implies that, for all $\vecnot{w}\in W$,
\[ \left\langle \vecnot{m} | \vecnot{w} \right\rangle =  \left\langle \lim_{n\rightarrow\infty} \vecnot{m}_n | \vecnot{w}  \right\rangle  =  \lim_{n\rightarrow\infty}  \left\langle \vecnot{m}_n | \vecnot{w}  \right\rangle = 0. \]
Therefore, $\vecnot{m} \in W^{\bot}$ and the orthogonal complement contains all of its limit points.

Notice also that any vector $\vecnot{w}$ in the subspace spanned by $W$ can be written as $\vecnot{w} = \sum_{i} s_i \vecnot{w}_i$ with $\vecnot{w}_i \in W$ and $s_i \in F$.
Therefore, the inner product of $\vecnot{w}$ with any $\vecnot{w}' \in W^{\bot}$ is given by
\begin{align*}
\left\langle \vecnot{w} | \vecnot{w}' \right\rangle
&= \left\langle \sum_{i} s_i \vecnot{w}_i \Big| \vecnot{w}' \right\rangle \\
&= \sum_{i} s_i \left\langle \vecnot{w}_i | \vecnot{w}' \right\rangle \\
&= 0.
\end{align*}
It follows that the subspace spanned by $W$ is orthogonal to the subspace $W^{\bot}$.

\subsection{Hilbert Spaces}

\begin{definition}
A complete inner-product space is called a \defn{inner-product space}{Hilbert space}.
\end{definition}

\begin{definition}
Recall that a subset $\left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$ of a Hilbert space $V$ is said to be orthonormal if $\left\| \vecnot{v}_{\alpha} \right\| = 1$ for every $\alpha \in A$ and $\left\langle \vecnot{v}_{\alpha} | \vecnot{v}_{\beta} \right\rangle = 0$ for all $\alpha \neq \beta$.
If the subspace spanned by the family $\left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$ is dense in $V$, we call this set an \defn{inner-product space}{orthonormal basis}.
\end{definition}

Note that, according to this definition, an orthonormal basis for a Hilbert space $V$ is not necessarily a Hamel basis for $V$.
However, it can be shown that any orthogonal basis is a subset of a Hamel basis.
In practice it is the orthonormal basis, not the Hamel basis itself, which is of most use.
None of these issues arise in finite-dimensional spaces, where an orthogonal basis is always a Hamel basis.

Let $\mathcal{B} = \left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$ be an orthonormal basis for Hilbert space $V$.
Each element $\vecnot{v} \in V$ has the form
\begin{equation*}
\vecnot{v} = \sum_{\alpha \in A} s_{\alpha} \vecnot{v}_{\alpha},
\end{equation*}
where the sum converges in the (2)-norm.
The \defn{inner-product space}{Parseval identity}
\begin{equation*}
\left\| \vecnot{v} \right\|^2 = \sum_{\alpha \in A} | s_{\alpha} |^2
\end{equation*}
is obtained by computing $\left\langle \vecnot{v} | \vecnot{v} \right\rangle$.

\begin{theorem}
Every orthogonal set in a Hilbert space $V$ can be enlarged to an orthonormal basis for $V$.
\end{theorem}
\begin{proof}
Let $X$ be the set of orthonormal subsets of $V$.
Furthermore, for $x, y \in X$ consider the strict partial order defined by proper inclusion.
If $x = \left\{ \vecnot{v}_{\alpha} | \alpha \in A_0 \right\}$ is an element of $X$, then by the maximum principle there exists a maximal simply ordered subset $Z$ of $X$ containing $x$.
This shows the existence of a maximal orthonormal set $\left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$, where $A_0 \subset A$.

Let $W$ be the closed subspace of $V$ generated by $\left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$.
If $W \neq V$, there is a unit vector $\vecnot{u} \in W^{\bot}$, contradicting the maximality of the system $\left\{ \vecnot{v}_{\alpha} | \alpha \in A \right\}$.
Thus, $W = V$ and we have an orthonormal basis.
\end{proof}

\begin{theorem} \label{theorem:SeparableHilbertSpace}
A Hilbert space $V$ has a countable orthonormal basis if and only if $V$ is separable.
\end{theorem}
\begin{proof}[Sketch of proof]
If $V$ is separable, then it contains a countable dense subset.
Using the well-ordering theorem, this subset can be ordered into a sequence $\vecnot{v}_1 , \vecnot{v}_2 , \ldots$ such that, for every vector $\vecnot{v}\in V$ and any $\epsilon>0$, there exists an $n$ such that $\left\| \vecnot{v} - \vecnot{v}_n \right\| < \epsilon$.
Therefore, applying Gram-Schmidt orthogonalization, to this ordered sequence of vectors, generates a countable orthonormal basis.
Conversely, if $V$ has a countable orthonormal basis, then linear combinations with rational coefficients can be used to construct a countable dense subset.
\end{proof}

\section{Linear Transformations}

\subsection{Definitions}

\index{linear transform}
\begin{definition}
Let $V$ and $W$ be vector spaces over a field $F$.
A \defn{vector space}{linear transform} from $V$ to $W$ is a function $T$ from $V$ into $W$ such that
\begin{equation*}
T \left( s \vecnot{v}_1 + \vecnot{v}_2 \right)
= s T \vecnot{v}_1 + T \vecnot{v}_2
\end{equation*}
for all $\vecnot{v}_1$ and $\vecnot{v}_2$ in $V$ and all scalars $s$ in $F$.
\end{definition}

\begin{example}
Let $A$ be a fixed $m \times n$ matrix over $F$.
The function $T$ defined by $T \left( \vecnot{v} \right) = A \vecnot{v}$ is a linear transformation from $F^{n \times 1}$ into $F^{m \times 1}$.
\end{example}

\begin{example}
Let $P \in F^{m \times m}$ and $Q \in F^{n \times n}$ be fixed matrices.
Define the function $T$ from $F^{m \times n}$ into itself by $T(A) = P A Q$.
Then $T$ is a linear transformation from $F^{m \times n}$ into $F^{m \times n}$.
In particular,
\begin{equation*}
\begin{split}
T \left( s A + B \right)
&= P \left( s A + B \right) Q \\
&= s P A Q + P B Q \\
&= s T \left( A \right) + T \left( B \right) .
\end{split}
\end{equation*}
\end{example}

\begin{example}
Let $V$ be the space of continuous functions from $\RealNumbers$ to $\RealNumbers$, and define $T$ by
\begin{equation*}
(Tf)(x) = \int_{0}^x f(t) dt .
\end{equation*}
Then $T$ is a linear transformation from $V$ into $V$.
The function $Tf$ is continuous and differentiable.
\end{example}

It is important to note that if $T$ is a linear transformation from $V$ to $W$, then $T \left( \vecnot{0} \right) = \vecnot{0}$.
This is essential since
\begin{equation*}
T \left( \vecnot{0} \right)
= T \left( \vecnot{0} + \vecnot{0} \right)
= T \left( \vecnot{0} \right) + T \left( \vecnot{0} \right) .
\end{equation*}

\begin{definition}
A linear transformation $T: V \rightarrow W$ is \defn{linear transform}{singular} if there is a non-zero vector $\vecnot{v} \in V$ such that $T \vecnot{v} = \vecnot{0}$.
Otherwise, it is called \defn{linear transform}{non-singular}.
\end{definition}

\subsection{Properties}

\begin{theorem} \label{theorem:UniqueLinearTransformation}
Let $V,W$ be vector spaces over $F$ and $\mathcal{B} = \{ \vecnot{v}_{\alpha} | \alpha \in A \}$ be a Hamel basis for $V$.
For each mapping $G: \mathcal{B} \rightarrow W$, there is a unique linear transformation $T: V \rightarrow W$ such that $T \vecnot{v}_{\alpha} = G \left( \vecnot{v}_{\alpha} \right)$.
\end{theorem}
\begin{proof}
Since $\mathcal{B}$ is a Hamel basis for $V$, every vector $\vecnot{v} \in V$ has a unique expansion
\begin{equation*}
\vecnot{v} = \sum_{\alpha \in A} s_{\alpha} \vecnot{v}_{\alpha},
\end{equation*}
where $s_{\alpha} \neq 0$ for only a finite subset of $A$.
Using this expansion and the mapping $G$ for basis vectors, we define the mapping $T$, for each $\vecnot{v}$, as
\begin{equation*}
T \vecnot{v} = \sum_{\alpha \in A} s_{\alpha} G \left( \vecnot{v}_{\alpha} \right).
\end{equation*}
Using the unique decomposition of vectors, it is easy to verify that this mapping is linear.

To show that it is unique, we assume that there is another linear mapping $U : V \rightarrow W$ such that $U \vecnot{v}_{\alpha} = G \left( \vecnot{v}_{\alpha} \right)$.
In this case, the linearity guarantees that
\begin{equation*}
U \vecnot{v} = U \left( \sum_{\alpha \in A} s_{\alpha} \vecnot{v}_{\alpha} \right)
=  \sum_{\alpha \in A} s_{\alpha} U \left( \vecnot{v}_{\alpha} \right) 
=  \sum_{\alpha \in A} s_{\alpha} G \left( \vecnot{v}_{\alpha} \right).
\end{equation*}
This implies that $U \vecnot{v} = T \vecnot{v}$ for all $\vecnot{v} \in V$ and therefore that $U = T$.
\end{proof}

This theorem illuminates a very important structural element of linear transformations: they are uniquely defined by where they map basis vectors of their domain.

\iffalse
\begin{theorem} \label{theorem:UniqueLinearTransformation}
Let $V$ be a finite-dimensional vector space and let $\vecnot{v}_1, \ldots, \vecnot{v}_n$ be an ordered basis for $V$.
Let $W$ be a vector space let $\vecnot{w}_1, \ldots, \vecnot{w}_n$ be arbitrary vectors in $W$.
There exists a unique linear transformation $T$ from $V$ into $W$ such that
\begin{equation*}
T \vecnot{v}_j = \vecnot{w}_j, \quad j= 1, \ldots, n.
\end{equation*}
\end{theorem}
\begin{proof}
To prove that there exists a linear transformation $T$ with $T \vecnot{v}_j = \vecnot{w}_j$, we proceed as follows.
Since $\vecnot{v}_1, \ldots, \vecnot{v}_n$ is a basis for $V$, given $\vecnot{v} \in V$ there is a unique $n$-tuple $(t_1, \ldots, t_n)$ such that
\begin{equation*}
\vecnot{v} = \sum_{j=1}^n t_j \vecnot{v}_j .
\end{equation*}
For this vector $\vecnot{v}$, define the transformation
\begin{equation*}
T \vecnot{v} = \sum_{j=1}^n t_j \vecnot{w}_j .
\end{equation*}
Note that $T$ is a well-defined rule for associating with each vector $\vecnot{v} \in V$ a vector $T\vecnot{v} \in W$.
From this definition, it is clear that $T \vecnot{v}_j = \vecnot{w}_j$ for $j = 1, \ldots, n$.
Let $u \in V$ with
\begin{equation*}
\vecnot{u} = \sum_{j=1}^n s_j \vecnot{v}_j
\end{equation*}
and let $r \in F$.
Then, we have
\begin{equation*}
r \vecnot{u} + \vecnot{v} = \sum_{j=1}^n (r s_j + t_j) \vecnot{v}_j
\end{equation*}
and so by definition
\begin{equation*}
T \left( r \vecnot{u} + \vecnot{v} \right)
= \sum_{j=1}^n (r s_j + t_j) \vecnot{w}_j .
\end{equation*}
Also,
\begin{equation*}
\begin{split}
r T \left( \vecnot{u} \right) + T \left( \vecnot{v} \right)
&= r \sum_{j=1}^n s_j \vecnot{w}_j
+ \sum_{j=1}^n t_j \vecnot{w}_j \\
&= \sum_{j=1}^n (r s_j + t_j) \vecnot{w}_j
= T \left( r \vecnot{u} + \vecnot{v} \right) .
\end{split}
\end{equation*}
That is, $T$ is a linear transformation.

If $U$ is a linear transformation from $V$ into $W$ such that $U \vecnot{v}_j = \vecnot{w}_j$ for $j = 1, \ldots, n$, then the vector $\vecnot{v} = \sum_{j=1}^n t_j \vecnot{v}_j$ we have
\begin{equation*}
\begin{split}
U \vecnot{v} &= U \left( \sum_{j=1}^n t_j \vecnot{v}_j \right) \\
&= \sum_{j=1}^n t_j U \left( \vecnot{v}_j \right) \\
&= \sum_{j=1}^n t_j \vecnot{w}_j
= T \vecnot{v} .
\end{split}
\end{equation*}
That is, $U = T$ and the linear transformation $T$ with $T \vecnot{v}_j = \vecnot{w}_j$ for $j = 1, \ldots, n$ is unique.
\end{proof}
\fi

\begin{definition}
If $T$ is a linear transformation from $V$ into $W$, the \defn{linear transform}{range} of $T$ is the set of all vectors $\vecnot{w} \in W$ such that $\vecnot{w} = T \vecnot{v}$ for some $\vecnot{v} \in V$.
We denote the range of $T$ by
\[ \mathcal{R}(T) \triangleq \{ \vecnot{w}\in W | \exists \vecnot{v}\in V \textrm{ s.t. } T\vecnot{v} = \vecnot{w} \} = \{ T \vecnot{v} | \vecnot{v} \in V\}. \]
\end{definition}
The set $\mathcal{R}(T)$ is a subspace of $W$.
Let $\vecnot{w}_1, \vecnot{w}_2 \in \mathcal{R}(T)$ and let $s$ be a scalar.
By definition, there exist vectors $\vecnot{v}_1$ and $\vecnot{v}_2$ in $V$ such that $T \vecnot{v}_1 = \vecnot{w}_1$ and $T \vecnot{v}_2 = \vecnot{w}_2$.
Since $T$ is a linear transformation, we have
\begin{equation*}
T \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) = s T \vecnot{v}_1 + T \vecnot{v}_2 = s \vecnot{w}_1 + \vecnot{w}_2 ,
\end{equation*}
which shows that $s \vecnot{w}_1 + \vecnot{w}_2$ is also in $\mathcal{R}(T)$.

\begin{definition}
If $T$ is a linear transformation from $V$ into $W$, the \defn{linear transform}{nullspace} of $T$ is the set of all vectors $\vecnot{v} \in V$ such that $T \vecnot{v} = \vecnot{0}$.
We denote the nullspace of $T$ by
\[ \mathcal{N}(T) \triangleq \{ \vecnot{v}\in V | T \vecnot{v} = \vecnot{0} \} .\]
\end{definition}
It can easily be verified that $\mathcal{N}(T)$ is a subspace of $V$.
\begin{equation*}
T \left( \vecnot{0} \right) = \vecnot{0} \implies \vecnot{0} \in \mathcal{N}(T).
\end{equation*}
Furthermore, if $T \vecnot{v}_1 = T \vecnot{v}_2 = 0$ then
\begin{equation*}
T \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) = s T \left( \vecnot{v}_1 \right) + \left( \vecnot{v}_2 \right) = s \vecnot{0} + \vecnot{0} = \vecnot{0} ,
\end{equation*}
so that $s \vecnot{v}_1 + \vecnot{v}_2 \in \mathcal{N}(T)$.

\begin{definition}
Let $V$ and $W$ be vector spaces over a field $F$ and let $T$ be a linear transformation from $V$ into $W$.
If $V$ is finite-dimensional, the \defn{linear transform}{rank} of $T$ is the dimension of the range of $T$ and the \defn{linear transform}{nullity} of $T$ is the dimension of the nullspace of $T$.
\end{definition}

\begin{theorem}
Let $V$ and $W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.
If $V$ is finite-dimensional, then
\begin{equation*}
\Rank (T) + \Nullity (T) = \dim (V)
\end{equation*}
\end{theorem}
\begin{proof}
Let $\vecnot{v}_1, \ldots, \vecnot{v}_k$ be a basis for $\mathcal{N}(T)$, the nullspace of $T$.
There are vectors $\vecnot{v}_{k+1}, \ldots, \vecnot{v}_n \in V$ such that $\vecnot{v}_1, \ldots, \vecnot{v}_n$ is a basis for $V$.
We want to show that $T \vecnot{v}_{k+1}, \ldots, T \vecnot{v}_n$ is a basis for the range of $T$.
The vectors $T \vecnot{v}_1, \ldots, T \vecnot{v}_n$ certainly span $\mathcal{R}(T)$ and, since $T \vecnot{v}_j = \vecnot{0}$ for $j = 1, \ldots, k$, it follows that $T \vecnot{v}_{k+1}, \ldots, \vecnot{v}_n$ span $\mathcal{R}(T)$.
Suppose that there exist scalars $s_{k+1}, \ldots, s_n$ such that
\begin{equation*}
\sum_{j=k+1}^n s_j T \vecnot{v}_j = \vecnot{0}.
\end{equation*}
This implies that
\begin{equation*}
T \left( \sum_{j=k+1}^n s_j \vecnot{v}_j \right) = \vecnot{0}.
\end{equation*}
and accordingly the vector $\vecnot{v} = \sum_{j=k+1}^n s_j \vecnot{v}_j$ is in the nullspace of $T$.
Since $\vecnot{v}_1, \ldots, \vecnot{v}_k$ form a basis for $\mathcal{N}(T)$, there must be a linear combination such that
\begin{equation*}
\vecnot{v} = \sum_{j=1}^k t_j \vecnot{v}_j .
\end{equation*}
But then,
\begin{equation*}
\sum_{j=1}^k t_j \vecnot{v}_j - \sum_{j=k+1}^n s_j \vecnot{v}_j = \vecnot{0}.
\end{equation*}
Since the vectors $\vecnot{v}_1, \ldots, \vecnot{v}_n$ are linearly independent, this implies that
\begin{equation*}
t_1 = \cdots = t_k = s_{k+1} = \hdots s_n = 0.
\end{equation*}
That is, the set $T \vecnot{v}_{k+1}, \ldots, T \vecnot{v}_n$ is linearly independent in $W$ and therefore forms a basis for $\mathcal{R}(T)$.
In turn, this implies that $n = \Rank(T) + \Nullity(T)$.
\end{proof}

\begin{corollary}
If $A$ is an $m \times n$ matrix with entries in the field $F$, then
\begin{equation*}
\mathrm{row~rank} (A) \triangleq \dim( \mathcal{R}(A^H)) =  \dim( \mathcal{R}(A)) \triangleq \mathrm{column~rank} (A).
\end{equation*}
\end{corollary}
\begin{proof}
First, we note that the range $\mathcal{R}(A)$ of $A$ equals the column space of $A$ and therefore $\Rank(A) = \dim \left( \mathcal{R}(A) \right)$.
%Next, we show the null space $\mathcal{N}(A)$ of $A$ is the orthogonal complement of the row space $\mathcal{R}(A^H)$.
Next, we use the definition of the orthogonal complement and $\mathcal{R}(A^H) = \{ A^H \vecnot{y} | \vecnot{y} \in F^m \}$ to see that
\[ \mathcal{R}(A^H)^{\perp} \! = \! \left\{ \vecnot{x} \in F^n \, | \, \vecnot{x}^H \vecnot{z} = \vecnot{0} \; \forall \, \vecnot{z}\in \mathcal{R}(A^H \right\}
\! = \! \left\{ \vecnot{x} \in F^n \, | \, \vecnot{x}^H A^H \vecnot{y} = 0 \; \forall \, \vecnot{y}\in F^m \right\}. \]
This implies that $\mathcal{R}(A^H)^{\perp} =  \left\{ \vecnot{x} \in F^n \, | \, \vecnot{x}^H A^H = 0 \right\} = \mathcal{N}(A)$ and therefore that
\[ \dim\left( \mathcal{R}(A^H) \right) = n-\dim \left( \mathcal{R}(A^H) \right)^{\bot} = n - \dim \left(\mathcal{N}(A) \right) = \dim \left( \mathcal{R}(A) \right).\]
\end{proof}

\subsection{Projections}

\begin{definition}
A function $F: X \rightarrow Y$ with $Y \subseteq X$ is \defn{linear transform}{idempotent} if $F(F(x))=F(x)$.  When $F$ is a linear transformation, this reduces to $F^2 = F \cdot F = F$.
\end{definition}

\begin{definition}
Let $V$ be a vector space and $T: V \rightarrow V$ be a linear transformation.
Then, $T$ is a projection if $T$ is idempotent.
\end{definition}

\begin{example}
The idempotent matrix $A$ is a projection onto the first two coordinates.
\begin{equation*}
A = \left[
\begin{array}{ccc}
1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{array}
\right]
\end{equation*}
\end{example}

\begin{theorem}
Let $V$ be a vector space and $T: V \rightarrow V$ be a projection operator.
Then, the range $\mathcal{R}(T)$ and the $\mathcal{N}(T)$ are disjoint subspaces of $V$.
\end{theorem}
\begin{proof}
For all $\vecnot{v} \in V - \{0\}$, we need to prove that $\vecnot{v}$ is not in both the range and nullspace.
Let $\vecnot{v}\in V$ be in the range of $T$ so that there is a $\vecnot{w}\in V$ such that $T \vecnot{w} = \vecnot{v}$.
Then, $T \vecnot{v} = T^2 \vecnot{w} = T \vecnot{w} = \vecnot{v}$ and $\vecnot{v}$ is not in the null space unless $\vecnot{v} = \vecnot{0}$.

Let $\vecnot{v}$ be in the null space of $T$, then $T \vecnot{v} = \vecnot{0}$.
But, $T \vecnot{v} = \vecnot{v}$ for all $\vecnot{v}$ in the range.
Therefore, $\vecnot{v}$ is not in the range unless $\vecnot{v} = \vecnot{0}$.
From this, we see that only $\vecnot{0}\in V$ is in both the range and nullspace.
Therefore, they are disjoint subspaces.
\end{proof}

\begin{example}
Consider the linear transform $T: V \rightarrow V$ defined by $T = I - P$, where $P$ is a projection.
It is easy to verify that $T$ is a projection operator because
\[ T^2 = (I-P)(I-P) = I - P- P + P^2 = I-P = T .\]
Notice also that $P(I-P) \vecnot{v} = \vecnot{0}$ implies that $\mathcal{R}(T) \subseteq \mathcal{N}(P)$ and $T \vecnot{v} = \vecnot{v}$ for $\vecnot{v} \in \mathcal{N}(P)$ implies $\mathcal{N}(P) \subseteq \mathcal{R}(T)$.
Therefore, $\mathcal{R}(T) = \mathcal{N}(P)$ and $I-P$ is a projection onto $\mathcal{N}(P)$.
\end{example}

\begin{definition}
\label{definition:OrthogonalProjection}
Let $V$ be an inner-product space and $P: V \rightarrow V$ be a projection operator.
Then, $P$ is an \defn{linear transform}{orthogonal projection} if $\mathcal{R}(P) \bot \mathcal{N}(P)$.
\end{definition}

\begin{example}
Let $V$ be an inner-product space and $P: V \rightarrow V$ be an orthogonal projection.
Then, $\vecnot{v} = P \vecnot{v} + (I-P) \vecnot{v}$ defines a natural orthogonal decomposition of $\vecnot{v}$ because $P \vecnot{v} \in \mathcal{R}(P)$ and $(I-P) \vecnot{v} \in \mathcal{N}(P)$.
Therefore, $\mathcal{N}(P) = \mathcal{R}(P)^{\bot}$ and $P \vecnot{v} = \vecnot{v}$ if and only if $\vecnot{v} \in \mathcal{R}(P)$.
\end{example}

\begin{theorem}
Any idempotent Hermitian matrix $P$ defines an orthogonal projection operator.
\end{theorem}
\begin{proof}
We simply must verify that the range a null space are orthogonal.
Since $P \vecnot{u} \in \mathcal{R}(P)$ and $(I-P) \vecnot{v} \in \mathcal{N}(P)$, we can simply point out that
\[ \langle P\vecnot{u} | (I-P)\vecnot{v} \rangle = \vecnot{v}^H (I-P)^H P \vecnot{u} = \vecnot{v}^H (P - P^H P) \vecnot{u} = \vecnot{v}^H (P - P^2) \vecnot{u} = 0. \]
\end{proof}


