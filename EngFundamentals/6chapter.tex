\chapter{Canonical Forms}

\section{Characteristic Values}

\begin{definition}
Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on $V$.
An \emph{eigenvalue} of $T$ is a scalar $\lambda \in F$ such that there exists a non-zero vector $\vecnot{v} \in V$ with $T \vecnot{v} = \lambda \vecnot{v}$.
Any vector $\vecnot{v}$ such that $T \vecnot{v} = \lambda \vecnot{v}$ is called an \emph{eigenvector} of $T$ associated with the characteristic value $\lambda$.
\end{definition}

\begin{definition}
If $A$ is an $n \times n$ matrix over the field $F$, a \emph{characteristic value} of $A$ is a scalar $\lambda \in F$ such that the matrix $(A - \lambda I)$ is singular.
\end{definition}

Let $A$ be a matrix over the field of real or complex numbers.
A nonzero vector $\vecnot{v}$ is called a \emph{right eighenvector} for the eigenvalue $\lambda$ if $A \vecnot{v} = \lambda \vecnot{v}$.
It is called a \emph{left eigenvector} if $\vecnot{v}^H A = \lambda \vecnot{v}^H$.

The polynomial $\chi_A (\lambda) = \det (\lambda I - A)$ is called the \emph{characteristic polynomial} of $A$.
The equation $\det (\lambda I - A) = 0$ is called the characteristic equation of $A$.
The set of roots of the characteristic polynomial $\chi_A (\lambda)$ is called the \emph{spectrum} of $A$, and is denoted $\lambda(A)$.

\begin{fact}
Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $\lambda$ be a scalar.
The following are equivalent,
\begin{enumerate}
\item $\lambda$ is a characteristic value of $T$
\item the operator $(T - \lambda I)$ is singular
\item $\det (T - \lambda I) = 0$.
\end{enumerate}
\end{fact}

The latter criterion is important.
It states that an eigenvalue must be a root of the characteristic polynomial $\det( T - sI )$.

\begin{theorem}
If the eigenvalues of an $n \times n$ matrix are all distinct, then the eigenvectors of $A$ are linearly independent.
\end{theorem}
\begin{proof}
Suppose that the eigenvectors are linearly dependent.
Then there exist scalars $c_1, c_2, \ldots, c_n$, not all zeros, such that
\begin{equation*}
\sum_{i=1}^n c_i \vecnot{v}_i = \vecnot{0}
\end{equation*}
where $\vecnot{v}_1, \ldots, \vecnot{v}_n$ denote the $n$ linearly dependent eigenvectors.
Let $m \geq 1$ be the smallest number of vectors such that
\begin{equation} \label{equation:eigenvectors1}
\sum_{j=1}^m c_{i_j} \vecnot{v}_{i_j} = \vecnot{0}
\end{equation}
and $c_{i_j} \neq 0$ for $j = 1, \ldots, m$.
Because the eigenvectors are linearly dependent, we must have $1 \geq m \geq n$.
Furthermore, by the properties of $A$, we obtain
\begin{equation} \label{equation:eigenvectors2}
A \sum_{j=1}^m c_{i_j} \vecnot{v}_{i_j}
= \sum_{j=1}^m c_{i_j} A \vecnot{v}_{i_j}
= \sum_{j=1}^m c_{i_j} \lambda_{i_j} \vecnot{v}_{i_j}
= A \vecnot{0} = \vecnot{0} .
\end{equation}
We now take $\lambda_{i_1}$ times \eqref{equation:eigenvectors1}, and subtract it from the last equation to get
\begin{equation*}
\sum_{j=2}^m c_{i_j} (\lambda_{i_j} - \lambda_{i_1})
\vecnot{v}_{i_j} = \vecnot{0} .
\end{equation*}
That is, there exists a linearly dependent sum of vectors with less than $m$ elements.
This contradict our previous minimality assumption.
Hence, we must conclude that the set of eigenvectors is linearly independent.
\end{proof}

\begin{definition}
Let $T$ be a linear operator on a finite-dimensional vector space $V$.
The operator $T$ is \emph{diagonalizable} if there exists a basis $\mathcal{B}$ for $V$ such that each basis vector is an eigenvector of $T$,
\begin{equation*}
\left[ T \right]_{\mathcal{B}}
= \left[ \begin{array}{cccc}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{array} \right]
\end{equation*}
\end{definition}

Similarly, a matrix $A$ is diagonalizable if there exists an invertible matrix $S$ such that
\begin{equation*}
A = S \Lambda S^{-1}
\end{equation*}
where $\Lambda$ is a diagonal matrix.
We proceed to show that if an $n \times n$ matrix has $n$ linearly independent eigenvecotrs, then it is diagonalizable.

Suppose that the $n \times n$ matrix $A$ has $n$ linearly independent eigenvectors, which we denote by $\vecnot{v}_1, \ldots, \vecnot{v}_n$.
Let the eigenvalue of $\vecnot{v}_i$ be denoted by $\lambda_i$ so that
\begin{equation*}
A \vecnot{v}_j = \lambda_j \vecnot{v}_j, \quad j = 1, \ldots, n.
\end{equation*}
In matrix form, we have
\begin{equation*}
\begin{split}
A \left[ \begin{array}{ccc} \vecnot{v}_1 & \cdots & \vecnot{v}_n \end{array} \right]
&= \left[ \begin{array}{ccc} A \vecnot{v}_1 & \cdots & A \vecnot{v}_n \end{array} \right] \\
&= \left[ \begin{array}{ccc} \lambda_1 \vecnot{v}_1 & \cdots & \lambda_n \vecnot{v}_n \end{array} \right] .
\end{split}
\end{equation*}
We can rewrite the last matrix on the write as
\begin{equation*}
\left[ \begin{array}{ccc} \lambda_1 \vecnot{v}_1 & \cdots & \lambda_n \vecnot{v}_n \end{array} \right]
= \left[ \begin{array}{ccc} \vecnot{v}_1 & \cdots & \vecnot{v}_n \end{array} \right]
\left[ \begin{array}{ccc}
\lambda_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_n
\end{array} \right]
= S \Lambda .
\end{equation*}
where
\begin{equation*}
S = \left[ \begin{array}{ccc} \vecnot{v}_1 & \cdots & \vecnot{v}_n \end{array} \right]
\quad \text{and} \quad
\Lambda = \left[ \begin{array}{ccc}
\lambda_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_n
\end{array} \right] ,
\end{equation*}
Combining these two equations, we obtain the equality
\begin{equation*}
A S = S \Lambda .
\end{equation*}
Since the eigenvectors are linearly independent, the matrix $S$ is full rank and hence invertible.
We can therefore write
\begin{align*}
A &= S \Lambda S^{-1} \\
\Lambda &= S^{-1} A S .
\end{align*}
That is, the matrix $A$ is diagonalizable.

The type of the transformation from $A$ to $\Lambda$ arises in a variety of contexts.
If there exists an inveritble matrix $T$ such that
\begin{equation*}
A = T B T^{-1},
\end{equation*}
then matrices $A$ and $B$ are said to be \emph{similar}.
If $A$ and $B$ are similar, then they have the same eigenvalues.
Similar matrices can be considered representations of the same linear operator using different bases.

\subsubsection{Matrix Exponential}

The diagonal form of a diagonalizable matrix can be used in a number of applications.
One such application is the computation of matrix exponentials.
If $A = S \Lambda S^{-1}$ then
\begin{equation*}
A^2 = S \Lambda S^{-1} S \Lambda S^{-1}
= S \Lambda^2 S^{-1}
\end{equation*}
and, more generally,
\begin{equation*}
A^n = S \Lambda^n S^{-1}.
\end{equation*}
Note that $\Lambda^n$ is obtained in a straightforward manner as
\begin{equation*}
\Lambda^n = \left[ \begin{array}{ccc}
\lambda_1^n & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_n^n
\end{array} \right] .
\end{equation*}
This observation drastically simplifies the computation of the matrix exponential $e^A$,
\begin{equation*}
e^A = \sum_{i=0}^{\infty} \frac{A^i}{i!}
= S \left( \sum_{i=0}^{\infty} \frac{\Lambda^i}{i!} \right) S^{-1}
= S e^{\Lambda} S^{-1},
\end{equation*}
where
\begin{equation*}
e^{\Lambda} = \left[ \begin{array}{ccc}
e^{\lambda_1} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & e^{\lambda_n}
\end{array} \right] .
\end{equation*}

\begin{theorem}
Let $p( \cdot )$ be a given polynomial.
If $\lambda$ is an eigenvalue of $A$, while $\vecnot{v}$ is an associated eigenvector, then $p ( \lambda )$ is an eigenvalue of the matrix $p (A)$ and $\vecnot{v}$ is an eigenvector of $p(A)$ associated with $p(\lambda)$.
\end{theorem}
\begin{proof}
Consider $p(A) \vecnot{v}$.
Then,
\begin{equation*}
p( A ) \vecnot{v} = \sum_{k=0}^l p_k A^k \vecnot{v}
= \sum_{k=0}^l p_k \lambda^k \vecnot{v}
= p( \lambda ) \vecnot{v} .
\end{equation*}
That is $p(A) \vecnot{v} = p( \lambda ) \vecnot{v}$.
\end{proof}

A matrix $A$ is singular if and only if $0$ is an eighenvalue of $A$.

\section{The Jordan Form}

Not all matrices are diagonalizable.
In particular if $A$ has repeated eigenvalues then it is not always possible to diagonalize it.

\begin{theorem}
Let $A$ be an $n \times n$ matrix.
Then $A$ is diagonalizable if and only if there is a set of $n$ linearly independent vectors, each of which is an eigenvector of $A$.
\end{theorem}

\begin{proof}
If $A$ has $n$ linearly independent eigenvectors $\vecnot{v}_1, \ldots, \vecnot{v}_n$, then let $S$ be an invertible matrix whose columns are there $n$ vectors.
Consider
\begin{equation*}
\begin{split}
S^{-1} A S &= S^{-1} \left[ \begin{array}{ccc} A \vecnot{v}_1 & \cdots & A \vecnot{v}_n \end{array} \right] \\
&= S^{-1} \left[ \begin{array}{ccc} \lambda_1 \vecnot{v}_1 & \cdots & \lambda_n \vecnot{v}_n \end{array} \right] \\
&= S^{-1} S \Lambda = \Lambda .
\end{split}
\end{equation*}
Conversely, suppose that there is a similarity matrix $S$ such that $S^{-1} A S = \Lambda$ is a diagonal matrix.
Then $A S = S \Lambda$.
This implies that $A$ times the $i$th column of $S$ is the $i$th diagonal entry of $\Lambda$ times the $i$th column of $S$.
That is, the $i$th column of $S$ is an eigenvector of $A$ associated with the $i$th diagonal entry of $\Lambda$.
Since $S$ is nonsingular, there are exactly $n$ linearly independent eigenvectors.
\end{proof}

\begin{fact}
An $m \times m$ matrix $A$ with $k \leq m$ linearly independent eigenvectors can be writte as
\begin{equation*}
A = T J T^{-1} ,
\end{equation*}
where $J$ is a block-diagonal matrix,
\begin{equation*}
J = \left[ \begin{array}{ccc}
J_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & J_n
\end{array} \right] .
\end{equation*}
The blocks $J_i$ are called Jordan blocks, and they have the form
\begin{equation*}
J = \left[ \begin{array}{ccccc}
\lambda_i & 1 & 0 &\cdots & 0 \\
0 & \lambda_i & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \lambda_i
\end{array} \right] .
\end{equation*}
When $J_i$ is an $l \times l$ matrix, then the eigen value $\lambda_i$ is repeated $l$ times along the diagonal, and $1$ appears $l - 1$ times above the diagonal.
Two matrices are similar if they have identical Jordan form.
\end{fact}



