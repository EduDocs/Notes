\chapter{Logic and Set Theory}


\begin{quote}
To criticize mathematics for its abstraction is to miss the point entirely.
Abstraction is what makes mathematics work.
If you concentrate too closely on too limited an application of a mathematical idea, you rob the mathematician of his most important tools: analogy, generality, and simplicity.
\flushright{\textit{-- Ian Stewart} \\ Does God play dice? The mathematics of chaos}
\end{quote}

\index{logic}

In mathematics, a \defn{logic}{proof} is a demonstration that, assuming certain axioms, some statement is necessarily true.
That is, a proof is a logical argument, not an empirical one.
One must demonstrate that a proposition is true in all cases before it is considered a theorem of mathematics.
An unproven proposition for which there is some sort of empirical evidence is known as a \defn{logic}{conjecture}.
Mathematical logic is the framework upon which rigorous proofs are built.
It is the study of the principles and criteria of valid inference and demonstrations.

Logicians have analyzed set theory in great details, formulating a collection of axioms that affords a broad enough and strong enough foundation to mathematical reasoning.
The standard form of axiomatic set theory is the Zermelo-Fraenkel set theory, together with the axiom of choice.
Each of the axioms included in this theory expresses a property of sets that is widely accepted by mathematicians.
It is unfortunately true that careless use of set theory can lead to contradictions.
Avoiding such contradictions was one of the original motivations for the axiomatization of set theory.

A rigorous analysis of set theory belongs to the foundations of mathematics and mathematical logic.
The study of these topics is, in itself, a formidable task.
For our purposes, it will suffice to approach basic logical concepts informally.
That is, we adopt a naive point of view regarding set theory and assume that the meaning of a set as a collection of objects is intuitively clear.
While informal logic is not itself rigorous, it provides the underpinning for rigorous proofs.
The rules we follow in dealing with sets are derived from established axioms.
At some point of your academic career, you may wish to study set theory and logic in greater detail.
Our main purpose here is to learn how to state mathematical results clearly and how to prove them.


\section{Statements}

A proof in mathematics demonstrates the truth of certain \textbf{statement}.
It is therefore natural to begin with a brief discussion of statements.
A statement, or \textbf{proposition}, is the content of an assertion.
It is either true or false, but cannot be both true and false at the same time.
For example, the expression ``There are no classes at Texas A\&M University today'' is a statement since it is either true or false.
The expression ``Do not cheat and do not tolerate those who do'' is not a statement.
Note that an expression being a statement does not depend on whether we personally can verify its validity.
The expression ``The base of the natural logarithm, denoted $e$, is an irrational number'' is a statement that most of us cannot prove.

Statements on their own are fairly uninteresting.
What brings value to logic is the fact that there are a number of ways to form new statements from old ones.
In this section, we present five ways to form new statements from old ones.
They correspond to the English expressions: and; or; not; if, then; if and only if.
In the discussion below, $P$ and $Q$ represent two abstract statements.

A logical \defn{logic}{conjunction} is an operation on two logical propositions that produces a value of true if both statements are true, and is false otherwise.
The conjunction (or logical AND) of $P$ and $Q$, denoted by $P \wedge Q$, is precisely defined by
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$P$ & $Q$ & $P \wedge Q$ \\
\hline
T & T & T \\
T & F & F \\
F & T & F \\
F & F & F \\
\hline
\end{tabular} .
\end{center}

Similarly, a logical \defn{logic}{disjunction} is an operator on two logical propositions that is true if either statements is true or both are true, and is false otherwise.
The disjunction (or logical OR) of $P$ and $Q$, denoted $P \vee Q$, is defined by
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$P$ & $Q$ & $P \vee Q$ \\
\hline
T & T & T \\
T & F & T \\
F & T & T \\
F & F & F \\
\hline
\end{tabular} .
\end{center}

In mathematics, a \defn{logic}{negation} is an operator on the logical value of a proposition that sends true to false and false to true.
The negation (or logical NOT) of $P$, denoted $\neg P$, is given by
\begin{center}
\begin{tabular}{|c|c|}
\hline
$P$ & $\neg P$ \\
\hline
T & F \\
F & T \\
\hline
\end{tabular} .
\end{center}

The next method of combining mathematical statements is slightly more subtle than the preceding ones.
It is connected to the notion of logical implication.
The \defn{logic}{conditional} from $P$ to $Q$, denoted $P \rightarrow Q$, is mathematically true if it is not the case that $P$ is true and $Q$ is false.
The precise definition of $P \rightarrow Q$ is given in the truth table
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$P$ & $Q$ & $P \rightarrow Q$ \\
\hline
T & T & T \\
T & F & F \\
F & T & T \\
F & F & T \\
\hline
\end{tabular} .
\end{center}
This table should match your intuition when $P$ is true.
When $P$ is false, students often think the resulting truth value should be undefined.
Although it may seem strange at first glance, this truth table is universally accepted by mathematicians.
To motivate this definition, one can think of $P \rightarrow Q$ as a promise that $Q$ is true whenever $P$ is true.
When $P$ is false, the promise is kept by default.
This definition allows one to combine many statements together and detect broken promises without being distracted by uninformative statements.

Logicians draw a firm distinction between the \defn{logic}{conditional connective} and the \defn{logic}{implication relation}.
They use the phrase ``if $P$ then $Q$'' for the conditional connective and the phrase ``$P$ implies $Q$'' for the implication relation.
They explain the difference between these two forms by saying that the conditional is the contemplated relation, while the implication is the asserted relation.
We will discuss this distinction in the Section~\ref{section:Relations}, where we formally study relations between statements.
The importance and soundness of the conditional form $P \rightarrow Q$ will become clearer then.

The logical \defn{logic}{biconditional} is an operator connecting two logical propositions that is true if the statements are both true or both false, and it is false otherwise.
The biconditional from $P$ to $Q$, denoted $P \leftrightarrow Q$, is precisely defined by
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$P$ & $Q$ & $P \leftrightarrow Q$ \\
\hline
T & T & T \\
T & F & F \\
F & T & F \\
F & F & T \\
\hline
\end{tabular} .
\end{center}
We read $P \leftrightarrow Q$ as ``$P$ if and only if $Q$.''
The phrase ``if and only if'' is often abbreviated as ``iff''.

Using the five basic operations defined above, it is possible to form more complicated compound statements.
We sometimes need parentheses to avoid ambiguity in writing compound statements.
We use the convention that $\neg$ takes precedence over the other four operations, but none of these operations takes precedence over the others.
For example, let $P$, $Q$ and $R$ be three propositions.
We wish to make a truth table for the following statement,
\begin{equation} \label{equation:LogicStatement}
(P \rightarrow R) \wedge (Q \vee \neg R) .
\end{equation}
We can form the true table for this statement, using simple steps, as follows
\begin{center}
\begin{tabular}{|c|c|c|ccccccc|}
\hline
$P$ & $Q$ & $R$
& $(P$ & $\rightarrow$ & $R)$ & $\wedge$ & $(Q$ & $\vee$ & $\neg R)$ \\
\hline
T & T & T & T & T & T & T & T & T & F \\
T & T & F & T & F & F & F & T & T & T \\
T & F & T & T & T & T & F & F & F & F \\
T & F & F & T & F & F & F & F & T & T \\
F & T & T & F & T & T & T & T & T & F \\
F & T & F & F & T & F & T & T & T & T \\
F & F & T & F & T & T & F & F & F & F \\
F & F & F & F & T & F & T & F & T & T \\
& & & 1 & 5 & 2 & 7 & 3 & 6 & 4 \\
\hline
\end{tabular} .
\end{center}
%\end{example}

We conclude this section with a brief mention of two important concepts.
A \defn{logic}{tautology} is a statement that is true in every valuation of its propositional variables, independent of the truth values assigned to these variables.
The proverbial tautology is $P \vee \neg P$,
\begin{center}
\begin{tabular}{|c|ccc|}
\hline
$P$ & $P$ & $\vee$ & $\neg P$ \\
\hline
T & T & T & F \\
F & F & T & T \\
& 1 & 3 & 2 \\
\hline
\end{tabular} .
\end{center}
For instance, the statement ``The Aggies won their last football game or the Aggies did not win their last football game'' is true regardless of whether the Aggies actually defeated their latest opponent.

The negation of a tautology is a \defn{logic}{contradiction}, a statement that is necessarily false regardless of the truth values of its propositional variables.
The statement $P \wedge \neg P$ is a contradiction, and its truth table is
\begin{center}
\begin{tabular}{|c|ccc|}
\hline
$P$ & $P$ & $\wedge$ & $\neg P$ \\
\hline
T & T & F & F \\
F & F & F & T \\
& 1 & 3 & 2 \\
\hline
\end{tabular} .
\end{center}

Of course, most statement we encounter are neither tautologies nor contradictions.
For example, \eqref{equation:LogicStatement} is not necessarily either true or false.
Its truth value depends on the values of $P$, $Q$ and $R$.
Try to see whether the statement
\begin{equation*}
((P \wedge Q) \rightarrow R) \rightarrow (P \rightarrow (Q \rightarrow R))
\end{equation*}
is a tautology, a contradiction, or neither.


\section{Relations between Statements}
\label{section:Relations}

Strictly speaking, relations between statements are not formal statements themselves.
They are \emph{meta-statements} about some propositions.
We study two types of relations between statements, \emph{implication} and \emph{equivalence}.
An example of an implication meta-statement is the observation that ``if the statement `Robert graduated from Texas A\&M University' is true, then it implies that the statement `Robert is an Aggie' is also true.''
Another example of a meta-statement is ``the statement `Fred is an Aggie and Fred is honest' being true is equivalent to the statement `Fred is honest and Fred is an Aggie' being true.''
These two examples illustrate how meta-statements describe the relationship between statements.
It is also instructive to note that implications and equivalences are the meta-statement analogs of conditionals and biconditionals.

A \defn{logic}{logical implication} can be intuitively described as ``$P$ implies $Q$'' if $Q$ must be true whenever $P$ is true.
That is, $Q$ cannot be false if $P$ is true.
Necessity is the key aspect of this sentence, the fact that $P$ and $Q$ both happen to be true cannot be coincidental.
To have $P$ implies $Q$, we need the conditional $P \rightarrow Q$ to be true under all possible circumstances.

Meta-statements, such as ``$P$ implies Q'', can be defined formally when $P$ and $Q$ are both logical functions of other propositions.
For example, consider $P=R \wedge (R \to S)$ and $Q=S$.
Then, the truth of the statement $P \to Q$ depends only on the truth of external propositions $R$ and $S$.

The notion of implication can be rigorously defined as follows, $P$ implies $Q$ if the statement $P \rightarrow Q$ is a tautology.
We abbreviate $P$ implies $Q$ by writing $P \Rightarrow Q$.
It is important to understand the difference between ``$P \rightarrow Q$'' and ``$P \Rightarrow Q$.''
The former, $P \rightarrow Q$, is a compound statement that may or may not be true.
On the other hand, $P \Rightarrow Q$ is a relation stating that the compound statement $P \rightarrow Q$ is true under all instances of the external propositions.

While the distinction between implication and conditional may seem extraneous, we will soon see that meta-statements become extremely useful in building valid arguments.
In particular, the following implications are used extensively in constructing proofs.
\begin{fact}
Let $P$, $Q$, $R$ and $S$ be statements.
\begin{enumerate}
\item $(P \rightarrow Q) \wedge P \Rightarrow Q$.
\item $(P \rightarrow Q) \wedge \neg Q \Rightarrow \neg P$.
\item $P \wedge Q \Rightarrow P$.
\item $(P \vee Q) \wedge \neg P \Rightarrow Q$.
\item $P \leftrightarrow Q \Rightarrow P \rightarrow Q$.
\item $(P \rightarrow Q) \wedge (Q \rightarrow P) \Rightarrow P \rightarrow Q$.
\item $(P \rightarrow Q) \wedge (Q \rightarrow R) \Rightarrow P \rightarrow R$
\item $(P \rightarrow Q) \wedge (R \rightarrow S) \wedge (P \vee R) \Rightarrow Q \vee S$.
\end{enumerate}
\end{fact}

As an illustrative example, we show that $(P \rightarrow Q) \wedge (Q \rightarrow R)$ implies $P \rightarrow R$.
To demonstrate this assertion, we need to show that
\begin{equation} \label{equation:HypotheticalSyllogism}
((P \rightarrow Q) \wedge (Q \rightarrow R)) \rightarrow (P \rightarrow R)
\end{equation}
is a tautology.
This is accomplished in the truth table below
\begin{center}
\begin{tabular}{|c|c|c|ccccccccccc|}
\hline
$P$ & $Q$ & $R$
& $((P$ & $\rightarrow$ & $Q)$ & $\wedge$ & $(Q$ & $\rightarrow$ & $R))$ & $\rightarrow$ & $(P$ & $\rightarrow$ & $R)$ \\
\hline
T & T & T & T & T & T & T & T & T & T & T & T & T & T \\
T & T & F & T & T & T & F & T & F & F & T & T & F & F \\
T & F & T & T & F & F & F & F & T & T & T & T & T & T \\
T & F & F & T & F & F & F & F & T & F & T & T & F & F \\
F & T & T & F & T & T & T & T & T & T & T & F & T & T \\
F & T & F & F & T & T & F & T & F & F & T & F & T & F \\
F & F & T & F & T & F & T & F & T & T & T & F & T & T \\
F & F & F & F & T & F & T & F & T & F & T & F & T & F \\
& & & 1 & 7 & 2 & 10 & 3 & 8 & 4 & 11 & 5 & 9 & 6 \\
\hline
\end{tabular} .
\end{center}
Column~11 has the truth values for statement \eqref{equation:HypotheticalSyllogism}.
Since \eqref{equation:HypotheticalSyllogism} is true under all circumstances, it is a tautology and the implication holds.
Showing that the other relations are valid is left to the reader as an exercise.

Reversing the arrow in a conditional statement gives the \defn{logic}{converse} of that statement.
For example, the statement $Q \rightarrow P$ is the converse of $P \rightarrow Q$.
This reversal may not preserve the truth of the statement though and therefore logical implications are not always reversible.
For instance, although $(P \rightarrow Q) \wedge (Q \rightarrow R)$ implies $P \rightarrow R$, the converse is not always true.
It can easily be seen from columns~9~\&~10 above that
\begin{equation*}
(P \rightarrow R) \rightarrow ((P \rightarrow Q) \wedge (Q \rightarrow R))
\end{equation*}
is not a tautology.
That is, $P \rightarrow R$ certainly does not imply $(P \rightarrow Q) \wedge (Q \rightarrow R)$.

A logical implication that is reversible is called a \defn{logic}{logical equivalence}.
More precisely, $P$ is equivalent to $Q$ if the statement $P \leftrightarrow Q$ is a tautology.
We denote the sentence ``$P$ is equivalent to $Q$'' by simply writing ``$P \Leftrightarrow Q$.''
The meta-statement $P \Leftrightarrow Q$ holds if and only if $P \Rightarrow Q$ and $Q \Rightarrow P$ are both true.
Being able to recognize that two statements are equivalent will become handy.
It is sometime possible to demonstrate a result by finding an alternative, equivalent form of the statement that is easier to prove than the original form.
A list of important equivalences appears below.

\begin{fact}
Let $P$, $Q$ and $R$ be statements.
\begin{enumerate}
\item $\neg (\neg P) \Leftrightarrow P$.
\item $P \vee Q \Leftrightarrow Q \vee P$.
\item $P \wedge Q \Leftrightarrow Q \wedge P$.
\item $(P \vee Q) \vee R \Leftrightarrow P \vee (Q \vee R)$.
\item $(P \wedge Q) \wedge R \Leftrightarrow P \wedge (Q \wedge R)$.
\item $P \wedge (Q \vee R) \Leftrightarrow (P \wedge Q) \vee (P \wedge R)$.
\item $P \vee (Q \wedge R) \Leftrightarrow (P \vee Q) \wedge (P \vee R)$.
\item $P \rightarrow Q \Leftrightarrow \neg P \vee Q$.
\item $P \rightarrow Q \Leftrightarrow \neg Q \rightarrow \neg P$ (Contrapositive).
\item $P \leftrightarrow Q \Leftrightarrow (P \rightarrow Q) \wedge (Q \rightarrow P)$.
\item $\neg (P \wedge Q) \Leftrightarrow \neg P \vee \neg Q$ (De Morgan's Law).
\item $\neg (P \vee Q) \Leftrightarrow \neg P \wedge \neg Q$ (De Morgan's Law).
\end{enumerate}
\end{fact}

The first equivalence in this list, $\neg (\neg P) \Leftrightarrow P$, may appear trivial.
However, from the point of view of constructing mathematical proofs, this equivalence is frequently employed.
Indeed, one method to prove that statement $P$ is true is to hypothesize that $\neg P$ is true and then derive a contradiction.
It then follows that, if $\neg P$ is false, then $P$ is true.
This popular technique is called \defn{logic}{proof by contradiction}.
It is illustrated below through a classic example.

Given a conditional statement of the form $P \rightarrow Q$, we call $\neg Q \rightarrow \neg P$ the \defn{logic}{contrapositive} of the original statement.
The equivalence $P \rightarrow Q \Leftrightarrow \neg Q \rightarrow \neg P$ is also used extensively in constructing mathematical proofs.


\subsection{Fallacious Arguments}

A \defn{logic}{fallacy} is a component of an argument that is demonstrably flawed in its logic or form, thus rendering the argument invalid.
Recognizing fallacies in mathematical proofs may be difficult since arguments are often structured using convoluted patterns that obscure the logical connections between assertions.
We give below examples for three types of fallacies that are often found in attempted mathematical proofs.

\paragraph{Affirming the Consequent:}
If the Indian cricket team wins a test match, then all the players will drink tea together.
All the players drank tea together.
Therefore the Indian cricket team won a test match.

\paragraph{Denying the Antecedent:}
If Diego Maradona drinks coffee, then he will be fidgety.
Diego Maradona did not drink coffee.
Therefore, he is not fidgety.

\paragraph{Unwarranted Assumptions:}
If Yao Ming gets close to the basket, then he scores a lot of points.
Therefore, Yao Ming scores a lot of points.


\subsection{Quantifiers}

Quantifiers are of paramount importance in rigorous proofs.
They are employed to make statements about collections of elements.
Universal quantification is used to formalize the notion that a statement is true for all possible values of a collection.
The \defn{logic}{universal quantifier} is typically denoted by $\forall$ and it is informally read ``for all.''
Let $U$ be a specific collection of elements, and let $P(x)$ be a statement that applies to $x$.
Then the statement $\forall x \in U, P(x)$ is true if $P(x)$ is true for all values of $x$ in $U$.
The other type of quantifier often encountered in mathematical proofs is the \defn{logic}{existential quantifier}, denoted $\exists$.
The statement $\exists x \in U, P(x)$ is true if $P(x)$ is true for at least one value of $x$ in $U$.
Based on the meanings of the quantifiers, we have the following equivalences
\begin{gather*}
\neg \left( \left( \forall x \in U \right) P(x) \right)
\Leftrightarrow \left( \exists x \in U \right) \left( \neg P(x) \right) \\
\neg \left( \left( \exists x \in U \right) P(x) \right)
\Leftrightarrow \left( \forall x \in U \right) \left( \neg P(x) \right) .
\end{gather*}

In mathematics, a \textbf{free variable} is a notation for a place or places in an expression into which some definite substitution may take place, or with respect to which some operation (e.g. quantification) may take place. 
A \textbf{bound variable} is a variable for which we have no ability to choose the value.


\section{Strategies for Proofs}

The relation between intuition and formal rigor is not a trivial matter.
Intuition tells us what is important, what might be true, and what mathematical tools may be used to prove it.
Rigorous proofs are used to verify that a given statement that appears intuitively true is indeed true.
Ultimately, a mathematical proof is a convincing argument that starts from some premises, and logically deduces the desired conclusion.
Most proofs do not mention the logical rules of inference used in the derivation.
Rather, they focus on the mathematical justification of each step, leaving to the reader the task of filling the logical gaps.
The mathematics is the major issue.
Yet, it is essential that you understand the underlying logic behind the derivation as to not get confused while reading or writing a proof.

True statements in mathematics have different names.
They can be called theorems, propositions, lemmas, corollaries and exercises.
A \defn{logic}{theorem} is a statement that can be proved on the basis of explicitly stated or previously agreed assumptions.
A \defn{logic}{proposition} is a statement not associated with any particular theorem; this term sometimes connotes a statement with a simple proof.
A \defn{logic}{lemma} is a proven proposition which is used as a stepping stone to a larger result rather than an independent statement in itself.
A \defn{logic}{corollary} is a mathematical statement which follows easily from a previously proven statement, typically a mathematical theorem. 
The distinction between these names and their definitions is somewhat arbitrary.
Ultimately, they are all synonymous to a true statement.

A proof should be written in grammatically correct English.
Complete sentences should be used, with full punctuation.
In particular, every sentence should end with a period, even if the sentence ends in a displayed equation.
Mathematical formulas and symbols are parts of sentences, and are treated no differently than words.
One way to learn to construct proofs is to read a lot of well written proofs, to write progressively more difficult proofs, and to get detailed feedback on the proofs you write.


\paragraph{Direct Proof:}

The simplest form of proof for a statement of the form $P \rightarrow Q$ is the \textbf{direct proof}.
First assume that $P$ is true.
Produce a series of steps, each one following from the previous ones, that eventually leads to conclusion $Q$.
It warrants the name ``direct proof'' only to distinguish it from other, more intricate, methods of proof.

\paragraph{Proof by Contrapositive:}
A proof by contrapositive takes advantage of the mathematical equivalence $P \rightarrow Q \Leftrightarrow \neg Q \rightarrow \neg P$.
That is, a proof by contrapositive begins by assuming that $Q$ is false (i.e., $\neg Q$ is true).
It then produces a series of direct implications leading to the conclusion that $P$ is false (i.e., $\neg P$ is true).
It follows that $Q$ cannot be false when $P$ is true, so $P \rightarrow Q$.

\paragraph{Proof by Contradiction:}
A proof by contradiction is based on the mathematical equivalence $\neg (P \rightarrow Q) \Leftrightarrow P \wedge \neg Q$.
In a proof by contradiction, one starts by assuming that both $P$ and $\neg Q$ are true.
Then, a series of direct implications are given that lead to a logical contradiction.
Hence, $P \wedge \neg Q$ cannot be true and $P \rightarrow Q$.

\begin{example} \label{example:SquareRoot2}
We wish to show that $\sqrt{2}$ is an irrational number.

First, assume that $\sqrt{2}$ is a rational number.
This assumption implies that there exist integers $p$ and $q$ with $q \neq 0$ such that $p/q = \sqrt{2}$.
In fact, we can further assume that the fraction $p/q$ is irreducible.
That is, $p$ and $q$ are coprime integers (they have no common factor greater than 1).
From $p/q = \sqrt{2}$, it follows that $p = \sqrt{2} q$, and so $p^2 = 2 q^2$.
Thus $p^2$ is an even number, which implies that $p$ itself is even (only even numbers have even squares).
Because $p$ is even, there exists an integer $r$ satisfying $p = 2r$.
We then obtain the equation $(2r)^2 = 2q^2$, which is equivalent to $2r^2 = q^2$ after simplification.
Because $2r^2$ is even, it follows that $q^2$ is even, which means that $q$ is also even.
We conclude that $p$ and $q$ are both even.
This contradicts the fact that $p/q$ is irreducible.
Hence, the initial assumption that $\sqrt{2}$ is a rational number must be false.
That is to say, $\sqrt{2}$ is irrational.
\end{example}

\begin{example}
Consider the following statement, which is related to Example~\ref{example:SquareRoot2}.
``If $\sqrt{2}$ is rational, then it can be expressed as an irreducible fraction.''
The contrapositive of this statement is ``If $\sqrt{2}$ cannot be expressed as an irreducible fraction, then it is not rational.''
Above, we proved that $\sqrt{2}$ cannot be expressed as an irreducible fraction and therefore $\sqrt{2}$ is not a rational number.
\end{example}

\section{Set Theory}

Set theory is generally considered to be the foundation of all modern mathematics.
This means that most mathematical objects (numbers, relations, functions, etc.) are defined in terms of sets.
Unfortunately for engineers, set theory is not quite as simple as it seems.
It turns out that simple approaches to set theory include paradoxes (e.g., statements which are both true and false).
These paradoxes can be resolved by putting set theory in a firm axiomatic framework, but that exercise is rather unproductive for engineers.
Instead, we adopt what is called \defn{set theory}{naive set theory} which rigorously defines the operations of set theory without worrying about possible contradictions.
This approach is sufficient for most of mathematics and also acts as a stepping-stone to more formal treatments.

A \defn{set theory}{set} is any collection of objects, mathematical or otherwise.
For example, we can think of the set of all books published in 2007.
The objects in a set are referred to as \defn{set theory}{elements} or members of the set.
The logical statement ``$a$ is a member of the set $A$'' is written
\[ a \in A. \]
Likewise, its logical negation ``$a$ is not a member of the set $A$'' is written $a \notin A$.
Therefore, exactly one of these two statements is true.

One may present a set by listing its elements.
For example, $A= \{ a,e,i,o,u \}$ is the set of standard English vowels.
It is important to note that the order elements are presented is irrelevant and the set $\{ i,o,u,a,e \}$ is the same as $A$.
Likewise, repeated elements have no effect and the set $\{ a,e,i,o,u,e,o \}$ is the same as $A$.
A \defn{set theory}{singleton} set is a set containing exactly one element such as $\{a\}$.

There are a number of standard sets worth mentioning: the \defn{set theory}{integers} $\mathbb{Z}$, the \defn{set theory}{real numbers} $\mathbb{R}$, and the \defn{set theory}{complex numbers} $\mathbb{C}$.
It is possible to construct these sets in a rigorous manner, but instead we will assume their meaning is intuitively clear.
New sets can be defined in terms of old sets using \defn{set theory}{set-builder notation}.
Let $P(x)$ be a logical statement about objects $x$ in the set $X$, then the ``set of elements in $X$ such that $P(x)$ is true'' is denoted by
\[ \{ x\in X | P(x) \}. \]
For example, the set of even integers is given by
\[ \{ x\in \mathbb{Z} | \textrm{``}x\textrm{ is even''} \} = \{ \ldots,-4,-2,0,2,4,\ldots \}. \]
If no element $x\in X$ satisfies the condition, then the result is the \defn{set theory}{empty set} which is denoted $\emptyset$.
Using set-builder notation, we can also recreate the \defn{set theory}{natural numbers} $\mathbb{N}$ and the \defn{set theory}{rational numbers} $\mathbb{Q}$ with
\begin{align*}
\mathbb{N} & = \{ n\in \mathbb{Z} | n\geq 1 \} \\
\mathbb{Q} & = \{ q\in \mathbb{R} | q=a/b, a\in \mathbb{Z}, b\in \mathbb{N} \}.
\end{align*}
The following standard notation is used for interval subsets of the real numbers:
\begin{align*}
\textrm{Open interval:} \; &  (a,b) \triangleq \{ x\in \mathbb{R} | a<x<b \} \\
\textrm{Closed interval:} \; & [a,b] \;\;\!\! \triangleq \{ x\in \mathbb{R} | a \leq x \leq b \} \\
\textrm{Half-open intervals:} \; & (a,b] \;\! \triangleq \{ x\in \mathbb{R} | a < x \leq b \} \\
& [a,b) \;\! \triangleq \{ x\in \mathbb{R} | a \leq x < b \}
\end{align*}

\begin{example}[\defn{set theory}{Russell's Paradox}]
Let $R$ be the set of all sets that do not contain themselves or $R = \{S|S\notin S\}$.
The paradox arises from the fact that the definition leads to the logical contradiction $R\in R \Leftrightarrow R\notin R$.
Axiomatic set theory eliminates this paradox by disallowing self-referencing constructions like this.
\end{example}

There are also some standard relations defined between any two sets $A,B$.
\begin{definition}
We say that $A$ \textbf{equals} $B$ (denoted $A=B$) if, for all $x$,  $x\in A$ iff $x\in B$.
This means that
\[ A=B \Leftrightarrow \forall x \left( (x\in A)\leftrightarrow (x\in B) \right). \]
\end{definition}

\begin{definition}
We say that $A$ is a \defn{set theory}{subset} of $B$ (denoted $A\subseteq B$) if, for all $x$, if $x\in A$ then $x\in B$.
This means that
\[ A\subseteq B  \Leftrightarrow \forall x \left( (x\in A)\rightarrow (x\in B) \right). \]
It is a \textbf{proper subset} (denoted $A\subset B$) if $A\subseteq B$ and $A\neq B$.
\end{definition}

There are also a number of operations between sets.
Let $A,B$ be any two sets.
\begin{definition}
The \defn{set theory}{union} of $A$ and $B$ (denoted $A\cup B$) is the set of elements in either $A$ or $B$.
This means that $A \cup B = \{ x\in A \textrm{ or } x\in B \}$ is also defined by
\[ x\in A\cup B \Leftrightarrow (x\in A)\vee (x\in B). \]
\end{definition}

\begin{definition}
The \defn{set theory}{intersection} of $A$ and $B$ (denoted $A\cap B$) is the set of elements in both $A$ and $B$.
This means that $A\cap B = \{x\in A | x\in B \}$ is also defined by
\[ x\in A\cap B \Leftrightarrow (x\in A)\wedge (x\in B). \]
Two sets are said to be \defn{set theory}{disjoint} if $A\cap B = \emptyset$.
\end{definition}

\begin{definition}
The \defn{set theory}{set difference} between $A$ and $B$ (denoted $A-B$ or $A \setminus \!B$) is the set of elements in $A$ but not in $B$
This means that
\[ x\in A-B \Leftrightarrow (x\in A)\wedge (x\notin B). \]
If there is some implied universal set $U$, then the \defn{set theory}{complement} (denoted $A^c$) is defined by $A^c = U-A$
\end{definition}

One can apply De Morgan's Law in set theory to verify that
\begin{align*}
(A \cup B)^c & = A^c \cap B^c \\
(A \cap B)^c & = A^c \cup B^c,
\end{align*}
which allows us to interchange union or intersection with set difference.

We can also form the union or the intersection of arbitrarily many sets.
This is defined in a straightforward way,
\begin{align*}
\bigcup_{\alpha \in I} S_{\alpha}
&= \{ x | x \in S_{\alpha} \text{ for some } \alpha \in I \} \\
\bigcap_{\alpha \in I} S_{\alpha}
&= \{ x | x \in S_{\alpha} \text{ for all } \alpha \in I \} .
\end{align*}
It is worth noting that the definitions apply whether the index set is finite, countably infinite, or even uncountably infinite.

Another way to build sets is by grouping elements into pairs, triples, and vectors.
\begin{definition}
The \defn{set theory}{Cartesian Product}, denoted $A\times B$, of two sets is the set of ordered pairs $\{(a,b) | a\in A, b\in B\}$.
For $n$-tuples taken from the same set, the notation $A^n$ denotes the $n$-fold product $A\times A \times \cdots \times A$.
\end{definition}
\begin{example}
If $A  = \{ a,b \}$, then the set of all 3-tuples from $A$ is given by 
\begin{equation*}
A^3 = \{ (a,a,a),(a,a,b),(a,b,a),(a,b,b),(b,a,a),(b,a,b),(b,b,a),(b,b,b) \}.
\end{equation*}
\end{example}
The countably infinite product of $X$, denoted $X^\omega$, is the set of infinite sequences $(x_1,x_2,x_3,\ldots)$ where $x_n \in X$ is arbitrary for $n\in \mathbb{N}$.
If the sequences are restricted to have only a finite number of non-zero terms, then the set is usually denoted $X^\infty$.

One can also formalize relationships between elements of a set.
A \textbf{relation} $\sim$ between elements of the set $A$ is defined by the pairs $(x,y)\in A\times A$ for which the relation holds.
Specifically, the relation is defined by the subset of ordered pairs $E\subseteq A\times A$ where the relation $a\sim b$ holds; so $x\sim y$ if and only if $(x,y)\in E$.
A relation on $A$ is said to be:
\begin{enumerate}
\item Reflexive if $x\sim x$ holds for all $x\in A$
\item Symmetric if $x\sim y$ implies $y\sim x$ for all $x,y\in A$
\item Transitive if $x\sim y$ and $y\sim z$, then $x\sim z$ for all $x,y,z\in A$
\end{enumerate}

A relation is called an \defn{set theory}{equivalence relation} if it is reflexive, symmetric, and transitive.
For example, let $A$ be a set of people and $P(x,y)$ be the statement ``$x$ has the same birthday (month and day) as $y$.''
Then, we can define $\sim$ such that $a\sim b$ holds if and only if $P(x,y)$ is true.
In this case, the set $E$ is given by $E = \{ (x,y)\in A\times A | P(x,y) \}$.
One can verify that this is an equivalence relation by checking that it is reflexive, symmetric, and transitive.

One important characteristic of an equivalence relation is that it partitions the entire set $A$ into disjoint \defn{set theory}{equivalence classes}.
The equivalence class associated with $a\in A$ is given by  $[a] = \left\{x\in A | x \sim a\right\}$.
In the birthday example, there is a natural equivalence class associated with each day of the year.
The set of all equivalence classes is called the \defn{set theory}{quotient set} and is denoted $A \setminus \!\sim = \{ [a] | a\in A \}$.

In fact, there is a natural equivalence relation defined by any disjoint partition of a set.
For example, let $A_{i,j}$ be the set of people in $A$ whose birthday was on the $j$-th day of the $i$-th month.
It follows that $x\sim y$ if and only if there exists a unique pair $i,j$ such that $x,y \in A_{i,j}$.
In this case, the days of year are used as equivalence classes to define the equivalence relation.

\begin{example}
Consider the set $\mathbb{N} \times \mathbb{N} = \{ (a,b) | a,b\in\mathbb{N} \}$ of ordered pairs of natural numbers.
If one associates the element $(a,b)$ with the fraction $a/b$, then the entire set is associated with the set of (possibly reducible) fractions.
Now, consider the equivalence relation $(a,b) \sim (c,d)$ if $ad=bc$.
In this case, two ordered pairs are equivalent if their associated fractions evaluate to the same real number.
The quotient set $\mathbb{N} \setminus \!\sim$ can therefore be associated with the set of reduced fractions.
\end{example}


\section{Functions}

In elementary mathematics, functions are typically described in terms of graphs and formulas.
The drawback of this approach is that one tends to picture only ``nice'' functions.
In fact, Cauchy himself published in 1821 an incorrect proof of the false assertion that ``a sequence of continuous functions that converges everywhere has a continuous limit function.''
Nowadays, every teacher warns their students that one must be careful because the world is filled with not so ``nice'' functions.

\index{functions}
The modern approach to defining functions is based on set theory.
A \textbf{function} $f \colon X\rightarrow Y$ is a rule that assigns a single value $f(x)\in Y$ to each element $x\in X$.
The notation $f \colon X \rightarrow Y$ is used to emphasize the role of the \defn{functions}{domain} $X$ and the \defn{functions}{codomain} $Y$.
The \textbf{range} of $f$ is the subset of $Y$ which is actually achieved by $f$, $\{f(x) \in Y | x\in X \}$.
Since the term codomain is somewhat uncommon, people often use the term range instead of codomain either intentionally (for simplicity) or unintentionally (due to confusion).
\begin{definition}
Formally, a \textbf{function} $f \colon X \rightarrow Y$ from $X$ to $Y$ is defined by a subset $F \subset X \times Y$ such that $A_x = \{ y\in Y | (x,y)\in F \}$ has exactly one element for each $x\in X$.
The \textbf{value} of $f$ at $x\in X$, denoted $f(x)$, is the unique element of $Y$ contained in $A_x$.
\end{definition}

Two functions are said to be equal if they have the same domain, codomain, and value for all elements of the domain.
A function $f$ is called:
\begin{enumerate}
\item \defn{functions}{one-to-one} or \defn{functions}{injective} if, for all $x,x'\in X$, if $f(x)=f(x')$ then $x=x'$;
\item \defn{functions}{onto} or \defn{functions}{surjective} if its range $\{ f(x) | x\in X\}$ equals $Y$;
\item a \defn{functions}{one-to-one correspondence} or \defn{functions}{bijective} if it is both one-to-one and onto.
\end{enumerate}
A bijective function $f \colon X\rightarrow Y$ has a unique \defn{functions}{inverse function} $f^{-1} \colon Y\rightarrow X$ such that $f^{-1}(f(x)) = x$ for all $x\in X$ and $f(f^{-1}(y)) = y$ for all $y\in Y$.
In fact, any one-to-one function $f \colon X\rightarrow Y$ can be transformed into a bijective function $g \colon X \rightarrow R$ with $g(x)=f(x)$ by restricting its codomain $Y$ to its range $R$.

Functions can also be applied to sets in a natural way.
For a function $f \colon X\rightarrow Y$ and subset $A\subseteq X$, the \defn{functions}{image} of $A$ under $f$ is
\[ f(A) \triangleq \{ y\in Y | \exists x\in A \textrm{ s.t. } f(x)=y\} = \{f(x) | x\in A\}. \]
Using this definition, we see that the range of $f$ is simply $f(X)$.
One benefit of allowing functions to have set-valued images is that a set-valued inverse function always exists.
The \defn{functions}{inverse image} or \defn{functions}{preimage} of a subset $B\subseteq Y$ is
\[ f^{-1}(B) \triangleq \{ x\in X | f(x)\in B\}. \]
For a one-to-one function $f$, the inverse image of any singleton set $\{ f(x) \}$ is the singleton set $\{ x \}$.
It is worth noting that the notation $f^{-1}(B)$ for the preimage of $B$ can be somewhat misleading because, in some cases, $f^{-1}(f(A)) \neq A$.
In general, a function gives rise to the following property, $f(f^{-1}(B)) \subseteq B$ and $f^{-1}(f(A)) \supseteq A$.

\begin{example}
Let the function $f \colon \mathbb{R} \rightarrow \mathbb{R}$ be defined by $f(x)=x^2$. Let $A=[1,2]$ and $B = f(A) = [1,4]$.  Then,
\[ f^{-1}(B) = f^{-1}([1,4]) = [-2,-1] \cup [1,2] \supseteq A. \]
\end{example}
\begin{example}
Let the function $f \colon \mathbb{R} \rightarrow \mathbb{R}$ be defined by $f(x)=x^2+1$. Let $B=[0,2]$ and $A = f^{-1}(B) = [-1,1]$.  Then,
\[ f(A) = f([-1,1]) = [1,2] \subseteq B. \]
\end{example}

\begin{problem}
\label{problem:SetFunctionInverse}
For all $f:X\rightarrow Y$, $A\subseteq X$, and $B\subseteq Y$, we have the rules:
\begin{align*}
(a) & \; x \in A \Rightarrow f(x) \in f(A) &
(b) & \; y \in f(A) \Rightarrow \exists x\in A \textrm{ s.t. } f(x)=y \\
(c) & \; x\in f^{-1}(B) \Rightarrow f(x) \in B &
(d) & \; f(x) \in B \Rightarrow x\in f^{-1}(B) .
\end{align*}
Use these rules to show that $f^{-1}(f(A)) \supseteq A$ and $f(f^{-1}(B)) \subseteq B$.
\end{problem}
\noindent \textbf{Solution~\ref{problem:SetFunctionInverse}.}
The first result follows from
\[ x\in A \; \stackrel{(a)}{\Rightarrow} \; f(x)\in f(A) \; \stackrel{(d)}{\Rightarrow} \; x\in f^{-1}(f(A)), \]
and the definition of subset.
The second result follows from
\[ y\in f(f^{-1}(B)) \stackrel{(b)}{\Rightarrow} \exists x\in f^{-1}(B) \textrm{ s.t. } f(x)=y \, \stackrel{(c)}{\Rightarrow} y\in B, \]
and the definition of subset.

\begin{problem}
Let $f \colon X\rightarrow Y$, $A_i \subseteq X$ for all $i\in I$, and $B_i \subseteq Y$ for all $i\in I$.
Show that the following expressions hold:
\begin{align*}
(1) & \;\;\;\:\, f \left( \bigcup_{i\in I} A_i \right) = \bigcup_{i\in I} f \left( A_i \right) &
(2) & \;\;\;\:\, f \left( \bigcap_{i\in I} A_i \right) \subseteq \bigcap_{i\in I} f \left( A_i \right) \\
(3) & \; f^{-1} \left( \bigcup_{i\in I} B_i \right) = \bigcup_{i\in I} f^{-1} \left( B_i \right) &
(4) & \; f^{-1} \left( \bigcap_{i\in I} B_i \right) = \bigcap_{i\in I} f^{-1} \left( B_i \right). 
\end{align*}
\end{problem}

