\chapter{Singular Value Decomposition}


\section{Diagonalization of Hermitian Matrices}

\begin{lemma}[Schur Decomposition]
For any square matrix $A$, there exists a unitary matrix $U$ such that
\begin{equation*}
U^H A U = T
\end{equation*}
where $T$ is upper triangular.
That is, every square matrix is similar to an upper-triangular matrix.
\end{lemma}

\begin{proof}
We prove this lemma by induction on the size $n$ of the matrix.
Since it is clearly true for scalars (i.e., matrices of size $n=1$), the base case is trivial.
Now, suppose that the result holds for all $k=1,2,\ldots,n-1$ and let $A \in \ComplexNumbers^{n \times n}$.
Since every matrix has at least one eigenvector, we let $\vecnot{u}$ be an eigenvector of $A$ normalized so that $\left\| \vecnot{u} \right\|_2 = 1$.
Using the Gram-Schmidt procedure, it is possible to construct an orthonormal basis $\mathcal{B} = \vecnot{x}_1, \ldots, \vecnot{x}_n$ for $\ComplexNumbers^n$, with $\vecnot{x}_1 = \vecnot{u}$.
Define the matrix $U_n$ by
\begin{equation*}
U_n = \begin{bmatrix} \vecnot{x}_1 & \cdots & \vecnot{x}_n \end{bmatrix} .
\end{equation*}
Since $\mathcal{B}$ is a basis for $\ComplexNumbers^n$, every column of the matrix $AU_n$ can be expressed as a linear combination of vectors in $\mathcal{B}$, say,
\begin{equation*}
A \vecnot{x}_i = \sum_{j=1}^n s_{j,i} \vecnot{x}_j
\quad i = 1, \ldots, n .
\end{equation*}
Note that $A \vecnot{x}_1 = \lambda_1 \vecnot{x}_1$ for some $\lambda_1$ since $\vecnot{x}_1 = \vecnot{u}$, an eigenvector of $A$.
We can then write
\begin{equation*}
A U_n = \begin{bmatrix} A \vecnot{x}_1 & \cdots & A \vecnot{x}_n \end{bmatrix}
= U_n \begin{bmatrix} \lambda_1 & s_{1,2} & \ldots & s_{1,n} \\
0 & s_{2,2} & \cdots & s_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & s_{n,2} & \cdots & s_{n,n} \end{bmatrix}
= U_n \begin{bmatrix} \lambda_1 & \vecnot{s}^T \\
\vecnot{0} & A_{n-1} \end{bmatrix} ,
\end{equation*}
where we have used the convenient notation
\begin{equation*}
A_{n-1} = \begin{bmatrix} s_{2,2} & \cdots & \vecnot{s}_{2,n} \\
\vdots & \ddots & \vdots \\
s_{n,2} & \cdots & s_{n,n} \end{bmatrix}
\end{equation*}
and $\vecnot{s}^T = (s_{1,2}, \ldots, s_{1,n})$.
By the inductive hypothesis, we can write $A_{n-1} = U_{n-1} T_{n-1} U_{n-1}^H$ where $T_{n-1}$ is upper triangular and $U_{n-1}$ is unitary.
It follows that
\begin{equation*}
\begin{split}
A U_n &= U_n \begin{bmatrix} \lambda_1 & \vecnot{s}^T \\
\vecnot{0} & A_{n-1} \end{bmatrix}
= U_n \begin{bmatrix} \lambda_1 & \vecnot{s}^T \\
\vecnot{0} & U_{n-1} T_{n-1} U_{n-1}^H \end{bmatrix} \\
&= U_n \begin{bmatrix} 1 & \vecnot{0}^T \\
\vecnot{0} & U_{n-1} \end{bmatrix}
\begin{bmatrix} \lambda_1 & \vecnot{s}^T U_{n-1} \\
\vecnot{0} & T_{n-1} \end{bmatrix}
\begin{bmatrix} 1 & \vecnot{0}^T \\
\vecnot{0} & U_{n-1}^H \end{bmatrix} .
\end{split}
\end{equation*}
Let $U$ be the matrix given by
\begin{equation*}
U = U_n \left[ \begin{array}{cc} 1 & \vecnot{0}^T \\
\vecnot{0} & U_{n-1} \end{array} \right] ,
\end{equation*}
and note that $U$ is unitary.
It follows that
\begin{equation*}
U^H A U = 
\left[ \begin{array}{cc} \lambda_1 & \vecnot{s}^T U_{n-1} \\
\vecnot{0} & T_{n-1} \end{array} \right] .
\end{equation*}
That is, $U$ is a unitary matrix such that $U^H A U$ is upper-triangular.
\end{proof}

We use this lemma to prove the following theorem.

\begin{theorem} \label{theorem:HermitanDiagonalizable}
Every Hermitian $n \times n$ matrix $A$ can be diagonalized by a unitary matrix,
\begin{equation*}
U^H A U = \Lambda,
\end{equation*}
where $U$ is unitary and $\Lambda$ is a diagonal matrix.
\end{theorem}

\begin{proof}
Note that $A^H = A$ and $T = U^H A U$.
Consider the matrix $T^H$ given by
\begin{equation*}
T^H = (U^H A U)^H = U^H A^H U = U^H A U = T.
\end{equation*}
That is, $T$ is also Hermitian.
Since $T$ is upper triangular, this implies that $T$ is a diagonal matrix.
We must conclude that every Hermitian matrix is diagonalized by a unitary matrix.
\end{proof}

This proves every Hermitian matrix has a complete set of orthonormal eigenvectors.


\section{Singular Value Decomposition}

The singular value decomposition (SVD) provides a matrix factorization related to the eigenvalue decomposition that works for all matrices.
In general, any matrix $A \in \mathbb{C}^{m \times n}$ can be factored into a product of unitary matrices and a diagonal matrix, as explained below.

\begin{theorem}
Let $A$ be a matrix in $\ComplexNumbers^{m \times n}$.
Then $A$ can be factored as
\begin{equation*}
A = U \Sigma V^H
\end{equation*}
where $U \in \ComplexNumbers^{m \times m}$ is unitary, $V \in \ComplexNumbers^{n \times n}$ is unitary, and $\Sigma \in \RealNumbers^{m \times n}$ has the form
\begin{equation*}
\Sigma = \Diag ( \sigma_1, \sigma_2, \ldots, \sigma_p ),
\end{equation*}
where $p = \min (m,n)$.
\end{theorem}

The diagonal elements of $\Sigma$ are called the \emph{singular values} of $A$ and are typically ordered so that
\begin{equation*}
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_p \geq 0.
\end{equation*}

\begin{proof}
Let
\begin{equation*}
A^H A V = V \Diag ( \lambda_1, \lambda_2, \ldots, \lambda_n )
\end{equation*}
be the spectral decomposition of $A^H A$, where the columns of $V$ are orthonormal eigenvectors
\begin{equation*}
V = \left[ \begin{array}{cccc} \vecnot{v}_1 & \vecnot{v}_2 & \cdots & \vecnot{v}_n \end{array} \right], 
\end{equation*}
with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_r > 0 = \lambda_{r+1} = \cdots = \lambda_{n}$ and $r \leq p$.
For $i \leq r$, let
\begin{equation*}
\vecnot{u}_i = \frac{A \vecnot{v}_i}{\sqrt{\lambda_i}},
\end{equation*}
and observe that
\begin{equation*}
\left\langle \vecnot{u}_i | \vecnot{u}_j \right\rangle
= \frac{\vecnot{v}_j^H A^H A \vecnot{v}_i}{\sqrt{\lambda_i \lambda_j}}
= \frac{\vecnot{v}_j^H \vecnot{v}_i \lambda_i}{\sqrt{\lambda_i \lambda_j}}
= \delta_{ij} .
\end{equation*}
Also note that $\{ \vecnot{u}_i \}$ are eigenvectors of $A A^H$ since
\begin{equation*}
A A^H \vecnot{u}_i
= A A^H A \frac{\vecnot{v}_i}{\sqrt{\lambda_i}}
= \sqrt{\lambda_i} A \vecnot{v}_i
= \lambda_i \vecnot{u}_i .
\end{equation*}
The set $\left\{ \vecnot{u}_i: i = 1, \ldots, r \right\}$ can be extended using the Gram-Schmidt procedure to form an orthonormal basis for $\ComplexNumbers^m$.
Let
\begin{equation*}
U = \left[ \begin{array}{ccc} \vecnot{u}_1 & \cdots & \vecnot{u}_m \end{array} \right] .
\end{equation*}
For the zero eigenvalues, the eigenvectors must come from the nullspace of $A A^H$ since the eigenvectors with zero eigenvalues are, by construction, orthogonal to the eigenvectors with nonzero eigenvalues that are in the range of $A A^H$.

For $\vecnot{u}_i$ where $i \leq r$, we get
\begin{equation*}
\vecnot{u}_i^H A V 
= \frac{1}{\sqrt{\lambda_i}} \vecnot{v}_i^H A^H A V
= \sqrt{\lambda_i} \vecnot{e}_i^H .
\end{equation*}
On the other hand, if $i > r$ then $\vecnot{u}_i^H A V = \vecnot{0}$.
Hence,
\begin{equation*}
U^H A V = \Diag \left( \sqrt{\lambda_1}, \ldots, \sqrt{\lambda_n} \right)
= \Sigma ,
\end{equation*}
as desired.
\end{proof}

This proof gives a recipe for computing the SVD of an arbitrary matrix.
Consider the matrix 
\[ A = \left[ \begin{array}{cc}
1 & 1 \\
5 & -1 \\
-1 & 5 
\end{array} \right]. \]
The eigenvalue decomposition of $A^H A$ is given by
\[ A^H A = \left[ \begin{array}{cc}
27 & -9 \\
-9 & 27
\end{array} \right]
= V \Lambda V^H
= \left( \frac{1}{\sqrt{2}}\left[ \begin{array}{cc}
-1 & 1 \\
1 & 1 
\end{array} \right] \right)
\left[ \begin{array}{cc}
36 & 0 \\
0 & 18
\end{array} \right]
\left( \frac{1}{\sqrt{2}} \left[ \begin{array}{cc}
-1 & 1 \\
1 & 1 
\end{array} \right] \right).
\]
This implies that $\Sigma_1 = \Lambda^{1/2}$ and $V_1 = V$.
Therefore, we can compute $U_1 = A V_1 \Sigma_1^{-1}$ with
\[ U_1 = \left[ \begin{array}{cc}
1 & 1 \\
5 & -1 \\
-1 & 5 
\end{array} \right]
\left( \frac{1}{\sqrt{2}} \left[ \begin{array}{cc}
-1 & 1 \\
1 & 1
\end{array} \right] \right)
\left[ \begin{array}{cc}
\frac{1}{\sqrt{36}} & 0 \\
0 & \frac{1}{\sqrt{18}}
\end{array} \right]
= \left[ \begin{array}{cc}
0 & \frac{1}{3} \\
\frac{1}{\sqrt{2}} & \frac{2}{3}  \\
-\frac{1}{\sqrt{2}} & \frac{2}{3} 
\end{array} \right]. \]
Putting this all together, we have the compact SVD
\[ A = U_1 \Sigma_1 V_1^H =
 \left[ \begin{array}{cc}
0 & \frac{1}{3} \\
\frac{1}{\sqrt{2}} & \frac{2}{3}  \\
-\frac{1}{\sqrt{2}} & \frac{2}{3} 
\end{array} \right]
\left[ \begin{array}{cc}
\sqrt{36} & 0 \\
0 & \sqrt{18}
\end{array} \right]
\left( \frac{1}{\sqrt{2}} \left[ \begin{array}{cc}
-1 & 1 \\
1 & 1 
\end{array} \right] \right). \]


\section{Properties of the SVD}

Many of the important properties of the SVD can be understood better by separating the non-zero singular values from the zero singular values.
To do this, we note that every rank $r$ matrix $A \in \ComplexNumbers^{m \times n}$ has a singular value decomposition
\begin{equation*}
A = U \Sigma V^H
= \left[ \begin{array}{cc} U_1 & U_2 \end{array} \right]
\left[ \begin{array}{cc} \Sigma_1 & 0 \\ 0 & 0 \end{array} \right]
\left[ \begin{array}{c} V_1^H \\ V_2^H \end{array} \right]
= U_1 \Sigma_1 V_1^H,
\end{equation*}
where $U \in \ComplexNumbers^{m \times m}$ and $V \in \ComplexNumbers^{n \times n}$ are unitary and $U_1 \in \ComplexNumbers^{m \times r}$, $U_2 \in \ComplexNumbers^{m \times m-r}$, $V_1 \in \ComplexNumbers^{n \times r}$, and $V_2 \in \ComplexNumbers^{n \times n-r}$ have orthonormal columns.
The diagonal matrix $\Sigma_1 \in \RealNumbers^{r \times r}$ contains the non-zero singular values
\[ \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0. \]
The factorization $A = U \Sigma V^H$ is called the \textbf{full SVD} of the matrix $A$ while the factorization $A = U_1 \Sigma_1 V_1$ is called the \defn{matrix}{compact SVD} of $A$.
The compact SVD of a rank-$r$ matrix retains only the $r$ columns of $U,V$ associated with non-zero singular values.

Let $X,Y$ be inner product spaces and let $A$ define a mapping from $X$ to $Y$.
Then, the columns of $V_1$ form an orthonormal basis for the vectors in $X$ that are mapped to non-zero vectors (i.e., $\mathcal{N}(A)^\bot$) while the columns of $V_2$ form an orthonormal basis of $\mathcal{N}(A)$.
Likewise, the columns of $U_1$ form a orthonormal basis for the vectors in $Y$ that lie in the range of $A$ while the vectors in $U_2$ form orthonormal basis for $\mathcal{R}(A)^\bot$.
It follows that the full SVD computes orthonormal bases for all of the four fundamental subspaces of the matrix $A$.
For example, it is easy to show that
\begin{align*}
\mathcal{R}(A) &= \textrm{span} (U_1) \\
\mathcal{R}(A^H) &= \textrm{span} (V_1) \\
\mathcal{N}(A) &= \textrm{span} (V_2) \\
\mathcal{N}(A^H) &= \textrm{span} (U_2)
\end{align*}
To see this, notice that $A \sum_{i=1}^t c_i \vecnot{v}_i = \sum_{i=1}^t c_i \sigma_i \vecnot{u}_i$.

From this, we can compute easily any projection onto a fundamental subspace.
First, we point out that the projection onto the column space of any matrix $W\in \mathbb{C}^{m \times n}$ with orthonormal columns (i.e., $W^H W = I$) is given by
\[ P_W = W (W^H W)^{-1} W^H = W W^H. \]
Therefore, the projection matrices for the fundamental subspaces are given by
\begin{align*}
P_{\mathcal{R}(A)} &= U_1 U_1^H \\
P_{\mathcal{R}(A^H)} &= V_1 V_1^H \\
P_{\mathcal{N}(A)} &= V_2 V_2^H = I - V_1 V_1^H \\
P_{\mathcal{N}(A^H)} &= U_2 U_2^H = I - U_1 U_1^H.
\end{align*}

This decomposition also provides a rank revealing decomposition of a rank-$r$ matrix
\[ A = \sum_{i=1}^r \sigma_i \vecnot{u}_i \vecnot{v}_i^H, \]
where $\vecnot{u}_i$ is the $i$th column of $U$ and $\vecnot{v}$ is the $i$th column of $V$.
This shows $A$ as the sum of $r$ rank-1 matrices.
It also allows one to compute
\begin{align*}
\| A \|_F &= \sum_{i=1}^r \sigma_i^2 \\
\| A \|_2 &= \sigma_1
\end{align*}

The pseudoinverse of $A$ is also very easy to compute from the SVD.
In particular, one finds that
\[ A^\dagger = V \Sigma^\dagger U^H = V_1 \Sigma_1^{-1} U_1^H. \]
One can verify this by computing $A^\dagger A$ and $A A^\dagger$.
It also follows from the fact that the pseudoinverse of a scalar $\sigma$ is $\sigma^{-1}$ if $\sigma \neq 0$ and zero otherwise.



