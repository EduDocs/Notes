\chapter{Representation and Approximation}

\section{Best Approximation}
\index{inner-product space!best approximation}
Suppose $W$ is a subspace of a Banach space $V$.
For any $\vecnot{v} \in V$, consider the problem of finding a vector $\vecnot{w} \in W$ such that $\left\| \vecnot{v} - \vecnot{w} \right\|$ is as small as possible.
\begin{definition}
The vector $\vecnot{w} \in W$ is a \defn{Banach space}{best approximation} of $\vecnot{v} \in V$ by vectors in $W$ if
\begin{equation*}
\left\| \vecnot{v} - \vecnot{w} \right\| \leq \left\| \vecnot{v} - \vecnot{w}' \right\|
\end{equation*}
for all $\vecnot{w}' \in W$.
\end{definition}
\noindent If $W$ is spanned by the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n \in V$, then we can write
\begin{equation*}
\begin{split}
\vecnot{v} &= \vecnot{w} + \vecnot{e} \\
&= s_1 \vecnot{w}_1 + \cdots + s_n \vecnot{w}_n + \vecnot{e},
\end{split}
\end{equation*}
where $\vecnot{e}$ is the approximation error.

This problem is, in general, rather difficult.
However, if the norm $\| \cdot \|$ corresponds to the induced norm of an inner product, then one can use orthogonal projection and the problem is greatly simplified.
This chapter focuses mainly on computing the best approximation of arbitrary vectors in a Hilbert space.
\begin{theorem} \label{theorem:OrthogonalProjection}
Suppose $W$ is a subspace of a Hilbert space $V$ and $\vecnot{v}$ is a vector in $V$.
Then, we have the following:
\begin{enumerate}
\item The vector $\vecnot{w} \in W$ is the best approximation of $\vecnot{v} \in V$ by vectors in $W$ if and only if $\vecnot{v} - \vecnot{w}$ is orthogonal to every vector in $W$.
\item If a best approximation of $\vecnot{v} \in V$ by vectors in $W$ exists, it is unique.
\item If $W$ has a countable orthogonal basis $\vecnot{w}_1, \vecnot{w}_2, \ldots$ and is closed, then
\begin{equation}
\label{equation:OrthogonalProjectionOrthogonalVectors}
\vecnot{w} = \sum_{i=1}^\infty \frac{ \left\langle \vecnot{v} | \vecnot{w}_i \right\rangle }{ \left\| \vecnot{w}_i \right\|^2 } \vecnot{w}_i
\end{equation}
is the best approximation of $\vecnot{v}$ by vectors in $W$.
\end{enumerate}
\end{theorem}
\begin{proof}
Let $\vecnot{w} \in W$ and suppose $\vecnot{v} - \vecnot{w}$ is orthogonal to every vector in $W$.
Let $\vecnot{w}' \in W$ such that $\vecnot{w}' \neq \vecnot{w}$.
Then $\vecnot{v} - \vecnot{w}' = \left( \vecnot{v} - \vecnot{w} \right) + \left( \vecnot{w} - \vecnot{w}' \right)$ and
\begin{equation} \label{equation:OrthogonalVector}
\begin{split}
\left\| \vecnot{v} - \vecnot{w}' \right\|^2
&= \left\| \vecnot{v} - \vecnot{w} \right\|^2
+ 2 \Real \left\langle \vecnot{v} - \vecnot{w} | \vecnot{w} - \vecnot{w}' \right\rangle
+ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 \\
&= \left\| \vecnot{v} - \vecnot{w} \right\|^2
+ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 \\
&\geq \left\| \vecnot{v} - \vecnot{w} \right\|^2.
\end{split}
\end{equation}
Conversely, suppose that $\left\| \vecnot{v} - \vecnot{w}' \right\| \geq \left\| \vecnot{v} - \vecnot{w} \right\|$ for every $\vecnot{w}' \in W$.
From \eqref{equation:OrthogonalVector}, we get
\begin{equation*}
2 \Real \left\langle \vecnot{v} - \vecnot{w} | \vecnot{w} - \vecnot{w}' \right\rangle
+ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 \geq 0
\end{equation*}
for all $\vecnot{w}' \in W$.
Note that every vector in $W$ can be expressed as $\vecnot{w} - \vecnot{w}'$ where $\vecnot{w}' \in W$, it follows that
\begin{equation} \label{equation:InequalityOrthogonalVectors}
2 \Real \left\langle \vecnot{v} - \vecnot{w} | \vecnot{w}'' \right\rangle
+ \left\| \vecnot{w}'' \right\|^2 \geq 0
\end{equation}
for every $\vecnot{w}'' \in W$.
If $\vecnot{w}'$ is in $W$ and $\vecnot{w}' \neq \vecnot{w}$ then we may take
\begin{equation*}
\vecnot{w}'' = - \frac{ \left\langle  \vecnot{w} - \vecnot{w}' | \vecnot{v} - \vecnot{w} \right\rangle }{ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 } \left( \vecnot{w} - \vecnot{w}' \right).
\end{equation*}
Inequality~\eqref{equation:InequalityOrthogonalVectors} then reduces to the statement
\begin{equation*}
- 2 \frac{ \left| \left\langle \vecnot{v} - \vecnot{w} | \vecnot{w} - \vecnot{w}' \right\rangle \right|^2 }{ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 }
+ \frac{ \left| \left\langle \vecnot{v} - \vecnot{w} | \vecnot{w} - \vecnot{w}' \right\rangle \right|^2 }{ \left\| \vecnot{w} - \vecnot{w}' \right\|^2 }
\geq 0.
\end{equation*}
This inequality holds if and only if $\left\langle \vecnot{v} - \vecnot{w} | \vecnot{w} - \vecnot{w}' \right\rangle = 0$.
Therefore, $\vecnot{v} - \vecnot{w}$ is orthogonal to every vector in $W$.
Hence the vector $\vecnot{w} \in W$ is a best approximation of $\vecnot{v} \in V$ by vectors in $W$ if and only if $\vecnot{v} - \vecnot{w}$ is orthogonal to every vector in $W$.

Suppose $\vecnot{w}, \vecnot{w}' \in W$ are best approximations of $\vecnot{v}$ by vectors in $W$.
Then $\left\| \vecnot{v} - \vecnot{w} \right\| = \left\| \vecnot{v} - \vecnot{w}' \right\|$ and \eqref{equation:OrthogonalVector} implies that $\left\| \vecnot{w} - \vecnot{w}' \right\| = 0$.
That is, if a best approximation exists then it is unique.

Assume that $W$ is closed and has a countable orthogonal basis $\vecnot{w}_1,\vecnot{w}_2, \ldots$ and let $\vecnot{w}$ be defined by \eqref{equation:OrthogonalProjectionOrthogonalVectors}.
Then $\vecnot{v} -\vecnot{w}$ is orthogonal to $\vecnot{w}_j$ for $j \in \mathbb{N}$, i.e.,
\begin{equation*}
\begin{split}
\left\langle \vecnot{v} - \vecnot{w} | \vecnot{w}_j \right\rangle
&= \left\langle \vecnot{v} | \vecnot{w}_j \right\rangle
- \left\langle \sum_{i=1}^\infty \frac{ \left\langle \vecnot{v} | \vecnot{w}_i \right\rangle }{ \left\| \vecnot{w}_i \right\|^2 } \vecnot{w}_i \Big| \vecnot{w}_j \right\rangle \\
&= \left\langle \vecnot{v} | \vecnot{w}_j \right\rangle
- \frac{ \left\langle \vecnot{v} | \vecnot{w}_j \right\rangle }{ \left\| \vecnot{w}_i \right\|^2 } \left\langle \vecnot{w}_j | \vecnot{w}_j \right\rangle
= 0.
\end{split}
\end{equation*}
That is, $\vecnot{v} - \vecnot{w}$ is orthogonal to every vector in $W$ and therefore $\vecnot{w}$ is the best approximation of $\vecnot{v}$ by vectors in $W$.
\end{proof}

\begin{definition}
Whenever the vector $\vecnot{w}$ in Theorem~\ref{theorem:OrthogonalProjection} exists, it is called the \textbf{orthogonal projection} of $\vecnot{v}$ onto $W$.
If every vector in $V$ has an orthogonal projection onto $W$, then the mapping $E \colon V \rightarrow W$, which assigns to each vector in $V$ its orthogonal projection onto $W$, is called the orthogonal projection of $V$ onto $W$.
\end{definition}

One can use Theorem~\ref{theorem:OrthogonalSubspaceDirectSum} to verify that this is consistent with the concept of orthogonal projection from Definition~\ref{definition:OrthogonalProjection}.


\begin{problem}
Let $W$ be the subspace of $\RealNumbers^2$ spanned by the vector $(1,2)$.
Using the standard inner product, let $E$ be the orthogonal projection of $\RealNumbers^2$ onto $W$.
Find
\begin{enumerate}
\item a formula for $E(x_1, x_2)$
\item the matrix of $E$ in the standard ordered basis, i.e., $E(x_1, x_2) = E \vecnot{x}$
\item $W^{\bot}$
\item an orthonormal basis in which $E$ is represented by the matrix
\begin{equation*}
E = \left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right].
\end{equation*}
\end{enumerate}
\end{problem}

%\begin{corollary}
%Let $V$ be an inner-product space, $W$ be a finite-dimensional subspace, and $E$ be the orthogonal projection of $V$ on $W$.
%Then the mapping
%\begin{equation*}
%\vecnot{v} \mapsto \vecnot{v} - E \vecnot{v}
%\end{equation*}
%is the orthogonal projection of $V$ on $W^{\bot}$.
%\end{corollary}
%\begin{proof}
%Let $\vecnot{v}$ be any vector in $V$.
%Then, $\vecnot{v} - E\vecnot{v}$ is in $W^{\bot}$, and for any $\vecnot{u}$ in $W^{\bot}$, $\vecnot{v} - \vecnot{u} = E \vecnot{v} + \left( \vecnot{v} - E \vecnot{v} - \vecnot{u} \right)$.
%Since $E \vecnot{v} \in W$ and $\vecnot{v} - E \vecnot{v} - \vecnot{u} \in W^{\bot}$, it follows that
%\begin{equation*}
%\begin{split}
%\left\| \vecnot{v} - \vecnot{u} \right\|^2 &= \left\| E \vecnot{v} \right\|^2 + \left\| \vecnot{v} - E \vecnot{v} - \vecnot{u} \right\|^2 \\
%&\geq \left\| \vecnot{v} - \left( \vecnot{v} - E \vecnot{v} \right) \right\|^2
%\end{split}
%\end{equation*}
%with strict inequality when $\vecnot{u} \neq \vecnot{v} - E \vecnot{v}$.
%Thus, $\vecnot{v} - E \vecnot{v}$ is the best approximation of $\vecnot{v}$ by vectors in $W^{\bot}$.
%\end{proof}


\subsection{Projection Operators}

\begin{definition}
A function $F \colon X \rightarrow Y$ with $Y \subseteq X$ is \defn{linear transform}{idempotent} if $F(F(x))=F(x)$.  When $F$ is a linear transformation, this reduces to $F^2 = F \cdot F = F$.
\end{definition}

\begin{definition}
Let $V$ be a vector space and $T \colon V \rightarrow V$ be a linear transformation.
Then, $T$ is a projection if $T$ is idempotent.
\end{definition}

\begin{example}
The idempotent matrix $A$ is a projection onto the first two coordinates.
\begin{equation*}
A = \begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 0 & 0
\end{bmatrix}
\end{equation*}
\end{example}

\begin{theorem}
Let $V$ be a vector space and $T \colon V \rightarrow V$ be a projection operator.
Then, the range $\mathcal{R}(T)$ and the $\mathcal{N}(T)$ are disjoint subspaces of $V$.
\end{theorem}
\begin{proof}
For all $\vecnot{v} \in V - \{0\}$, we need to prove that $\vecnot{v}$ is not in both the range and nullspace.
Let $\vecnot{v}\in V$ be in the range of $T$ so that there is a $\vecnot{w}\in V$ such that $T \vecnot{w} = \vecnot{v}$.
Then, $T \vecnot{v} = T^2 \vecnot{w} = T \vecnot{w} = \vecnot{v}$ and $\vecnot{v}$ is not in the null space unless $\vecnot{v} = \vecnot{0}$.

Let $\vecnot{v}$ be in the null space of $T$, then $T \vecnot{v} = \vecnot{0}$.
But, $T \vecnot{v} = \vecnot{v}$ for all $\vecnot{v}$ in the range.
Therefore, $\vecnot{v}$ is not in the range unless $\vecnot{v} = \vecnot{0}$.
From this, we see that only $\vecnot{0}\in V$ is in both the range and nullspace.
Therefore, they are disjoint subspaces.
\end{proof}

\begin{example}
Consider the linear transform $T \colon V \rightarrow V$ defined by $T = I - P$, where $P$ is a projection.
It is easy to verify that $T$ is a projection operator because
\[ T^2 = (I-P)(I-P) = I - P- P + P^2 = I-P = T .\]
Notice also that $P(I-P) \vecnot{v} = \vecnot{0}$ implies that $\mathcal{R}(T) \subseteq \mathcal{N}(P)$ and $T \vecnot{v} = \vecnot{v}$ for $\vecnot{v} \in \mathcal{N}(P)$ implies $\mathcal{N}(P) \subseteq \mathcal{R}(T)$.
Therefore, $\mathcal{R}(T) = \mathcal{N}(P)$ and $I-P$ is a projection onto $\mathcal{N}(P)$.
\end{example}

\begin{definition}
\label{definition:OrthogonalProjection}
Let $V$ be an inner-product space and $P \colon V \rightarrow V$ be a projection operator.
Then, $P$ is an \defn{linear transform}{orthogonal projection} if $\mathcal{R}(P) \bot \mathcal{N}(P)$.
\end{definition}

\begin{example}
Let $V$ be an inner-product space and $P \colon V \rightarrow V$ be an orthogonal projection.
Then, $\vecnot{v} = P \vecnot{v} + (I-P) \vecnot{v}$ defines a natural orthogonal decomposition of $\vecnot{v}$ because $P \vecnot{v} \in \mathcal{R}(P)$ and $(I-P) \vecnot{v} \in \mathcal{N}(P)$.
Therefore, $\mathcal{N}(P) = \mathcal{R}(P)^{\bot}$ and $P \vecnot{v} = \vecnot{v}$ if and only if $\vecnot{v} \in \mathcal{R}(P)$.
\end{example}

\begin{theorem}
Any idempotent Hermitian matrix $P$ defines an orthogonal projection operator.
\end{theorem}
\begin{proof}
We simply must verify that the range a null space are orthogonal.
Since $P \vecnot{u} \in \mathcal{R}(P)$ and $(I-P) \vecnot{v} \in \mathcal{N}(P)$, we can simply point out that
\[ \langle P\vecnot{u} | (I-P)\vecnot{v} \rangle = \vecnot{v}^H (I-P)^H P \vecnot{u} = \vecnot{v}^H (P - P^H P) \vecnot{u} = \vecnot{v}^H (P - P^2) \vecnot{u} = 0. \]
\end{proof}

\begin{theorem} \label{theorem:OrthogonalSubspaceDirectSum}
Suppose $W$ is a closed subspace of a separable Hilbert space $V$ and let $E$ denote the orthogonal projection of $V$ on $W$.
Then, $E$ is an idempotent linear transformation of $V$ onto $W$, $E \vecnot{w}' = \vecnot{0}\,$ iff $\vecnot{w}' \in W^{\bot}$, and
\begin{equation*}
V = W \oplus W^{\bot}.
\end{equation*}
\end{theorem}
\begin{proof}
Let $\vecnot{v}$ be any vector in $V$.
Since $E \vecnot{v}$ is the best approximation of $\vecnot{v}$ by vectors in $W$, it follows that $\vecnot{v} \in W$ implies $E \vecnot{v} = \vecnot{v}$.
Therefore, $E \left( E \vecnot{v} \right) = E \vecnot{v}$ for any $\vecnot{v} \in V$ since $E \vecnot{v} \in W$.
That is, $E^2 = E$ and $E$ is idempotent.

To show that $E$ is a linear transformation, let $\vecnot{w}_1 , \vecnot{w}_2 , \ldots$ be a countable orthonormal basis for $W$ (whose existence follows from Theorem~\ref{theorem:SeparableHilbertSpace}).
Using part 3 of Theorem~\ref{theorem:OrthogonalProjection}, we find that
\begin{align*}
E \left( s_1 \vecnot{v}_1 + \vecnot{v}_2 \right)
& = \sum_{i=1}^\infty \langle  s_1 \vecnot{v}_1 + \vecnot{v}_2 | \vecnot{w}_i \rangle \vecnot{w}_i \\
& = s_1 \sum_{i=1}^\infty \langle  \vecnot{v}_1 | \vecnot{w}_i \rangle \vecnot{w}_i + \sum_{i=1}^\infty \langle  \vecnot{v}_2 | \vecnot{w}_i \rangle \vecnot{w}_i \\
& = s_1 E \vecnot{v}_1 + E \vecnot{v}_2.
\end{align*}
Therefore, $E$ is a linear transformation.
It also follows that $E \vecnot{w}' = \vecnot{0}$ iff $\vecnot{w}' \in W^{\bot}$ because $W^{\bot}$ can be defined by the fact that $\langle \vecnot{w}' | \vecnot{w}_i \rangle = 0$ for $i\in \mathbb{N}$.

\iffalse
To show that $E$ is a linear transformation, consider vectors $\vecnot{v}_1, \vecnot{v}_2 \in V$ and scalar $s \in F$.
Then $\vecnot{v}_1 - E \vecnot{v}_1$ and $\vecnot{v}_2 - E \vecnot{v}_2$ are each orthogonal to every vector in $W$.
The vector
\begin{equation*}
s \left( \vecnot{v}_1 - E \vecnot{v}_1 \right) + \left( \vecnot{v}_2 - E \vecnot{v}_2 \right) = \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) - \left( s E \vecnot{v}_1 + E \vecnot{v}_2 \right)
\end{equation*}
is therefore also orthogonal to every vector in $W$.
Since $s E \vecnot{v}_1 + E \vecnot{v}_2$ is a vector in $W$, it follows from Theorem~\ref{theorem:OrthogonalProjection} that
\begin{equation*}
E \left( s \vecnot{v}_1 + \vecnot{v}_2 \right) = s E \vecnot{v}_1 + E \vecnot{v}_2.
\end{equation*}
That is, $E$ is a linear transformation.

Again, let $\vecnot{v} \in V$.
Then $E \vecnot{v}$ is the unique vector in $W$ such that $\vecnot{v} - E \vecnot{v}$ is in $W^{\bot}$.
In particular, $E \vecnot{v} = \vecnot{0}$ when $\vecnot{v} \in W^{\bot}$.
Conversely, if $E \vecnot{v} = \vecnot{0}$ then $\vecnot{v} \in W^{\bot}$.
Thus $W^{\bot}$ is the nullspace of $E$.
The equation
\begin{equation*}
\vecnot{v} = E \vecnot{v} + \vecnot{v} - E \vecnot{v}
\end{equation*}
shows that $V = W + W^{\bot}$.
Furthermore, $W \cap W^{\bot} = \left\{ \vecnot{0} \right\}$.
Hence $V$ is the direct sum of $W$ and $W^{\bot}$.
\fi

Again, let $\vecnot{v} \in V$ and recall that (by Theorem~\ref{theorem:OrthogonalProjection}) $E \vecnot{v}$ is the unique vector in $W$ such that $\vecnot{v} - E \vecnot{v}$ is in $W^{\bot}$.
Therefore, the equation $\vecnot{v} = E \vecnot{v} + \left( \vecnot{v} - E \vecnot{v} \right)$ gives a unique decomposition of $\vecnot{v}$ into $E \vecnot{v} \in W$ and $\vecnot{v} - E \vecnot{v} \in W^{\bot}$.
This unique decomposition implies that $V$ is the direct sum of $W$ and $W^{\bot}$.
Lastly, one finds from the definition of $W^{\bot}$ that
\[W \cap W^{\bot} = \left\{ \vecnot{u} \in W | \langle \vecnot{u} | \vecnot{w} \rangle =0 \; \forall \; \vecnot{w}\in W \right\}  \subseteq \left\{ \vecnot{u} \in W | \langle \vecnot{u} | \vecnot{u} \rangle =0 \right\}  = \{ \vecnot{0} \} .\]
\end{proof}

\begin{corollary}
Let $W$ be a closed subspace of a separable Hilbert space $V$ and $E$ be the orthogonal projection of $V$ on $W$.
Then $I - E$ is the orthogonal projection of $V$ on $W^{\bot}$.
\end{corollary}
\begin{proof}
This follows directly from the orthogonal decomposition in Theorem~\ref{theorem:OrthogonalSubspaceDirectSum}.
One can also verify that $I-E$ is an idempotent linear transformation of $V$ with range $W^{\bot}$ and nullspace $W$.
From Definition \ref{definition:OrthogonalProjection}, we see that $I-E$ is an orthogonal projection.
\end{proof}

Theorem~\ref{theorem:OrthogonalSubspaceDirectSum} also implies the following result, known as Bessel's inequality.

\begin{corollary}
Let $\vecnot{v}_1, \vecnot{v}_2, \ldots$ be a countable orthogonal set of distinct non-zero vectors in an inner-product space $V$.
If $\vecnot{v} \in V$ then
\begin{equation*}
\sum_{i=1}^\infty \frac{ \left| \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle \right|^2 }{ \left\| \vecnot{v}_i \right\|^2 }
\leq \left\| \vecnot{v} \right\|^2.
\end{equation*}
Moreover, equality holds if and only if
\begin{equation*}
\vecnot{v} = \sum_{i=1}^\infty \frac{ \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle }{ \left\| \vecnot{v}_i \right\|^2 } \vecnot{v}_i.
\end{equation*}
\end{corollary}
\begin{proof}
Define
\begin{equation*}
\vecnot{w} = \sum_{i=1}^\infty \frac{ \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle }{ \left\| \vecnot{v}_i \right\|^2 } \vecnot{v}_i.
\end{equation*}
Then $\vecnot{v} = \vecnot{w} + \vecnot{u}$, where $\left\langle \vecnot{w} | \vecnot{u} \right\rangle = 0$ and $\left\| \vecnot{v} \right\|^2 = \left\| \vecnot{w} \right\|^2 + \left\| \vecnot{u} \right\|^2$.
Noting that
\begin{equation*}
\left\| \vecnot{w} \right\|^2
= \sum_{i=1}^\infty \frac{ \left| \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle \right|^2 }{ \left\| \vecnot{v}_i \right\|^2 },
\end{equation*}
we obtain the desired result, with equality iff $\vecnot{u} = \vecnot{0}$.
\end{proof}

If $\vecnot{v}_1, \vecnot{v}_2, \ldots$ is an orthonormal set, Bessel's inequality states that
\begin{equation*}
\sum_{i=1}^\infty \left| \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle \right|^2 \leq \left\| \vecnot{v} \right\|^2.
\end{equation*}
It follows that the vector $\vecnot{v}$ is in the closure of the subspace spanned by $\vecnot{v}_1, \vecnot{v}_2, \ldots$ if an only if
\begin{equation*}
\vecnot{v} = \sum_{i=1}^\infty \left\langle \vecnot{v} | \vecnot{v}_i \right\rangle \vecnot{v}_i.
\end{equation*}


\section{Computing Approximations in Hilbert Spaces}

\subsection{Normal Equations}

Suppose $V$ is a Hilbert space the subspace $W$ is spanned by $\vecnot{w}_1, \ldots, \vecnot{w}_n \in V$.
Consider the situation where the sequence $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is linearly independent, but not orthogonal.
In this case, it is not possible to apply \eqref{equation:OrthogonalProjectionOrthogonalVectors} directly.
It is nevertheless possible to obtain a similar expression for the best approximation of $\vecnot{v}$ by vectors in $W$.
Theorem~\ref{theorem:OrthogonalProjection} asserts that $\hat{\vecnot{v}} \in W$ is a best approximation of $\vecnot{v} \in V$ by vectors in $W$ if and only if $\vecnot{v} - \hat{\vecnot{v}}$ is orthogonal to every vector in $W$.
This implies that
\begin{equation*}
\left\langle \vecnot{v} - \hat{\vecnot{v}} | \vecnot{w}_j \right\rangle
= \left\langle \vecnot{v} - \sum_{i=1}^n s_i \vecnot{w}_i \Big| \vecnot{w}_j \right\rangle
= 0
\end{equation*}
or, equivalently,
\begin{equation*}
\sum_{i=1}^n s_i \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle
= \left\langle \vecnot{v} | \vecnot{w}_j \right\rangle
\end{equation*}
for $j = 1, \ldots, n$.
These conditions yield a system of $n$ linear equations in $n$ unknowns, which can be written in the matrix form
\begin{equation*}
\left[ \begin{array}{cccc}
\left\langle \vecnot{w}_1 | \vecnot{w}_1 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_1 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{w}_1 | \vecnot{w}_2 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_2 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_2 \right\rangle \\
\vdots & \vdots & \ddots & \vdots \\
\left\langle \vecnot{w}_1 | \vecnot{w}_n \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_n \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_n \right\rangle
\end{array} \right]
\left[ \begin{array}{c}
s_1 \\ s_2 \\ \vdots \\ s_n \end{array} \right]
= \left[ \begin{array}{c}
\left\langle \vecnot{v} | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{v} | \vecnot{w}_2 \right\rangle \\ \vdots \\
\left\langle \vecnot{v} | \vecnot{w}_n \right\rangle \end{array} \right] .
\end{equation*}
We can rewrite this matrix equation as
\begin{equation*}
G \vecnot{s} = \vecnot{t}
\end{equation*}
where
\begin{equation*}
\vecnot{t}^T = 
\left(
\left\langle \vecnot{v} | \vecnot{w}_1 \right\rangle,
\left\langle \vecnot{v} | \vecnot{w}_2 \right\rangle, \ldots,
\left\langle \vecnot{v} | \vecnot{w}_n \right\rangle \right)
\end{equation*}
is the \textbf{cross-correlation vector}, and
\begin{equation*}
\vecnot{s}^T = 
\left( s_1, s_2, \ldots, s_n \right)
\end{equation*}
is the vector of coefficients.
Equations of this form are collectively known as the \defn{inner-product space}{normal equations}.

\begin{definition}
The $n \times n$ matrix
\begin{equation} \label{equation:GrammianMatrix}
G = \left[ \begin{array}{cccc}
\left\langle \vecnot{w}_1 | \vecnot{w}_1 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_1 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{w}_1 | \vecnot{w}_2 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_2 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_2 \right\rangle \\
\vdots & \vdots & \ddots & \vdots \\
\left\langle \vecnot{w}_1 | \vecnot{w}_n \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_n \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_n \right\rangle
\end{array} \right]
\end{equation}
is called the \defn{matrix}{Grammian} matrix.
Since $g_{ji} = \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle$, it follows that the Grammian is a Hermitian symmetric matrix, i.e., $G^H = G$.
\end{definition}

\begin{definition}
A matrix $M\in F^{n \times n}$ is \defn{matrix}{positive-semidefinite} if $\vecnot{v}^H M \vecnot{v} \geq 0$ for all $\vecnot{v} \in F^n$.
A matrix $M\in F^{n \times n}$ is \defn{matrix}{positive-definite} if $\vecnot{v}^H M \vecnot{v} > 0$ for all $\vecnot{v} \in F^n - \left\{ \vecnot{0} \right\}$.
\end{definition}

An important aspect of positive-definite matrices is that they are always invertible.
This follows from noting that $M\vecnot{v} = \vecnot{0}$ for $\vecnot{v}\neq\vecnot{0}$ implies that $\vecnot{v}^H M \vecnot{v} = \vecnot{0}$ and contradicts the definition of positive definite.

\begin{theorem}
A Grammian matrix $G$ is always positive-semidefinite.
Furthermore, it is positive-definite if and only if the sequence of vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is linearly independent.
\end{theorem}
\begin{proof}
Let $\vecnot{v} = \left( v_1, \ldots, v_n \right)^T \in F^n$.
Then,
\begin{equation} \label{equation:PositiveSemiDefiniteProof}
\begin{split}
\vecnot{v}^H G \vecnot{v} &=
\sum_{i=1}^n \sum_{j=1}^n \bar{v}_j g_{ji} v_i
= \sum_{i=1}^n \sum_{j=1}^n \bar{v}_j \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle v_i \\
&= \sum_{i=1}^n \sum_{j=1}^n \left\langle v_i \vecnot{w}_i | v_j \vecnot{w}_j \right\rangle
= \left\langle \sum_{i=1}^n v_i \vecnot{w}_i \Big| \sum_{j=1}^n v_j \vecnot{w}_j \right\rangle \\
&= \left\| \sum_{i=1}^n v_i \vecnot{w}_i \right\|^2
\geq 0.
\end{split}
\end{equation}
That is, $\vecnot{v}^H G \vecnot{v} \geq 0$ for all $\vecnot{v} \in F^n$.

Suppose that $G$ is not positive-definite.
Then, there exists $\vecnot{v} \in F^n - \left\{ \vecnot{0} \right\}$ such that $\vecnot{v}^H G \vecnot{v} = 0$.
By \eqref{equation:PositiveSemiDefiniteProof}, this implies that
\begin{equation*}
\sum_{i=1}^n v_i \vecnot{w}_i = 0
\end{equation*}
and hence the sequence of vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is not linearly independent.

Conversely, if $G$ is positive-definite then $\vecnot{v}^H G \vecnot{v} > 0$ and
\begin{equation*}
\left\| \sum_{i=1}^n v_i \vecnot{w}_i \right\| > 0
\end{equation*}
for all $\vecnot{v} \in F^n - \left\{ \vecnot{0} \right\}$.
Therefore, the sequence of vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ is linearly independent.
\end{proof}


\subsection{Orthogonality Principle}

\begin{theorem}
Let $\vecnot{w}_1, \ldots, \vecnot{w}_n$ be vectors in an inner-product space $V$ and denote the span of $\vecnot{w}_1, \ldots, \vecnot{w}_n$ by $W$.
For any vector $\vecnot{v} \in V$, the norm of the error vector
\begin{equation} \label{equation:OrthogonalProjectionError}
\vecnot{e} = \vecnot{v} - \sum_{i=1}^n s_i \vecnot{w}_i
\end{equation}
is minimized when the error vector $\vecnot{e}$ is orthogonal to every vector in $W$.
If $\hat{\vecnot{v}}$ denotes the \defn{inner-product space}{least-squares} approximation of $\vecnot{v}$ then
\begin{equation*}
\left\langle \vecnot{v} - \hat{\vecnot{v}} | \vecnot{w}_j \right\rangle = 0
\end{equation*}
for $j = 1, \ldots, n$.
\end{theorem}
\begin{proof}
Minimizing $\left\| \vecnot{e} \right\|^2$ over $\vecnot{s}$, where $\vecnot{e}$ is given by \eqref{equation:OrthogonalProjectionError} requires minimizing
\begin{equation*}
\begin{split}
J \left( \vecnot{s} \right)
&= \left\langle \vecnot{v} - \sum_{i=1}^n s_i \vecnot{w}_i \Big|
\vecnot{v} - \sum_{j=1}^n s_j \vecnot{w}_j \right\rangle \\
&= \left\langle \vecnot{v} | \vecnot{v} \right\rangle
- \sum_{i=1}^n \left\langle s_i \vecnot{w}_i | \vecnot{v} \right\rangle
- \sum_{j=1}^n \left\langle \vecnot{v} | s_j \vecnot{w}_j \right\rangle
+ \sum_{i=1}^n \sum_{j=1}^n \left\langle s_i \vecnot{w}_i | s_j \vecnot{w}_j \right\rangle \\
&= \left\langle \vecnot{v} | \vecnot{v} \right\rangle
- \sum_{i=1}^n s_i \left\langle \vecnot{w}_i | \vecnot{v} \right\rangle
- \sum_{j=1}^n \bar{s}_j \left\langle \vecnot{v} | \vecnot{w}_j \right\rangle
+ \sum_{i=1}^n \sum_{j=1}^n s_i \bar{s}_j \left\langle \vecnot{w}_i | \vecnot{w}_j \right\rangle .
\end{split}
\end{equation*}
Taking the gradient of $J \left( \vecnot{s} \right)$, we get
\begin{equation*}
\begin{split}
\nabla J \left( \vecnot{s} \right)
&= - \left[ \begin{array}{c}
\left\langle \vecnot{v} | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{v} | \vecnot{w}_2 \right\rangle \\
\vdots \\
\left\langle \vecnot{v} | \vecnot{w}_n \right\rangle
\end{array} \right]
+ \left[ \begin{array}{cccc}
\left\langle \vecnot{w}_1 | \vecnot{w}_1 \right\rangle &
\left\langle \vecnot{w}_2 | \vecnot{w}_1 \right\rangle & \hdots &
\left\langle \vecnot{w}_n | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{w}_1 | \vecnot{w}_2 \right\rangle &
\left\langle \vecnot{w}_2 | \vecnot{w}_2 \right\rangle & \hdots &
\left\langle \vecnot{w}_n | \vecnot{w}_2 \right\rangle \\
\vdots & \vdots & \ddots & \vdots \\
\left\langle \vecnot{w}_1 | \vecnot{w}_n \right\rangle &
\left\langle \vecnot{w}_2 | \vecnot{w}_n \right\rangle & \hdots &
\left\langle \vecnot{w}_n | \vecnot{w}_n \right\rangle \\
\end{array} \right]
\left[ \begin{array}{c} s_1 \\ s_2 \\ \vdots \\ s_n
\end{array} \right] \\
&= \vecnot{0} .
\end{split}
\end{equation*}
In matrix form, this yields the familiar equation
\begin{equation*}
G \vecnot{s} = \vecnot{t}.
\end{equation*}
To ensure that this extremum is in fact a minimum, we compute the Hessian matrix
\begin{equation*}
\nabla^2 J \left( \vecnot{s} \right) = G .
\end{equation*}
Since $G$ is a positive-semidefinite matrix, the extremum is indeed a minimum.

This implies that $\left\| \vecnot{e} \right\|^2$ is minimized if and only if $G \vecnot{s} = \vecnot{t}$.
That is, $\left\| \vecnot{e} \right\|^2$ is minimized if and only if $\vecnot{v} - \hat{\vecnot{v}}$ is orthogonal to every vector in $W$.
\end{proof}

Note that it is also possible to prove this theorem using the Cauchy-Schwarz inequality or the projection theorem.


\section{Approximation for Systems of Linear Equations}

\subsection{Matrix Representation}

For finite-dimensional vector spaces, least-squares (i.e., best approximation) problems have natural matrix representations.
Suppose $V = F^m$ and $\vecnot{w}_1, \vecnot{w}_2, \ldots, \vecnot{w}_n \in V$ are column vectors.
Then, the approximation vector is given by
\begin{equation*}
\hat{\vecnot{v}} = \sum_{i=1}^n s_i \vecnot{w}_i
%= \left[ \vecnot{w}_1 \cdots \vecnot{w}_n \right]
%\left[ \begin{array}{c} s_1 \\ \vdots \\ s_n \end{array} \right] .
\end{equation*}
In matrix form, we have
\begin{equation*}
\hat{\vecnot{v}} = A \vecnot{s},
\end{equation*}
where $A = \left[ \vecnot{w}_1 \cdots \vecnot{w}_n \right]$.
The optimization problem can then be reformulated as follows.
Determine $\vecnot{s} \in F^n$ such that
\begin{equation*}
\left\| \vecnot{e} \right\|^2
= \left\| \vecnot{v} - \hat{\vecnot{v}} \right\|^2
= \left\| \vecnot{v} - A \vecnot{s} \right\|^2
\end{equation*}
is minimized.
Note that this occurs when the error vector is orthogonal to every vector in $W$, i.e.,
\begin{equation*}
\left\langle \vecnot{e} | \vecnot{w}_j \right\rangle
= \left\langle \vecnot{v} - \hat{\vecnot{v}} | \vecnot{w}_j \right\rangle
= \left\langle \vecnot{v} - A \vecnot{s} | \vecnot{w}_j \right\rangle
= 0
\end{equation*}
for $j = 1, \ldots, n$.


\subsection{Standard Inner Products}

When $\| \cdot \|$ is the norm induced by the standard inner product, these conditions can be expressed as
\begin{equation*}
\left[ \begin{array}{c} \vecnot{w}_1^H \\ \vdots \\ \vecnot{w}_n^H \end{array} \right] \left( \vecnot{v} - A \vecnot{s} \right) = \vecnot{0} .
\end{equation*}
Using the definition of $A$, we obtain
\begin{equation*}
A^H A \vecnot{s} = A^H \vecnot{v} .
\end{equation*}
The matrix $A^H A$ is the Grammian $G$ defined in \eqref{equation:GrammianMatrix}.
The vector $A^H \vecnot{v}$ is the cross correlation vector $\vecnot{t}$.

When the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ are linearly independent, the Grammian matrix is positive definite and hence invertible.
The optimal solution for the least-squares problem is therefore given by
\begin{equation*}
\vecnot{s} = \left( A^H A \right)^{-1} A^H \vecnot{v} = G^{-1} \vecnot{t} .
\end{equation*}
The matrix $\left( A^H A \right)^{-1} A^H$ is often called the \defn{matrix}{pseudoinverse}.

The best approximation of $\vecnot{v} \in V$ by vectors in $W$ is equal to
\begin{equation*}
\hat{\vecnot{v}} = A \vecnot{s} = A \left( A^H A \right)^{-1} A^H \vecnot{v} .
\end{equation*}
The matrix $P = A \left( A^H A \right)^{-1} A^H$ is called the \defn{matrix}{projection matrix} for the range of $A$.
It defines an orthogonal projection onto the range of $A$ (i.e., the subspace spanned by the columns of $A$).


\subsection{Generalized Inner Products}

We can also consider the case of a general inner product.
Recall that an inner product on $V$ is completely determined by the values
\begin{equation*}
h_{ji} = \left\langle \vecnot{e}_i | \vecnot{e}_j \right\rangle
\end{equation*}
and that it can be expressed in terms of the matrix $H$ (where $[H]_{j,i} = h_{j,i}$) as
\begin{equation*}
\left\langle \vecnot{v} | \vecnot{w} \right\rangle
= \vecnot{w}^H H \vecnot{v}.
\end{equation*}
Minimizing $\left\| \vecnot{e} \right\|^2 = \left\| \vecnot{v} - A \vecnot{s} \right\|^2$ and using the orthogonality principle lead to the matrix equation
\begin{equation*}
A^H H A \vecnot{s} = A^H H \vecnot{v}.
\end{equation*}
When the vectors $\vecnot{w}_1, \ldots, \vecnot{w}_n$ are linearly independent, the optimal solution is given by
\begin{equation*}
\vecnot{s} = \left( A^H H A \right)^{-1} A^H H \vecnot{v}.
\end{equation*}


\subsection{Minimum Error}

Let $\hat{\vecnot{v}} \in W$ be the best approximation of $\vecnot{v}$ by vectors in $W$.
Again, we can write
\begin{equation*}
\vecnot{v} = \hat{\vecnot{v}} + \vecnot{e}
\end{equation*}
where $\vecnot{e} \in W^{\bot}$ is the minimum achievable error.
The squared norm of the minimum error is given implicitly by
\begin{equation*}
\left\| \vecnot{v} \right\|^2
= \left\| \hat{\vecnot{v}} + \vecnot{e} \right\|^2
= \left\langle \hat{\vecnot{v}} + \vecnot{e} | \hat{\vecnot{v}} + \vecnot{e} \right\rangle
= \left\langle \hat{\vecnot{v}} | \hat{\vecnot{v}} \right\rangle
+ \left\langle \vecnot{e} | \vecnot{e} \right\rangle
= \left\| \hat{\vecnot{v}} \right\|^2 + \left\| \vecnot{e} \right\|^2 .
\end{equation*}
We can then find an explicit expression for the approximation error,
\begin{equation*}
\begin{split}
\left\| \vecnot{e} \right\|^2
&= \left\| \vecnot{v} \right\|^2
- \left\| \hat{\vecnot{v}} \right\|^2
= \vecnot{v}^H H \vecnot{v} - \hat{\vecnot{v}}^H H \hat{\vecnot{v}} \\
&= \vecnot{v}^H H \vecnot{v} - \vecnot{s}^H A^H H A \vecnot{s} \\
&= \vecnot{v}^H H \vecnot{v}
- \vecnot{v}^H H A \left( A^H H A \right)^{-1} A^H H \vecnot{v} \\
&= \vecnot{v}^H
\left( H -  H A \left( A^H H A \right)^{-1} A^H H \right)
\vecnot{v}.
\end{split}
\end{equation*}


\section{Applications and Examples in Signal Processing}
\index{applications}

\subsection{Linear Regression}

Let $(x_1, y_1), \ldots, (x_n, y_n)$ be a collection of points in $\RealNumbers^2$.
A \defn{applications}{linear regression} problem consists in finding scalars $a$ and $b$ such that
\begin{equation*}
y_i \approx a x_i + b
\end{equation*}
for $i = 1, \ldots, n$.
Definite the error component $e_i$ by $e_i = y_i - a x_i - b$, then
\begin{equation*}
\left[ \begin{array}{c} y_1 \\ \vdots \\ y_n \end{array} \right]
= a \left[ \begin{array}{c} x_1 \\ \vdots \\ x_n \end{array} \right]
+ b \left[ \begin{array}{c} 1 \\ \vdots \\ 1 \end{array} \right]
+ \left[ \begin{array}{c} e_1 \\ \vdots \\ e_n \end{array} \right]
= \left[ \begin{array}{cc} x_1 & 1 \\
\vdots & \vdots \\ x_n & 1 \end{array} \right]
\left[ \begin{array}{c} a \\ b \end{array} \right]
+ \left[ \begin{array}{c} e_1 \\ \vdots \\ e_n \end{array} \right] .
\end{equation*}
In vector form, we can rewrite this equation as
\begin{equation*}
\vecnot{y} = A \vecnot{s} + \vecnot{e},
\end{equation*}
where $\vecnot{y} = \left( y_1, \ldots, y_n \right)^T$, $\vecnot{s} = (a, b)^T$, $\vecnot{e} = \left( e_1, \ldots, e_n \right)^T$, and
\begin{equation*}
A = \left[ \begin{array}{cc} x_1 & 1 \\
\vdots & \vdots \\ x_n & 1 \end{array} \right] .
\end{equation*}
This equation has a form analog to the matrix representation of a least-squares problems.
Consider the goal of minimizing $\left\| \vecnot{e} \right\|^2$.
The line that minimizes the sums of the squares of the \emph{vertical} distances between the data abscissas and the line is then given by
\begin{equation*}
\vecnot{s} = \left( A^H A \right)^{-1} A^H \vecnot{y} .
\end{equation*}


\subsection{Linear Minimum Mean-Squared Estimation}

Let $Y, X_1, \ldots, X_n$ be a set of zero-mean random variables.
The goal of the linear minimum mean-squared estimation (LMMSE) problem is to find coefficients $s_1, \ldots, s_n$ such that
\[ \hat{Y} = s_1 X_1 + \cdots + s_n X_n. \]
minimizes the MSE $\Expect[|Y-\hat{Y}|^2]$.
Using the inner product defined by
\begin{equation} \label{equation:ExpectationInnerProduct}
\left\langle X | Y \right\rangle = \Expect \left[ X \overline{Y} \right],
\end{equation}
we can compute the linear minimum mean-squared estimate $\hat{Y}$ using
\begin{equation*}
G \vecnot{s} = \vecnot{t},
\end{equation*}
where
\begin{equation*}
G = \left[ \begin{array}{cccc}
\Expect \left[ X_1 \overline{X}_1 \right]
& \Expect \left[ X_2 \overline{X}_1 \right] & \cdots
& \Expect \left[ X_n \overline{X}_1 \right] \\
\Expect \left[ X_1 \overline{X}_2 \right]
& \Expect \left[ X_2 \overline{X}_2 \right] & \cdots
& \Expect \left[ X_n \overline{X}_2 \right] \\
\vdots & \vdots & \ddots & \vdots \\
\Expect \left[ X_1 \overline{X}_n \right]
& \Expect \left[ X_2 \overline{X}_n \right] & \cdots
& \Expect \left[ X_n \overline{X}_n \right] \\
\end{array} \right]
\end{equation*}
and
\begin{equation*}
\vecnot{t} = \left[ \begin{array}{c}
\Expect \left[ Y \overline{X}_1 \right] \\
\Expect \left[ Y \overline{X}_2 \right] \\ \vdots \\
\Expect \left[ Y \overline{X}_n \right] \end{array} \right] .
\end{equation*}
If the matrix $G$ is invertible, the minimum mean-squared error is given by
\begin{equation*}
\left\| Y - \hat{Y} \right\|^2 = \Expect \left[ Y \overline{Y} \right]
- \vecnot{t}^H G^{-1} \vecnot{t} .
\end{equation*}


\subsection{The Wiener Filter}

Suppose that the sequence of zero-mean random variables $\left\{ X[t] \right\}$ is wide-sense stationary, and consider the FIR filter
\begin{equation*}
\begin{split}
Y[t] &= \sum_{k=0}^{K-1} h[k] X[t - k] \\
&= \left[ \begin{array}{ccc} X[t] & \hdots & X[t-K+1] \end{array} \right]
\left[ \begin{array}{c} h[0] \\ \vdots \\ h[K-1] \end{array} \right]
= \left( \vecnot{X}[t] \right)^T \vecnot{h} .
\end{split}
\end{equation*}
The goal is to design this filter in such a way that its output is as close as possible to a desired sequence $\left\{ Z[t] \right\}$.
In particular, we want to minimize the mean-squared error
\begin{equation*}
\left\| Z[t] - Y[t] \right\|^2 = \Expect \left[ \left| Z[t] - Y[t] \right|^2 \right].
\end{equation*}

By the orthogonality principle, the mean-squared error is minimized when the error is orthogonal to the data; that is, for $j = 0, 1, \ldots, K-1$, we have
\begin{equation*}
\left\langle Z[t] - \sum_{k=0}^{K-1} h[k] X[t - k] \Big| X[t - j] \right\rangle = 0,
\end{equation*}
or, equivalently, we can write
\begin{equation*}
\left\langle Z[t] | X[t - j] \right\rangle
= \sum_{k=0}^{K-1} h[k] \left\langle X[t - k] | X[t - j] \right\rangle .
\end{equation*}
Using \eqref{equation:ExpectationInnerProduct}, we obtain
\begin{equation} \label{equation:WienerHopfConditions}
\Expect \left[ Z[t] \overline{X}[t - j] \right]
= \sum_{k=0}^{K-1} h[k] \Expect \left[ X[t - k] \overline{X}[t - j] \right] .
\end{equation}
where $j = 1, \ldots, K-1$.

For this specific case where the normal equations are defined in terms of the expectation operator, these equations are called the \defn{applications}{Wiener-Hopf} equations.
The Grammian of the Wiener-Hopf equations can be expressed in a more familiar form using the autocorrelation matrix.
Recall that $\{ X[t] \}$ is a wide-sense stationary process.
As such, we have
\begin{equation*}
R_{xx} (j-k) = R_{xx} (j, k) = \Expect \left[ X[t-k] \overline{X}[t-j] \right]
= \left\langle X[t-k] | X[t-j] \right\rangle .
\end{equation*}
Also define
\begin{equation*}
R_{zx}(j) = \Expect \left[ Z[t] \overline{X}[t-j] \right]
= \left\langle Z[t] | X[t-j] \right\rangle .
\end{equation*}
Using this notation, we can rewrite \eqref{equation:WienerHopfConditions} as
\begin{equation*}
R_{zx} = \left[ \begin{array}{c}
R_{zx} (0) \\ R_{zx} (1) \\ \vdots \\ R_{zx} (K-1) \end{array} \right]
= R_{xx}
\left[ \begin{array}{c}
h [0] \\
h [1] \\ \vdots \\
h [K-1] \end{array} \right]
\end{equation*}
where the $K \times K$ autocorrelation matrix is given by
\begin{equation*}
R_{xx} = \left[ \begin{array}{cccc}
R_{xx} [0] & \overline{R}_{xx}[1] & \cdots & \overline{R}_{xx}[K-1] \\
R_{xx} [1] & R_{xx}[0] & \cdots & \overline{R}_{xx}[K-2] \\
\vdots & \vdots & \ddots & \vdots \\
R_{xx} [K-1] & R_{xx}[K-2] & \cdots & R_{xx}[0]
\end{array} \right] .
\end{equation*}
Note that the matrix $R_{xx}$ is Toeplitz, i.e., all the elements on a diagonal are equal.
Assuming that $R_{xx}$ is invertible, the optimal filter taps are then given by
\begin{equation*}
\vecnot{h} = R_{xx}^{-1} R_{zx}.
\end{equation*}

The minimum mean-squared error is given by
\begin{equation*}
\begin{split}
\| Z-Y \|^2 &= \| Z \|^2 - \| Y \|^2 \\
&= \Expect [Z \overline{Z}] - \Expect \left[ \vecnot{h}^H \overline{\vecnot{X}} \vecnot{X}^T \vecnot{h} \right] \\
&= \Expect [Z \overline{Z}] - \vecnot{h}^H R_{xx} \vecnot{h} \\
&= \Expect [Z \overline{Z}] - R_{zx}^H \vecnot{h} ,
\end{split}
\end{equation*}
where $t$ can be ignored because the processes are WSS.

\subsection{LMMSE Filtering in Practice}

While theoretical treatments of optimal filtering often assume one has well-defined random variables with known statistics, this is rarely the case in practice.
Yet, there is a very close connection between Wiener filtering and natural data driven approaches.
Consider the problem from the previous section and let $x[1],x[2],\ldots_1,x[N]$ and $z[1],z[2],\ldots,z[N]$ be realizations of the random processes.

As an application, one can think of the $x[t]$ sequence as the received samples in a wireless communication system and the $z[t]$ sequence as a \emph{pilot sequence} (i.e., known to both the transmitter and receiver).
It is assumed the transmitted sequence has been convolved with an unknown LTI system.
This type of degradation is known as intersymbol interference (ISI) and the goal is to find a linear filter $h[0],h[1],\ldots,h[K-1]$ that removes as much ISI as possible.
A suitable cost function for this goal is
\[ J(\vecnot{h}) = \sum_{t=K}^N \lambda^{N-t} \left| z[t] - \sum_{k=0}^{K-1} h[k] x[t-k] \right|^2, \]
where the exponential weighting factor $\lambda$ emphasizes the most recently received symbols because, in reality, the channel conditions are changing with time.

Using the vector $\vecnot{z} = [ \begin{array}{cccc} z[K] & z[K+1] & \cdots & z[N] \end{array} ]$ and the matrix
\[ A = \left[ \begin{array}{cccc} x[K] & x[K-1] & \cdots & x[1] \\ x[K+1] & x[K] & \cdots & x[2] \\ \vdots & \vdots & \ddots & \vdots \\ x[N] & x[N-1] & \cdots & x[N-K+1] \end{array} \right], \]
we can rewrite this cost function as
\[ J(\vecnot{h}) = (A\vecnot{h}-\vecnot{z})^H \Lambda (A\vecnot{h}-\vecnot{z}), \]
where $\Lambda$ is a diagonal matrix whose diagonal contains $ [ \begin{array}{ccccc} \lambda^{N-K} & \lambda^{N-K+1} & \cdots & \lambda^1 & \lambda^0 \end{array} ]$.
Using the orthogonality principle, one finds that the optimal solution is given by the normal equation
\[ A^H \Lambda A \vecnot{h} = A^H \Lambda \vecnot{z}. \]

To see the connection with Wiener filtering, the key observation is that the matrix $A^H \Lambda A$ and the vector $ A^H \Lambda \vecnot{z}$ are sample-average estimates of the correlation matrix and cross-correlation vector.
This is because, for large $N$ and $\lambda$ close to 1, we have
\[ \left[ A^H \Lambda A \right]_{ij} = \sum_{t=K}^N \lambda^{N-t} x[t-j+1] \overline{x}[t-i+1] \approx \frac{R_{xx}(i-j)}{1-\lambda} \]
and
\[ \left[ A^H \Lambda \vecnot{z} \right]_i = \sum_{t=K}^N \lambda^{N-t} z[t] \overline{x}[t-i+1] \approx \frac{R_{zx}(i)}{1-\lambda}. \]

Another benefit of this approach is that, as each new sample arrives, the solution $\vecnot{h}$ can be updated with low complexity.
Consider the matrix $G_N = A^H \Lambda A$ and vector $\vecnot{b}_N = A^H \Lambda \vecnot{z}$ as a function of $N$.
Then, $G_{N+1} = \lambda G_N + \vecnot{u}^H \vecnot{u}$ and $\vecnot{t}_{N+1} = \lambda \vecnot{b}_N + z[N+1] \vecnot{u}^H$, where
\[ \vecnot{u} = \left[ \begin{array}{cccc} x[N+1] & x[N] & \cdots & x[N-K+2] \end{array} \right]. \]
The updated solution vector $\vecnot{h}_{N+1} = G_{N+1}^{-1} \vecnot{b}_{N+1}$ can be computed efficiently using the Sherman-Morrison matrix inversion formula.


\section{Dual Approximation}

\subsection{Minimum-Norm Solutions}

In many cases, one is interested in finding the minimum-norm vector that satisfies some feasibility constraints.
For example, an underdetermined system of linear equations has an infinite number of solutions.
But, in practice, it often makes sense to prefer the minimum-norm solution over other solutions.
Finding this solution is very similar to finding the best approximation.

Let $V$ be a Hilbert space and $\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n$ be a set of linearly independent vectors in $W$.
For any $\vecnot{v} \in V$, consider finding the scalars $s_1,s_2,\ldots,s_n$ that minimize
\[ \left\| \vecnot{v} - \sum_{i=1}^n s_i \vecnot{w}_i \right\|. \]
The answer is clearly given by the best approximation of $\vecnot{v}$ by vectors in the span of $\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n$.
The orthogonality principle tells us that $s_1,s_2,\ldots,s_n$ must satisfy
\begin{equation} \label{eqn:DualNormalEquations}
\left[ \begin{array}{cccc}
\left\langle \vecnot{w}_1 | \vecnot{w}_1 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_1 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{w}_1 | \vecnot{w}_2 \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_2 \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_2 \right\rangle \\
\vdots & \vdots & \ddots & \vdots \\
\left\langle \vecnot{w}_1 | \vecnot{w}_n \right\rangle
& \left\langle \vecnot{w}_2 | \vecnot{w}_n \right\rangle & \cdots
& \left\langle \vecnot{w}_n | \vecnot{w}_n \right\rangle
\end{array} \right]
\left[ \begin{array}{c}
s_1 \\ s_2 \\ \vdots \\ s_n \end{array} \right]
= \left[ \begin{array}{c}
\left\langle \vecnot{v} | \vecnot{w}_1 \right\rangle \\
\left\langle \vecnot{v} | \vecnot{w}_2 \right\rangle \\ \vdots \\
\left\langle \vecnot{v} | \vecnot{w}_n \right\rangle \end{array} \right] .
\end{equation}

The same problem can also be posed in a different manner.
\begin{theorem}
Let $V$ be a Hilbert space and $\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n$ be a set of linearly independent vectors in $V$.
The \defn{inner-product space}{dual approximation} problem is to find the vector $\vecnot{w}\in V$ of minimum-norm that satisfies $\left\langle \vecnot{w} | \vecnot{w}_i \right\rangle = c_i$ for $i=1,\ldots,n$.
This vector is given by
\[ \vecnot{w} = \sum_{i=1}^n s_i \vecnot{w}_i, \]
where the coefficients  $s_1,s_2,\ldots,s_n$ can be found by solving \eqref{eqn:DualNormalEquations} with $\left\langle \vecnot{v} | \vecnot{w}_i \right\rangle = c_i$.
\end{theorem}
\begin{proof}
Let $W = \Span(\vecnot{w}_1 ,\vecnot{w}_2 , \ldots, \vecnot{w}_n)$ and notice that the subset
\[ A = \{ \vecnot{u} \in V | \left\langle \vecnot{u} | \vecnot{w}_i \right\rangle = c_i\; \forall\; i=1,\ldots,n \} \]
is simply the orthogonal complement $W^{\bot}$ translated by any vector $\vecnot{v}\in A$.
Therefore, the vector achieving $\min_{\vecnot{u}\in A} \| \vecnot{u} \|$ is the error vector in the best approximation of some $\vecnot{v}\in A$ by vectors in $W^{\bot}$.
Using the unique decomposition $\vecnot{v} = \hat{\vecnot{v}} + \vecnot{w}$ implied by the orthogonal decomposition $V = W^{\bot} \oplus W$, one finds that the error vector $\vecnot{w}$ must lie in $W$.
Moreover, the normal equations, given by modifying \eqref{eqn:DualNormalEquations}, show that the error vector $\vecnot{w}$ is the unique vector in $W$ that satisfies $\left\langle \vecnot{w} | \vecnot{w}_i \right\rangle = c_i$ for $i=1,\ldots,n$.
\end{proof}

\subsection{Underdetermined Linear Systems}

Let $A \in \ComplexNumbers^{m\times n}$ with $m<n$ be the matrix representation of an underdetermined system of linear equations and $\vecnot{v} \in \ComplexNumbers^m$ be any column vector.
Then, the dual approximation theorem can be applied to solve the problem
\[ \min_{\vecnot{s} : A\vecnot{s} = \vecnot{v}} \| \vecnot{s} \| . \]
To see this as a dual approximation, we can rewrite the constraint as $(A^H)^H \vecnot{s} = \vecnot{v}$.
Then, the theorem concludes that the minimum norm solution lies in $\mathcal{R}(A^H)$ (i.e., the column space of $A^H$).
Using this, one can define $\hat{\vecnot{s}} = A^H \vecnot{t}$ and see that $A (A^H \vecnot{t}) = \vecnot{v}$.
If the rows of $A$ are linearly independent, then the columns of $A^H$ are linearly independent and $(A A^H)^{-1}$ exists.
In this case, the solution $\hat{\vecnot{s}}$ can be obtained in closed form and is given by
\[ \hat{\vecnot{s}} = A^H \left( A A^H \right)^{-1} \vecnot{v}. \]


\section{Projection onto Convex Sets}

So far, we have focused on the projection of vectors on to subspaces.
Convex sets are generalize the closure property of subspaces to subsets of points.

\begin{definition}
Let $V$ be a vector space over $\RealNumbers$.
The subset $A \subseteq V$ is called a \defn{vector space}{convex set} if, for all $\vecnot{a}_1,\vecnot{a}_2 \in A$ and $\lambda\in[0,1]$, we have $\lambda \vecnot{a}_1 + (1-\lambda) \vecnot{a}_2 \in A$.
The set is \textbf{strictly convex} if, for all $\vecnot{a}_1,\vecnot{a}_2 \in \Closure{A}$ and $\lambda\in(0,1)$, we have $\lambda \vecnot{a}_1 + (1-\lambda) \vecnot{a}_2 \in \Interior{A}$.
\end{definition}

Let $V$ be a Hilbert space and $A\subseteq V$ be a closed convex set. For any $\vecnot v$, let
\[
f\left(\vecnot v\right)=\inf_{\vecnot u\in A}\left\Vert \vecnot u-\vecnot v\right\Vert 
\]
be the minimum distance between $\vecnot v$ and the set $A$. In this section, we will show that this infimum is achieved as a minimum by a unique $\vecnot u^{*}\in A$. Moreover, the mapping $P_{A}:V\rightarrow A$ is called the orthogonal projection of $\vecnot v$ onto $A$ and is defined by $\vecnot v\mapsto\vecnot u^{*}$.
\begin{theorem}
\label{thm:HilbertProjectionTheorem} The orthogonal projection of $\vecnot v\in V$ onto a closed convex set $A\subseteq V$ exists and is unique.
\end{theorem}
\begin{proof}
Let $d=f\left(\vecnot v\right)$ be the infimal distance and consider any sequence $\vecnot u_{1},\vecnot u_{2},\ldots\in A$ that achieves the infimum so that
\[
\lim_{n\rightarrow\infty}\left\Vert \vecnot u_{n}-\vecnot v\right\Vert =d.
\]
Since $A$ is complete, the next step is showing that this sequence is Cauchy. The paralleogram law states that $\left\Vert x-y\right\Vert ^{2}=2\left\Vert x\right\Vert ^{2}+2\left\Vert y\right\Vert ^{2}-\left\Vert x+y\right\Vert ^{2}$ and applying this to $\vecnot u_{n}+\vecnot u_{m}$ gives
\begin{align*}
\left\Vert \vecnot u_{m}-\vecnot u_{n}\right\Vert ^{2} & =\left\Vert \left(\vecnot v-\vecnot u_{n}\right)-\left(\vecnot v-\vecnot u_{m}\right)\right\Vert ^{2}\\
 & =2\left\Vert \vecnot v-\vecnot u_{n}\right\Vert ^{2}+2\left\Vert \vecnot v-\vecnot u_{m}\right\Vert ^{2}-\left\Vert \left(\vecnot v-\vecnot u_{n}\right)+\left(\vecnot v-\vecnot u_{m}\right)\right\Vert ^{2}\\
 & =2\left\Vert \vecnot v-\vecnot u_{n}\right\Vert ^{2}+2\left\Vert \vecnot v-\vecnot u_{m}\right\Vert ^{2}-4\left\Vert \vecnot v-\frac{\vecnot u_{n}+\vecnot u_{m}}{2}\right\Vert ^{2}\\
 & \leq2\left\Vert \vecnot v-\vecnot u_{n}\right\Vert ^{2}+2\left\Vert \vecnot v-\vecnot u_{m}\right\Vert ^{2}-4d^{2}
\end{align*}
because the convexity of $A$ implies $\frac{\vecnot u_{n}+\vecnot u_{m}}{2}\in A$ and therefore $\left\Vert \vecnot v-\frac{\vecnot u_{n}+\vecnot u_{m}}{2}\right\Vert ^{2}\geq d^{2}$. Since the limit of the RHS (as $m,n\rightarrow\infty$) equals $0$, we find that the sequence $\vecnot u_{n}$ is Cauchy and therefore the limit $\vecnot u^{*}$ must exist. Since $\vecnot u_{n}\in A$ and $A$ is closed, we also see that $\vecnot u^{*}\in A$. Therefore, the infimum is achieved as a minimum.

Uniqueness can be seen by assuming instead that $\vecnot u_{m},\vecnot u_{n}$ are two elements in $A$ which are both at a distance $d$ from $\vecnot v$. Then, the above derivation shows that $\left\Vert \vecnot u_{m}-\vecnot u_{n}\right\Vert ^{2}\leq0$. Therefore, they are the same point.
\end{proof}
\begin{remark}
The same result holds for norm projections in many other Banach spaces including $L^{p}$ and $\ell^{p}$ for $1<p<\infty$. In general, it is required that the Banach space be strictly convex (for uniqueness) and reflexive (for existence).
\end{remark}

Earlier in this chapter, we studied the equivalence between the orthogonality and Hilbert-space projections onto subspaces.
The following result can be seen as a generalization of that result to Hilbert-space projections onto convex sets.

\begin{lemma}
\label{lemma:convex_proj_lt0}
For any $\vecnot{v} \notin A$, a necessary and sufficient condition for $\vecnot{u}^* = P_A (\vecnot{v})$ is that $\langle \vecnot{v}-\vecnot{u}^*,\vecnot{u}-\vecnot{u}^* \rangle \leq 0$ for all $\vecnot{u}\in A$.
\end{lemma}
\begin{proof}
Let $\vecnot{u}^* = P_A (\vecnot{v})$ be the unique projection of $\vecnot{v}$ onto $A$.
For all $\vecnot{u} \in A$ %satisfying $\vecnot{u} \neq \vecnot{u}^*$
and any $\alpha\in (0,1)$, observe that $\vecnot{u}'=(1-\alpha) \vecnot{u}^* + \alpha \vecnot{u} = \vecnot{u}^* + \alpha (\vecnot{u}-\vecnot{u}^*)\in A$ due to convexity.
The optimality of $\vecnot{u}^*$ implies that
\begin{align*}
\| \vecnot{v}-\vecnot{u}^* \|^2
&\leq \| \vecnot{v}- \vecnot{u}' \| \\
&\leq \| \vecnot{v}- \vecnot{u}^* - \alpha (\vecnot{u}-\vecnot{u}^*) \|^2 \\
&= \| \vecnot{v} - \vecnot{u}^* \|^2 + \alpha^2 \| \vecnot{u} - \vecnot{u}^* \|^2 - 2 \alpha \langle v-\vecnot{u}^*,\vecnot{u}-\vecnot{u}^* \rangle. 
\end{align*}
Thus, $\langle v-\vecnot{u}^*,\vecnot{u}-\vecnot{u}^* \rangle \leq \frac{\alpha}{2} \| \vecnot{u} - \vecnot{u}^* \|^2$.
One can establish necessity by taking the limit as $\alpha \to 0$.
For sufficiency, we assume $\langle \vecnot{v}-\vecnot{u}^*,\vecnot{u}-\vecnot{u}^* \rangle \leq 0$ and we write
\begin{align*}
\| \vecnot{v}-\vecnot{u} \|^2 - \| \vecnot{v} - \vecnot{u}^* \|^2
&= \| (\vecnot{v} - \vecnot{u}^*) - (\vecnot{u} - \vecnot{u}^*) \|^2 - \|\vecnot{v} - \vecnot{u}^* \|^2 \\
&= \| \vecnot{v}-\vecnot{u}^* \|^2 + \| \vecnot{u} -\vecnot{u}^* \| ^2 - 2 \langle v-\vecnot{u}^*,\vecnot{u}-\vecnot{u}^* \rangle - \|\vecnot{v}-\vecnot{u}^* \|^2 \\
&\geq 0.
\end{align*}
Thus, $\| \vecnot{v}-\vecnot{u} \|^2 \geq \| \vecnot{v} - \vecnot{u}^* \|^2$ for all $\vecnot{u} \in A$ and $\vecnot{u}^* = P_A (\vecnot{v})$.
\end{proof}

\subsection{Minimum Distance Between Two Convex Sets}

Now, consider the smallest distance between two disjoint closed convex sets $A,B\subseteq V$. In this case, a unique solution may exist but a number of things can go wrong. If the two sets are not strictly convex (e.g., consider two squares), then it is clearly possible for their to multiple pairs of points that achieve the minimum distance. Even if the two sets are strictly convex, one may find that the infimum is achieved as the points wander off to infinity. For example, consider the strictly convex hyperbolic sets $A=\left\{ (x,y)|x^{2}-y^{2}\geq1,x>0\right\} $ and $B=\left\{ (x,y)|y^{2}-x^{2}\geq1,y\geq0\right\} $. These two sets share the line $x=y>0$ as an asymptote, so their infimal distance is 0.

To understand this behavior, we first note that the distance $f\left(\vecnot u,\vecnot v\right)=\left\Vert \vecnot u-\vecnot v\right\Vert $ is a convex function on the convex product set $A\times B$, one finds that any local minimum distance is a global minimum distance.we have the following results.
\begin{theorem}
\label{thm:MinDistTwoConvexSet} Let $V$ be a Hilbert space and consider the infimal distance
\[
d=\inf_{\vecnot u\in A,\vecnot v\in B}\left\Vert \vecnot u-\vecnot v\right\Vert 
\]
between two disjoint closed convex sets $A,B\subseteq V$.
If the set $B$ is compact, then the infimum is achieved.
If the set $A$ is strictly convex and the infimum is achieved, then the minimizing points $\vecnot u^{*},\vecnot v^{*}$ are unique.
\end{theorem}

\begin{proof}
Consider any sequence $(\vecnot u_{1},\vecnot v_{1}),(\vecnot u_{2},\vecnot v_{2}),\ldots\in A\times B$ which satisfies
\[
\lim_{n\rightarrow\infty}\left\Vert \vecnot u_{n}-\vecnot v_{n}\right\Vert =d.
\]
If $B$ is compact, then there is a subsequence $\vecnot v_{n_{j}}$ that converges to some $\vecnot v^{*}\in B$. Therefore, we can set $B=\left\{ v^{*}\right\} $ and use Theorem \ref{thm:HilbertProjectionTheorem} to see that there exists a unique $\vecnot u^{*}\in A$ which achieves the smallest distance. Notice that the $\vecnot v^{*}$ is not unique in general and that $\vecnot u^{*}$ is only unique for a given $\vecnot v^{*}$.

Since $\left\Vert \vecnot u-\vecnot v\right\Vert $ is a convex function on the convex product set $A\times B$, there is a (possibly empty) convex set of minimizers

\[
M=\left\{ (\vecnot u,\vecnot v)\in A\times B\;\big|\;\left\Vert \vecnot u-\vecnot v\right\Vert =d\right\} .
\]
Also, each component of the $(\vecnot{u},\vecnot{v})$ points in $M$ must lie on the boundary of its set because otherwise one could reduce the smallest distance by moving one point along the minimum distance line towards the boundary. Now, suppose that (i) $A$ is strictly convex and (ii) $M$ contains more than one pair of minimizers. Then, condition (ii) implies that there must be two boundary points $\vecnot u_{1},\vecnot u_{2}\in\partial A$ such that $\alpha\vecnot u_{1}+(1-\alpha)\vecnot u_{2}\in\partial A$ for $\alpha\in[0,1]$. But this contradicts condition (i) and shows that, if $A$ is strictly convex, then there is at most one pair $(\vecnot u^{*},\vecnot v^{*})\in M$ of minimizing points.
\end{proof}
\begin{remark}
Finding the minimum distance between two disjoint closed convex sets $A,B\subseteq V$ is a classic problem that is solved nicely by the idea of alternating minimization. Let $\vecnot v_{0}\in B$ be an arbitrary initial point and define

\begin{align*}
\vecnot u_{n+1} & =\arg\min_{\vecnot u\in A}\left\Vert \vecnot u-\vecnot v_{n}\right\Vert \\
\vecnot v_{n+1} & =\arg\min_{\vecnot v\in B}\left\Vert \vecnot u_{n+1}-\vecnot v\right\Vert .
\end{align*}
Notice that the sequence $d_{n}=\left\Vert \vecnot u_{n}-\vecnot v_{n}\right\Vert $ is non-increasing and must therefore have a limit.
One can show that, if either set is compact (resp. bounded), then the sequence $(\vecnot u_{n},\vecnot v_{n})$ converges strongly (resp. weakly) to vectors that minimize the distance. % cite []Bauschke, Finding Best Approx. Pairs]
Variations of this approach can even be useful when $A\cap B\neq\emptyset$.
In this case, sequence $\vecnot v_{n}$ converges weakly to some point $\vecnot v\in A\cap B$.
\end{remark}

%\begin{theorem}[Hyperplane Separation]
%Assume unique points minimize.
%Then projection followed by inner_prod_lt0 gives separataion?
%\end{theorem}

\iffalse
\subsection{Alternating Projections}

Another classic result for projections in a Hilbert space is that alternating between projections onto multiple subspaces leads to a projection onto the intersection of all the subspaces.
\begin{lemma}
Let $V$ be a separable Hilbert space and $P_{W}$ be a projection onto the closed subspace $W\subset V$. Then, $\left\Vert P_{W}\vecnot v_{n}\right\Vert \rightarrow\left\Vert \vecnot v_{n}\right\Vert \rightarrow d$ implies that $\vecnot v_{n}\rightarrow\vecnot v\in W$. (not correct) Need to show vector is decreasing in some sense!!! For any projection-like $T$, $\vecnot v_{n}=T^{n}\vecnot v_{0}$. non-expansive. If not fixed point, then coefficients reduced in some basis (i.e., cannot wander off to new basis elements)
\end{lemma}
\begin{proof}
Let $\vecnot w_{1},\vecnot w_{2},\ldots$ be a orthonormal basis for $W$ Orthogonal decomposition of projection shows that
\end{proof}
\fi
