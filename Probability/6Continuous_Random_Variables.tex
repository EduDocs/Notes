\chapter{Continuous Random Variables}

In the previous chapter, we introduced discrete random variables and discussed their properties.
Discrete random variables are very useful in many contexts, yet they form only a small subset of the collection of random variables pertinent to probability and engineering.
In this chapter, we consider random variables with a continuous range of possible values; that is, random variables that can take an uncountable number of values.

Continuous random variables are powerful mathematical tools that allow us to pose and solve important engineering problems, which cannot be addressed using discrete models.
While this extra flexibility is useful and desirable, it comes at a certain cost.
A continuous random variable cannot be characterized by a probability mass function.
This difficulty emerges from the limitations of the third axiom of probability laws, which only applies to countable collections of disjoint events.

Below, we provide a definition for continuous random variables.
Furthermore, we extend and apply the concepts and methods introduced in Chapter~\ref{chapter:DiscreteRandomVariables} to the class of continuous random variables.
In particular, we develop a continuous counterpart to the probability mass function.


\section{Cumulative Distribution Functions}

We begin this chapter by introducing a general concept which can be used to bridge our understanding of discrete and continuous random variables.
The \emph{cumulative distribution function} (CDF) of a random variable $X$ is defined as the probability of the event $\{X \leq x \}$,
\begin{equation*}
F_X (x) = \Pr ( \{ X \leq x \} ) = \Pr (X \leq x).
\end{equation*}
In terms of the underlying sample space, the function $F_X (x)$ represents the probability of the set of all outcomes in $\Omega$ for which $X$ is less than or equal to $x$,
\begin{equation*}
F_X (x) = \Pr \left( X^{-1} ( (- \infty, x]) \right)
= \Pr (\{ \omega \in \Omega | X(\omega) \leq x \}).
\end{equation*}
In essence, the CDF is a convenient way to specify the probability of all events of the form $X \in (-\infty, x]$.

Recall that a random variable is a real-valued function.
The CDF of random variable $X$ therefore exists and is well-defined for any $X$.
Furthermore, since the realization of $X$ is a real number, we have
\begin{gather*}
\lim_{x \rightarrow - \infty} F_X (x) = 0 \\
\lim_{x \rightarrow \infty} F_X (x) = 1.
\end{gather*}
Suppose $x_1 < x_2$, then we can write $\{ X \leq x_2 \}$ as the union of the two disjoint sets $\{ X \leq x_1 \}$ and $\{ x_1 < X \leq x_2 \}$.
It follows that
\begin{equation} \label{equation:MonotoneIncreasingCDF}
\begin{split}
F_X (x_2) &= \Pr (X \leq x_2) \\
&= \Pr (X \leq x_1) + \Pr (x_1 < X \leq x_2) \\
&\geq \Pr (X \leq x_1) = F_X (x_1).
\end{split}
\end{equation}
The CDF is a monotone non-decreasing function.
We note from \eqref{equation:MonotoneIncreasingCDF} that the probability of $X$ falling in the interval $(x_1, x_2]$ is
\begin{equation} \label{equation:IntervalCDF}
\Pr (x_1 < X \leq x_2) = F_X (x_2) - F_X (x_1).
\end{equation}

\subsection{Discrete Random Variables}

Suppose that $X$ is a discrete random variable that takes only integer values.
Then the CDF of $X$ is given by
\begin{equation*}
F_X (x) = \sum_{k \leq x} p_X (k),
\end{equation*}
and its PMF can be computed using the formula
\begin{equation*}
p_X (k) = \Pr ( X \leq k) - \Pr (X \leq k -1 ) = F_X (k) - F_X (k-1).
\end{equation*}

\begin{example}
For instance, let $X$ be a geometric random variable with parameter $p$ and PMF
\begin{equation*}
p_X (k) = (1 - p)^{k-1} p \quad k = 1, 2, \ldots 
\end{equation*}
For $x > 0$, its CDF is then given by
\begin{equation*}
F_X (x) = \sum_{k = 1}^{\lfloor x \rfloor} (1 - p)^{k-1} p
= 1 - (1 - p)^{\lfloor x \rfloor} .
\end{equation*}
For integer $k \geq 1$, the PMF of a geometric random variable $X$ is recovered by calculating
\begin{equation*}
\begin{split}
p_X (k) &= F_X (k) - F_X (k-1) \\
&= \left( 1 - (1-p)^k \right) - \left( 1 - (1-p)^{k-1} \right) \\
&= (1 - p)^{k-1} - (1-p) (1-p)^{k-1} \\
&= p (1 - p)^{k-1}.
\end{split}
\end{equation*}
\end{example}

\subsection{Continuous Random Variables}

With the concept of a CDF clearly defined, we can safely provide a more precise definition for continuous random variables.
Let $X$ be a random variable with CDF $F_X (x)$, then $X$ is said to be a \emph{continuous random variable} if $F_X (x)$ is differentiable with respect to $x$.

\subsection{Mixed Random Variables*}

It should be clear by now that the CDF of a discrete random variable is a discontinuous staircase-like function.
The CDF of a continuous random variable, on the other hand, is continuous and differentiable almost everywhere.
There exist random variables for which neither of these two situations apply.
Such random variables are sometimes called \emph{mixed random variables}.
Our exposition of mixed random variables in this book is limited.
However, it is beneficial to point out that a good understanding of discrete and continuous random variables is sufficient to understand and address most problems with mixed random variables.


\section{Probability Density Functions}

As mentioned above, the CDF of a continuous random variable $X$ is a differentiable function.
We denote this latter function by $f_X (x)$ and call it the \emph{probability density function} (PDF) of $X$.
If $X$ is a random variable with PDF $f_X (x)$ then
\begin{equation*}
F_X (x) = \int_{- \infty}^x f_X (\xi) d\xi ,
\end{equation*}
and, equivalently, we can write
\begin{equation*}
f_X (x) = \frac{d F_X}{dx} (x) .
\end{equation*}
Note that PDFs are only defined for continuous random variables.
This is somewhat restrictive, however the PDF is very convenient as it can be employed to derive properties for continuous random variables which would be difficult to compute otherwise.

For $x_1 < x_2$, we can combine the definition of $f_X(x)$ and \eqref{equation:IntervalCDF} to get
\begin{equation*}
\Pr (x_1 < X \leq x_2) = \int_{x_1}^{x_2} f_X (\xi) d\xi .
\end{equation*}
Furthermore, it is easily seen that for any continuous random variable
\begin{equation*}
\begin{split}
\Pr (X = x_2) &= \lim_{x_1 \uparrow x_2} \Pr (x_1 < X \leq x_2) \\
&= \lim_{x_1 \uparrow x_2} \int_{x_1}^{x_2} f_X (\xi) d\xi \\
&= \int_{x_2}^{x_2} f_X (\xi) d\xi = 0.
\end{split}
\end{equation*}
If $X$ is a continuous random variable, then the probability that $X = x$ for any single real number $x$ is zero.
An immediate corollary of this observation is that
\begin{equation*}
\Pr (x_1 < X < x_2)
= \Pr (x_1 \leq X < x_2)
= \Pr (x_1 < X \leq x_2)
= \Pr (x_1 \leq X \leq x_2) .
\end{equation*}
In other words, the inclusion or exclusion of the endpoints of an interval does not affect the probability of the corresponding interval when $X$ is a continuous random variable.

We can derive properties for the PDF of continuous random variable $X$ based on the axioms of probability laws.
First, the probability that $X$ is a real number is given by
\begin{equation*}
\Pr (-\infty < X < \infty) = \int_{-\infty}^{\infty} f_X (\xi) d\xi = 1.
\end{equation*}
Thus, $f_X (x)$ must integrate to one.
Also, since the probabilities of events are nonnegative, we must have $f_X (x) \geq 0$ (almost) everywhere.
Finally, given a measurable set $S$, the probability that $X \in S$ equals
\begin{equation*}
\Pr (X \in S) = \int_S f_X (\xi) d\xi .
\end{equation*}
This formula can be used to compute the probability of any admissible set $S$.


\section{The Gaussian (Normal) Random Variable}

The \emph{Gaussian random variable} is of fundamental importance in probability and statistics.
As we will see later, it is often used to model the distribution of a sum of random components.
The PDF for a Gaussian random variable $X$ is given by
\begin{equation*}
f_X (x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{- \frac{(x - m)^2}{2 \sigma^2}}
\quad - \infty < x < \infty,
\end{equation*}
where $m$ and $\sigma > 0$ are constants.
The CDF of a Gaussian random variable does not admit a closed-form expression, but it can be expressed as
\begin{equation*}
\begin{split}
F_X (x) &= \Pr (X \leq x) \\
&= \frac{1}{\sqrt{2 \pi} \sigma}
\int_{- \infty}^{x} e^{- \frac{(\xi - m)^2}{2 \sigma^2}} d\xi \\
&= \frac{1}{\sqrt{2 \pi}}
\int_{- \infty}^{(x - m)/\sigma} e^{- \frac{\zeta^2}{2}} d\zeta \\
&= \Phi \left( \frac{x - m}{\sigma} \right),
\end{split}
\end{equation*}
where $\Phi$ is the \emph{standard normal cumulative distribution function} defined by
\begin{equation*}
\Phi (x) = 
\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-\frac{\zeta^2}{2}} d\zeta .
\end{equation*}
We note that the function $\Phi$ is no more than a convenient notation.

\begin{example}
A binary signal is transmitted through a communication channel.
The sent signal takes on either a value of $1$ or $-1$.
The message received at the output of the communication channel is corrupted by additive thermal noise.
This noise can be accurately modeled as a Gaussian random variable.
The receiver declares that a $1$ ($-1$) was transmitted if the sent signal is positive (negative).
What is the probability of making an erroneous decision.

Let $s \in \{ -1, 1 \}$ be the transmitted signal, $N$ be the value of the thermal noise, and $Z$ represent the value of the received signal.
An error can occur in one of two possible ways: $s = 1$ was transmitted and $Z$ is less than zero, or $s = -1$ was transmitted and $Z$ is greater than zero.
Using the total probability theorem, we get
\begin{equation*}
\Pr (\text{error}) = \Pr (Z \geq 0 | s = -1) \Pr (s = -1)
+ \Pr (Z \leq 0 | s = 1) \Pr (s = 1).
\end{equation*}
By the symmetry of the problem, it is easily argued that
\begin{equation*}
\begin{split}
\Pr (\text{error}) &= \Pr (Z \geq 0 | s = -1) = \Pr (N > 1) \\
&= \int_{1}^{\infty} \frac{1}{\sqrt{2 \pi} \sigma}
e^{- \frac{\xi^2}{2 \sigma^2}} d\xi \\
&= 1 - \Phi \left( \frac{1}{\sigma} \right) .
\end{split}
\end{equation*}
The reliability of this transmission scheme depends on the amount of noise present at the receiver.
\end{example}

Next, we show that the standard normal PDF integrates to one.
The solution is easy to follow, but hard to discover.
It is therefore useful to include it here.
Consider a standard normal PDF,
\begin{equation*}
f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}} .
\end{equation*}
We can show that $f_X (x)$ integrates to one using a subtle argument and a straightforward change of variables.
Consider the square of the integrated PDF,
\begin{equation*}
\begin{split}
\left(\int_{- \infty}^{\infty} f_X (\xi) d\xi \right)^2
&= \int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2}{2}} d\xi
\int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{- \frac{\zeta^2}{2}} d\zeta \\
&= \int_{- \infty}^{\infty} \int_{- \infty}^{\infty}
\frac{1}{2 \pi} e^{- \frac{\xi^2 + \zeta^2}{2}} d\xi d\zeta \\
&= \int_{0}^{2 \pi} \frac{1}{2 \pi} d\theta
\int_{0}^{\infty} e^{- \frac{r^2}{2}} r dr \\
&= \left. \left( - e^{- \frac{r^2}{2}} \right) \right|_0^{\infty} = 1.
\end{split}
\end{equation*}
Since the square is equal to one and the integral is nonnegative, we conclude that the normal PDF integrates to one.


\subsection{The Error Function and the $Q$-function}

The normal random variable is so frequent in mathematics and engineering that variations of its CDF also possess their own names.
The \emph{error function} is a function which is primarily encountered in the fields of statistics and partial differential equations.
It is defined as
\begin{equation*}
\mathrm{erf} (x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-\xi^2} d\xi .
\end{equation*}
The error function is related to the standard normal cumulative distribution function by scaling and translation,
\begin{equation*}
\Phi (x) = \frac{1 + \mathrm{erf} \left( {x}/{\sqrt{2}} \right)}{2}.
\end{equation*}
If $X$ is a standard normal random variable, then $\mathrm{erf} \left( x/{\sqrt{2}} \right)$ denotes the probability that $X$ lies between $-x$ and $x$.

In engineering, it is customary to use the \emph{$Q$-function}, which is given by
\begin{equation*}
\begin{split}
Q (x) &= \frac{1}{\sqrt{2 \pi}} \int_x^{\infty} e^{-\frac{\xi^2}{2}} d\xi \\
&= 1 - \Phi (x) \\
&= \frac{ 1 - \mathrm{erf} \left( {x}/{\sqrt{2}} \right) }{2} .
\end{split}
\end{equation*}
The last equation may prove useful while using various software packages that provide a built-in implementation for $\mathrm{erf}(x)$, but not for the $Q$-function.


\section{Important Distributions}

Our intuition about continuous random variables can be further developed by looking at additional examples.
In this section, we introduce important random variables and their distributions.
These random variables find widespread application in various fields of engineering.


\subsection{The Exponential Distribution}

The \emph{exponential random variable} is also a very important continuous random variable.
It is used in engineering to model the lifetime of devices and systems, and the time between occurrences of events.
An exponential random variable $X$ with parameter $\lambda$ has PDF
\begin{equation*}
f_X (x) = \lambda e^{- \lambda x} \quad x \geq 0.
\end{equation*}
For $x \geq 0$, its CDF is equal to
\begin{equation*}
F_X (x) = 1 - e^{- \lambda x} .
\end{equation*}
The parameter $\lambda$ characterizes the rate at  which events occur.

\begin{example}
\end{example}

The exponential random variable can be obtained as the limit of a sequence of geometric random variables.
Let $\lambda$ be fixed and defined $p_n = \lambda/n$.
We define the PMF of the random variable $X_n$ as
\begin{equation*}
p_{X_n} (k) = (1 - p_n)^{k-1} p_n
= \left( 1 - \frac{\lambda}{n} \right)^{k-1} \frac{\lambda}{n}
\quad k = 1, 2, \ldots
\end{equation*}
The random variable $X_n$ is a standard geometric random variable with parameter $p_n = \lambda/n$.
For every $n$, we define a new variable $Y_n$ by
\begin{equation*}
Y_n = \frac{X_n}{n}.
\end{equation*}
The random variable $Y_n$ has PMF
\begin{equation*}
p_{Y_n} (y) = \left\{ \begin{array}{ll}
(1 - p_n)^{k-1} p_n, & \text{if }y = k/n \\
0, & \text{otherwise} .
\end{array} \right.
\end{equation*}
For any $x \geq 0$, the CDF of the random variable $Y_n$ can be computed as
\begin{equation*}
\begin{split}
\Pr (Y_n \leq x)
= \Pr (X_n \leq n x)
&= \sum_{k = 1}^{\lfloor n x \rfloor} p_{X_n} (k) \\
&= \sum_{k = 1}^{\lfloor n x \rfloor} (1 - p_n)^{k-1} p_n \\
% &= p_n \frac{1 - (1 - p_n)^{\lfloor n x \rfloor}}{1 - (1 - p_n)} \\
&= 1 - (1 - p_n)^{\lfloor n x \rfloor} .
\end{split}
\end{equation*}
In the limit, as $n$ grows unbounded, we get
\begin{equation*}
\begin{split}
\lim_{n \rightarrow \infty} \Pr (Y_n \leq x)
&= \lim_{n \rightarrow \infty} \left[ 1 - (1 - p_n)^{\lfloor n x \rfloor} \right] \\
&= 1 - \lim_{n \rightarrow \infty}
\left( 1 - \frac{\lambda}{n} \right)^{\lfloor n x \rfloor} \\
&= 1 - e^{- \lambda x} .
\end{split}
\end{equation*}
Thus, the sequence of scaled geometric random variable $\{ Y_n \}$ converges in distribution to the exponential random variable $X$.

\paragraph{Memoryless Property:}
An interesting feature of the exponential random variable is that it satisfies the \emph{memoryless property},
\begin{equation*}
\Pr (X > t + u | X > t) = \Pr (X > u).
\end{equation*}
The proof of this fact is a straightforward application of conditional probability.
Suppose that $X$ is an exponential random variable with parameter $\lambda$.
Also, let $t$ and $u$ be two positive numbers.
The memoryless property is obtained by expanding the conditional probability using definition \eqref{equation:ConditionalProbability},
\begin{equation*}
\begin{split}
\Pr (X > t + u | X > t)
&= \frac{\Pr( \{ X > t + u \} \cap \{ X > t \} ) }{ \Pr ( X > t ) } \\
&= \frac{\Pr( X > t + u ) }{ \Pr ( X > t ) } \\
&= \frac{e^{- \lambda (t + u)} }{ e^{- \lambda t } } \\
&= e^{- \lambda u } = \Pr (X > u).
\end{split}
\end{equation*}
In reality, the exponential random variable is the only continuous random variable that satisfies the memoryless property.

\begin{example}
\end{example}


\subsection{The Gamma Distribution}

The gamma distribution defines a versatile collection of random variables.
The PDF of a \emph{gamma random variable} is given by
\begin{equation*}
f_X (x) = \frac{\lambda (\lambda x)^{\alpha - 1} e^{-\lambda x}}{\Gamma (\alpha)} \quad  x > 0,
\end{equation*}
where $\Gamma(z)$ is the gamma function defined by
\begin{equation*}
\Gamma (z) = \int_0^{\infty} \xi^{z-1} e^{-\xi} d\xi \quad x > 0 .
\end{equation*}
The two parameters $\alpha > 0$ and $\lambda > 0$ affect the shape of this distribution significantly.
By varying these two parameters, it is possible for the gamma PDF to accurately model a wide array of empirical data.

The gamma function can be evaluated recursively using integration by parts; this yields the relation
\begin{equation*}
\Gamma (z+1) = z \Gamma (z) \quad z > 0.
\end{equation*}
For nonnegative integers, it can easily be shown that
\begin{equation*}
\Gamma (k + 1) = k! .
\end{equation*}
Perhaps, the most well-known value for the gamma function at a non-integer argument is
\begin{equation*}
\Gamma \left( \frac{1}{2} \right)
= \sqrt{\pi} .
\end{equation*}
Interestingly, this specific value for the gamma function can be evaluated by a method similar to the one we used to integrate the Gaussian distribution,
\begin{equation*}
\begin{split}
\left( \Gamma \left( \frac{1}{2} \right) \right)^2
&= \int_0^{\infty} \xi^{-\frac{1}{2}} e^{-\xi} d\xi
\int_0^{\infty} \zeta^{-\frac{1}{2}} e^{-\zeta} d\zeta \\
&= \int_0^{\infty} \int_0^{\infty}
\xi^{-\frac{1}{2}} \zeta^{-\frac{1}{2}} e^{-(\xi + \zeta)}
d\xi d\zeta \\
&= \int_0^{\pi / 2} \int_0^{\infty}
\frac{1}{r^2 \sin \theta \cos \theta} e^{-r^2}
4 r^3 \sin \theta \cos \theta dr d\theta \\
&= \int_0^{\pi / 2} \int_0^{\infty}
 e^{-r^2} 4 r dr d\theta \\
&= \pi .
\end{split}
\end{equation*}

Many common distributions are special cases of the gamma distribution.
The gamma distribution becomes an exponential distribution whenever $\alpha = 1$.

\paragraph{The Chi-Square Distribution:}
When $\lambda = 1/2$ and $\alpha = k/2$ for some positive integer $k$, the gamma distribution becomes a \emph{chi-square distribution},
\begin{equation*}
f_X (x) = \frac{x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma (k/2)} \quad  x > 0.
\end{equation*}
The chi-square distribution is one of the probability distributions most widely used in statistical inference problems.


\paragraph{The $m$-Erlang Distribution:}
When $\alpha = k$, a positive integer, the gamma distribution is called an \emph{$m$-Erlang distribution}.
This distribution finds application in queuing theory.
Its PDF is given by
\begin{equation*}
f_X (x) = \frac{\lambda (\lambda x)^{k - 1} e^{-\lambda x}}{(k-1)!} \quad  x > 0.
\end{equation*}

An $m$-Erlang can be obtained as the sum of $m$ exponential random variables.
Let $X_1, X_2, \ldots, X_m$ be $m$ independent exponential random variables.
Consider the random variable $S_m$ given by
\begin{equation*}
S_m = \sum_{k=1}^m X_m.
\end{equation*}
Then, $S_m$ is an $m$-Erlang random variable.

\begin{example}
Suppose that service request arrivals at an Internet server are independent and memoryless.
Let $S_m$ be a random variable that denotes the time of the $m$th arrival, then $S_m$ is an $m$-Erlang  random variable.
\end{example}

\subsection{Additional Distributions}

Probability distributions arise in many different contexts and assume various forms.
We conclude this section on important probability distribution by mentioning a few additional distributions that are commonly used in engineering.
It is interesting to note the interconnection between various random variables and their corresponding probability distributions.

\paragraph{The Laplace Distribution:}
The \emph{Laplace distribution} is sometimes called a double exponential distribution because it can be thought of as an exponential function and its reflection spliced together.
The PDF of a Laplacian random variable is
\begin{equation*}
f_X (x) = \frac{1}{2b} e^{- \frac{|x|}{b}} \quad - \infty < x < \infty,
\end{equation*}
where $b$ is a positive constant.
The difference between two independent and identically distributed exponential random variables is governed by a Laplace distribution.

\paragraph{The Cauchy Distribution:}
The \emph{Cauchy distribution} is also considered a heavy-tail distribution.
The PDF of a Cauchy random variable is given by
\begin{equation*}
f_X (x) = \frac{ \gamma^2 }{\pi \left( \gamma^2 + x^2 \right)} \quad - \infty < x < \infty.
\end{equation*}
This distribution has no mean, although its median is zero.
Cauchy random variables are sometimes used in detection theory to model communication channels with extremely impulsive noise.
It also finds applications in the fields of physics.
Physicists sometimes refer to this distribution as the \emph{Lorentz distribution}.


\section{Functions of Random Variables}

Much like in the discrete case, it is possible to create a new random variable $Y$ by applying a real-valued function $g$ to an existing random variable $X$.
Specifically, if $g$ is a continuous function and $X$ is a continuous random variable, then so is $Y = g(X)$.
Given $X$ and $g$, the probability that $Y$ falls in a specific set $S$ depends on both the function $g$ and the CDF of $X$,
\begin{equation*}
\Pr (Y \in S) = \Pr (g(X) \in S) 
= \Pr (X \in g^{-1}(S)).
\end{equation*}

To gain insight into this problem, it is best to first consider the situation where $g$ is a differentiable and strictly increasing function.
In this case, we can write the CDF of $Y$ as
\begin{equation*}
F_Y(y) = \Pr (Y \leq y) = \Pr (g(X) \leq y)
= \Pr \left( X \leq g^{-1}(y) \right)
= F_X \left( g^{-1} (y) \right) .
\end{equation*}
Differentiating this equation with respect to $y$, we obtain the PDF of $Y$
\begin{equation*}
\begin{split}
f_Y (y) &= \frac{d}{dy} F_Y(y)
= \frac{d}{dy} F_X \left( g^{-1} (y) \right) \\
&= f_X \left( g^{-1} (y) \right) \frac{d \left( g^{-1} \right)}{dy} \\
&= f_X \left( g^{-1} (y) \right) \frac{dx}{dy} .
\end{split}
\end{equation*}
With the simple substitution $x = g^{-1} (y)$, we get
\begin{equation*}
f_Y (y) = f_X (x) \left. \frac{dx}{dy} \right|_{x = g^{-1}(y)}
= \frac{f_X (x)}{{dy}/{dx}}
= \frac{f_X (x)}{g'(x)} .
\end{equation*}

On the other hand, suppose that $g$ is a differentiable and decreasing function.
In this case, The CDF of the random variable $Y = g(X)$ becomes
\begin{equation*}
F_Y(y) = \Pr (g(X) \leq y)
= \Pr \left( X \geq g^{-1}(y) \right)
= 1 - F_X \left( g^{-1} (y) \right) ,
\end{equation*}
and its PDF is given by
\begin{equation*}
\begin{split}
f_Y (y) &= \frac{d}{dy} \left( 1 - F_X \left( g^{-1} (y) \right) \right) \\
&= - f_X \left( g^{-1} (y) \right) \frac{dx}{dy} \\
&= - \frac{f_X (x)}{g'(x)} .
\end{split}
\end{equation*}
Combining these two results, we observe that if $g$ is differentiable and strictly monotone, the PDF of $Y$ is
\begin{equation} \label{equation:MonotoneFunctionPDF}
f_Y (y) = f_X \left( g^{-1} (y) \right) \left| \frac{dx}{dy} \right|
= \frac{f_X (x)}{\left| g'(x) \right|}
\end{equation}
where $x = g^{-1}(y)$.

\begin{example}
Suppose that $X$ is a Gaussian random variable with PDF
\begin{equation*}
f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}} .
\end{equation*}
We wish to find the PDF of random variable $Y$ where $Y = a X + b$ and $a \neq 0$.

In this example, $g(x) = ax + b$.
The inverse of function of $g$ is then equal to
\begin{equation*}
x = g^{-1} (y) = \frac{y - b}{a} ,
\end{equation*}
and its derivative is given by
\begin{equation*}
\frac{dx}{dy} = \frac{d (g^{-1})}{dy} = \frac{1}{a} .
\end{equation*}
The PDF of $Y$ can be computed using \eqref{equation:MonotoneFunctionPDF}
\begin{equation*}
f_Y(y) = f_X \left( g^{-1} (y) \right) \left| \frac{dx}{dy} \right|
= \frac{1}{\sqrt{2 \pi} |a|} e^{- \frac{(y-b)^2}{2 a^2} }.
\end{equation*}
In general, it can be shown that an affine function of a Gaussian random variable is also a Gaussian random variable.
\end{example}

Finally, suppose that $g$ is a differentiable function with a finite number of local extrema.
Then, $g$ is piecewise monotonic and we can write the PDF of $Y= g(X)$ as
\begin{equation} \label{equation:FunctionPDF}
f_Y (y) = \sum_{\{ x \in X(\Omega) | g(x) = y\}}
\frac{f_X (x)}{\left| g'(x) \right|} .
\end{equation}
In words, $f_Y (y)$ is obtained by first identifying all the values of $x$ for which $g(x) = y$.
The PDF of $Y$ is then computed explicitly by finding the local contribution of each of these values to $f_Y(y)$ using the methodology developed above.
This is accomplished by applying \eqref{equation:MonotoneFunctionPDF} repetitively to every value of $x$ such that $g(x) = y$.
% This is illustrated in Figure.
It is certainly useful to compare \eqref{equation:FunctionPDF} to its discrete equivalent \eqref{equation:DefinitionFunctionPMF}, which is easier to understand and visualize.

\begin{example}
The distribution of a Rayleigh random variable is given by
\begin{equation*}
f_X (x) = \frac{x}{\sigma^2} e^{- \frac{x^2}{2 \sigma^2} } \quad x \geq 0,
\end{equation*}
where $\sigma > 0$.
Let $Y = X^2$ and find the distribution of random variable $Y$.

Since $Y$ is the square of $X$, we have $g(x) = x^2$.
The PDF of $Y$ is found to be
\begin{equation*}
f_Y(y) = \frac{f_X (x)}{|g'(x)|}
= \frac{1}{\sigma^2} e^{- \frac{x^2}{2 \sigma^2} }
= \frac{1}{2 \sigma^2} e^{- \frac{y}{2 \sigma^2} }
\end{equation*}
where $y \geq 0$.
Random variable $Y$ has an exponential distribution with parameter $2 \sigma^2$.
Note that we do not need to account for the negative square root $x = - \sqrt{y}$ since the Rayleigh random variable has a one-side PDF.
\end{example}


\section{Expectations}

The concept of an expectation for a continuous random variable is very similar to analog for discrete random variable.
The weighted sum is replaced by a weighted integration.
For a continuous random variable $X$, the \emph{expected value} of $g(X)$ is given by
\begin{equation*}
\Expect [g(X)]
= \int_{X(\Omega)} g(x) f_X (x) dx .
\end{equation*}
In particular, the mean of $X$ is equal to
\begin{equation*}
\Expect [X]
= \int_{X(\Omega)} x f_X (x) dx
\end{equation*}
and the variance is
\begin{equation*}
\begin{split}
\Var (X) &= \Expect \left[ (X - \Expect[X])^2 \right] \\
&= \int_{X(\Omega)} (x - \Expect[X])^2 f_X (x) dx .
\end{split}
\end{equation*}
Note that the variance of continuous random variable $X$ can also be written as
\begin{equation*}
\Var(X) = \Expect \left[ X^2 \right] - \left( E[X] \right)^2 .
\end{equation*}

\begin{example}
We wish to find the mean and the variance of a Gaussian random variable with PDF
\begin{equation*}
f_X (x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-m)^2/2\sigma^2}
\end{equation*}
where $\sigma > 0$.

The mean of $X$ can easily be obtained by a simple change of variable,
\begin{equation*}
\begin{split}
\Expect [X]
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{- \infty}^{\infty} \xi e^{-(\xi-m)^2/2\sigma^2} d\xi \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\left( \zeta+\frac{m}{\sigma} \right) e^{-\zeta^2/2} d\xi \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\zeta e^{-\zeta^2/2} d\xi
+ \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\frac{m}{\sigma} e^{-\zeta^2/2} d\xi \\
&= m.
\end{split}
\end{equation*}
To derive the variance, we start with the following equation derived from the normalization condition
\begin{equation*}
\int_{-\infty}^{\infty} e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} \sigma ,
\end{equation*}
We differentiate both sides of this equation with respect to $\sigma$ and get
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sigma^3}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} .
\end{equation*}
A straightforward rearrangement of the terms yields the desired result,
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sqrt{2 \pi} \sigma}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sigma^2 .
\end{equation*}
Of course, the variance can also be obtained by solving the integral equation explicitly.
\end{example}


\section{Multiple Continuous Random Variables}

Probability density functions can be extended to the case of multiple continuous random variable.
We denote the \emph{joint PDF} of random variables $X$ and $Y$ by $f_{X,Y} (x, y)$.
In general, $f_{X,Y}$ is a nonnegative function such that
\begin{equation*}
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
f_{X, Y} (x, y) dx dy = 1.
\end{equation*}
Furthermore, for a measurable set $S$, the probability that $(X,Y) \in S$ can be computed through the integral
\begin{equation*}
\begin{split}
\Pr ((X,Y) \in S)
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
\SetIn_{S}(x,y) f_{X, Y} (x, y) dx dy \\
&= \int \int_{S}
f_{X, Y} (x, y) dx dy .
\end{split}
\end{equation*}
If $S$ is the rectangular set $S = \{ (x,y) | a \leq x \leq b, c \leq y \leq d \}$, then the probability that $(X,Y) \in S$ becomes the typical integral
\begin{equation*}
\Pr ((X,Y) \in S)
\Pr (a \leq X \leq b, c \leq Y \leq d)
= \int_{a}^{b} \int_{c}^{d}
f_{X, Y} (x, y) dx dy .
\end{equation*}

\begin{example}
\end{example}

\subsection{Joint Cumulative Distribution Functions}

It is also possible to obtain the joint CDF of a pair of random variables
\begin{equation*}
F_{X,Y} (x,y) = \Pr (X \leq x, Y \leq y) .
\end{equation*}
For continuous random variables, the joint CDF of $X$ and $Y$ can derived for their joint PDF,
\begin{equation*}
F_{X,Y} (x,y) = \int_{-\infty}^x \int_{-\infty}^y f_{X,Y} (\xi,\zeta) d\zeta d\xi .
\end{equation*}
Simple calculus asserts that the following is also true,
\begin{equation*}
f_{X,Y} (x,y) = \frac{\partial^2 F_{X,Y}}{\partial x \partial y} (x,y) .
\end{equation*}

\subsection{Conditional Distribution}

It is possible to derive the PDF of a random variable condition on a certain event, or on another random variable.
Let $X$ and $Y$ be two random variable associated with the same experiment.
The \emph{contional probability distribution function} of $Y$ given $X = x$, denoted by $f_{Y|X}$, is defined by
\begin{equation} \label{equation:ContinuousConditonalPDF}
f_{Y|X} (y|x) = \frac{f_{X,Y} (x,y)}{f_X(x)},
\end{equation}
provided that $f_X(x) \neq 0$.
We emphasize that there is a strong similarity between the definition of the conditional PDF and PMF.
For a fixed value $X = x$, the conditional PDF $f_{Y|X} (y|x)$ is a legitimate PDF since it is nonnegative and it integrates to one.

\begin{example}
\end{example}

Let $Y$ be an event such that $\Pr (Y \in S) > 0$.
The conditional PDF of $Y$ given $\{ Y \in S \}$ is defined by
\begin{equation*}
f_{Y|S} (y)
= \left\{ \begin{array}{cc} \frac{ f_Y(y) }{\Pr (Y \in S)}, & y \in S \\
0, & \text{otherwise}. \end{array} \right.
\end{equation*}
This PDF can be used to compute the conditional probability of specific events given $Y \in S$.
Let $T$ be a measurable set, then
\begin{equation*}
\begin{split}
\Pr ( Y \in T | Y \in S)
&= \frac{ \Pr ( Y \in T \cap Y \in S) }{ \Pr ( Y \in S) } \\
&= \frac{ \int_{Y \cap T} f_Y(y) dy }{ \Pr ( Y \in S) } \\
&= \int_{T} f_{Y|S} (y) dy .
\end{split}
\end{equation*}

\begin{example}
\end{example}

The conditional expectation of a function $g(Y)$ is simply the integral of $g(Y)$ weighted by the proper conditional PDF,
\begin{align*}
E[g(Y) | X = x] &= \int_{Y(\Omega)} g(Y) f_{Y|X} (y|x) dy \\
E[g(Y) | S] &= \int_{Y(\Omega)} g(Y) f_{Y|S} (y) dy .
\end{align*}
Note again that the function
\begin{equation*}
h(x) = \Expect [Y | X=x]
\end{equation*}
defines a random variable since the conditional expectation of $Y$ may vary as a function of $X$.
A conditional expectation is, itself, a random variable.

\begin{example}
\end{example}

\subsection{Independence}

Two continuous random variables $X$ and $Y$ are \emph{independent} if their joint PDF is the product of their respective marginal PDFs,
\begin{equation*}
f_{X,Y} (x,y) = f_X (x) f_Y(y)
\end{equation*}
for every $x$ and $y$.
From \eqref{equation:ContinuousConditonalPDF}, we gather that the conditional PDF of $Y$ given $X=x$ is equal to the marginal PDF of $Y$ whenever $X$ and $Y$ are independent
\begin{equation*}
f_{Y|X} (y|x) = \frac{f_{X,Y} (x, y)}{f_X(x)}
= \frac{f_X (x) f_Y (y)}{f_X(x)} = f_Y (y),
\end{equation*}
provided that $f_X(x) \neq 0$.

Furthermore, if $X$ and $Y$ are independent random variables then the two events $\{ X \in S \}$ and $\{ Y \in T \}$ are independent,
\begin{equation*}
\begin{split}
\Pr (X \in S, Y \in T) &= \int_S \int_T f_{X,Y} (x,y) dy dx \\
&= \int_S f_X (x) dx \int_T f_Y (y) dy \\
&= \Pr (X \in S) \Pr (Y \in T).
\end{split}
\end{equation*}

\begin{example}
\end{example}


\section{Some properties of the $Q$-function}

\begin{equation*}
\int_0^{\infty} Q(x) dx = \frac{1}{\sqrt{2 \pi}} .
\end{equation*}

\begin{gather*}
Q(x) < \frac{1}{\sqrt{2 \pi}} x e^{-x^2/2} \quad x > 0 \\
Q(x) \leq \frac{1}{2} e^{-x^2/2} \quad x \geq 0 \\
Q(x) \leq \frac{1}{2} e^{-\sqrt{2/\pi} x} \quad x \geq 0
\end{gather*}

