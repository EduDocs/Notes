\section{Functions of Random Variables}

Much like in the discrete case, it is possible to create a new random variable $Y$ by applying a real-valued function $g$ to an existing random variable $X$.
Specifically, if $g$ is a continuous function and $X$ is a continuous random variable, then so is $Y = g(X)$.
Given $X$ and $g$, the probability that $Y$ falls in a specific set $S$ depends on both the function $g$ and the CDF of $X$,
\begin{equation*}
\Pr (Y \in S) = \Pr (g(X) \in S) 
= \Pr (X \in g^{-1}(S)).
\end{equation*}

To gain insight into this problem, it is best to first consider the situation where $g$ is a differentiable and strictly increasing function.
In this case, we can write the CDF of $Y$ as
\begin{equation*}
F_Y(y) = \Pr (Y \leq y) = \Pr (g(X) \leq y)
= \Pr \left( X \leq g^{-1}(y) \right)
= F_X \left( g^{-1} (y) \right) .
\end{equation*}
Differentiating this equation with respect to $y$, we obtain the PDF of $Y$
\begin{equation*}
\begin{split}
f_Y (y) &= \frac{d}{dy} F_Y(y)
= \frac{d}{dy} F_X \left( g^{-1} (y) \right) \\
&= f_X \left( g^{-1} (y) \right) \frac{d \left( g^{-1} \right)}{dy} \\
&= f_X \left( g^{-1} (y) \right) \frac{dx}{dy} .
\end{split}
\end{equation*}
With the simple substitution $x = g^{-1} (y)$, we get
\begin{equation*}
f_Y (y) = f_X (x) \left. \frac{dx}{dy} \right|_{x = g^{-1}(y)}
= \frac{f_X (x)}{{dy}/{dx}}
= \frac{f_X (x)}{g'(x)} .
\end{equation*}

On the other hand, suppose that $g$ is a differentiable and decreasing function.
In this case, The CDF of the random variable $Y = g(X)$ becomes
\begin{equation*}
F_Y(y) = \Pr (g(X) \leq y)
= \Pr \left( X \geq g^{-1}(y) \right)
= 1 - F_X \left( g^{-1} (y) \right) ,
\end{equation*}
and its PDF is given by
\begin{equation*}
\begin{split}
f_Y (y) &= \frac{d}{dy} \left( 1 - F_X \left( g^{-1} (y) \right) \right) \\
&= - f_X \left( g^{-1} (y) \right) \frac{dx}{dy} \\
&= - \frac{f_X (x)}{g'(x)} .
\end{split}
\end{equation*}
Combining these two results, we observe that if $g$ is differentiable and strictly monotone, the PDF of $Y$ is
\begin{equation} \label{equation:MonotoneFunctionPDF}
f_Y (y) = f_X \left( g^{-1} (y) \right) \left| \frac{dx}{dy} \right|
= \frac{f_X (x)}{\left| g'(x) \right|}
\end{equation}
where $x = g^{-1}(y)$.

\begin{example}
Suppose that $X$ is a Gaussian random variable with PDF
\begin{equation*}
f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}} .
\end{equation*}
We wish to find the PDF of random variable $Y$ where $Y = a X + b$ and $a \neq 0$.

In this example, $g(x) = ax + b$.
The inverse of function of $g$ is then equal to
\begin{equation*}
x = g^{-1} (y) = \frac{y - b}{a} ,
\end{equation*}
and its derivative is given by
\begin{equation*}
\frac{dx}{dy} = \frac{d (g^{-1})}{dy} = \frac{1}{a} .
\end{equation*}
The PDF of $Y$ can be computed using \eqref{equation:MonotoneFunctionPDF}
\begin{equation*}
f_Y(y) = f_X \left( g^{-1} (y) \right) \left| \frac{dx}{dy} \right|
= \frac{1}{\sqrt{2 \pi} |a|} e^{- \frac{(y-b)^2}{2 a^2} }.
\end{equation*}
In general, it can be shown that an affine function of a Gaussian random variable is also a Gaussian random variable.
\end{example}

Finally, suppose that $g$ is a differentiable function with a finite number of local extrema.
Then, $g$ is piecewise monotonic and we can write the PDF of $Y= g(X)$ as
\begin{equation} \label{equation:FunctionPDF}
f_Y (y) = \sum_{\{ x \in X(\Omega) | g(x) = y\}}
\frac{f_X (x)}{\left| g'(x) \right|} .
\end{equation}
In words, $f_Y (y)$ is obtained by first identifying all the values of $x$ for which $g(x) = y$.
The PDF of $Y$ is then computed explicitly by finding the local contribution of each of these values to $f_Y(y)$ using the methodology developed above.
This is accomplished by applying \eqref{equation:MonotoneFunctionPDF} repetitively to every value of $x$ such that $g(x) = y$.
% This is illustrated in Figure.
It is certainly useful to compare \eqref{equation:FunctionPDF} to its discrete equivalent \eqref{equation:DefinitionFunctionPMF}, which is easier to understand and visualize.

\begin{example}
The distribution of a Rayleigh random variable is given by
\begin{equation*}
f_X (x) = \frac{x}{\sigma^2} e^{- \frac{x^2}{2 \sigma^2} } \quad x \geq 0,
\end{equation*}
where $\sigma > 0$.
Let $Y = X^2$ and find the distribution of random variable $Y$.

Since $Y$ is the square of $X$, we have $g(x) = x^2$.
The PDF of $Y$ is found to be
\begin{equation*}
f_Y(y) = \frac{f_X (x)}{|g'(x)|}
= \frac{1}{\sigma^2} e^{- \frac{x^2}{2 \sigma^2} }
= \frac{1}{2 \sigma^2} e^{- \frac{y}{2 \sigma^2} }
\end{equation*}
where $y \geq 0$.
Random variable $Y$ has an exponential distribution with parameter $2 \sigma^2$.
Note that we do not need to account for the negative square root $x = - \sqrt{y}$ since the Rayleigh random variable has a one-side PDF.
\end{example}


\section{Expectations}

The concept of an expectation for a continuous random variable is very similar to analog for discrete random variable.
The weighted sum is replaced by a weighted integration.
For a continuous random variable $X$, the \emph{expected value} of $g(X)$ is given by
\begin{equation*}
\Expect [g(X)]
= \int_{X(\Omega)} g(x) f_X (x) dx .
\end{equation*}
In particular, the mean of $X$ is equal to
\begin{equation*}
\Expect [X]
= \int_{X(\Omega)} x f_X (x) dx
\end{equation*}
and the variance is
\begin{equation*}
\begin{split}
\Var (X) &= \Expect \left[ (X - \Expect[X])^2 \right] \\
&= \int_{X(\Omega)} (x - \Expect[X])^2 f_X (x) dx .
\end{split}
\end{equation*}
Note that the variance of continuous random variable $X$ can also be written as
\begin{equation*}
\Var(X) = \Expect \left[ X^2 \right] - \left( E[X] \right)^2 .
\end{equation*}

\begin{example}
We wish to find the mean and the variance of a Gaussian random variable with PDF
\begin{equation*}
f_X (x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-m)^2/2\sigma^2}
\end{equation*}
where $\sigma > 0$.

The mean of $X$ can easily be obtained by a simple change of variable,
\begin{equation*}
\begin{split}
\Expect [X]
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{- \infty}^{\infty} \xi e^{-(\xi-m)^2/2\sigma^2} d\xi \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\left( \zeta+\frac{m}{\sigma} \right) e^{-\zeta^2/2} d\xi \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\zeta e^{-\zeta^2/2} d\xi
+ \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\frac{m}{\sigma} e^{-\zeta^2/2} d\xi \\
&= m.
\end{split}
\end{equation*}
To derive the variance, we start with the following equation derived from the normalization condition
\begin{equation*}
\int_{-\infty}^{\infty} e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} \sigma ,
\end{equation*}
We differentiate both sides of this equation with respect to $\sigma$ and get
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sigma^3}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} .
\end{equation*}
A straightforward rearrangement of the terms yields the desired result,
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sqrt{2 \pi} \sigma}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sigma^2 .
\end{equation*}
Of course, the variance can also be obtained by solving the integral equation explicitly.
\end{example}


\section{Multiple Continuous Random Variables}

Probability density functions can be extended to the case of multiple continuous random variable.
We denote the \emph{joint PDF} of random variables $X$ and $Y$ by $f_{X,Y} (x, y)$.
In general, $f_{X,Y}$ is a nonnegative function such that
\begin{equation*}
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
f_{X, Y} (x, y) dx dy = 1.
\end{equation*}
Furthermore, for a measurable set $S$, the probability that $(X,Y) \in S$ can be computed through the integral
\begin{equation*}
\begin{split}
\Pr ((X,Y) \in S)
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
\SetIn_{S}(x,y) f_{X, Y} (x, y) dx dy \\
&= \int \int_{S}
f_{X, Y} (x, y) dx dy .
\end{split}
\end{equation*}
If $S$ is the rectangular set $S = \{ (x,y) | a \leq x \leq b, c \leq y \leq d \}$, then the probability that $(X,Y) \in S$ becomes the typical integral
\begin{equation*}
\Pr ((X,Y) \in S)
\Pr (a \leq X \leq b, c \leq Y \leq d)
= \int_{a}^{b} \int_{c}^{d}
f_{X, Y} (x, y) dx dy .
\end{equation*}

\begin{example}
\end{example}

\subsection{Joint Cumulative Distribution Functions}

It is also possible to obtain the joint CDF of a pair of random variables
\begin{equation*}
F_{X,Y} (x,y) = \Pr (X \leq x, Y \leq y) .
\end{equation*}
For continuous random variables, the joint CDF of $X$ and $Y$ can derived for their joint PDF,
\begin{equation*}
F_{X,Y} (x,y) = \int_{-\infty}^x \int_{-\infty}^y f_{X,Y} (\xi,\zeta) d\zeta d\xi .
\end{equation*}
Simple calculus asserts that the following is also true,
\begin{equation*}
f_{X,Y} (x,y) = \frac{\partial^2 F_{X,Y}}{\partial x \partial y} (x,y) .
\end{equation*}

\subsection{Conditional Distribution}

It is possible to derive the PDF of a random variable condition on a certain event, or on another random variable.
Let $X$ and $Y$ be two random variable associated with the same experiment.
The \emph{contional probability distribution function} of $Y$ given $X = x$, denoted by $f_{Y|X}$, is defined by
\begin{equation} \label{equation:ContinuousConditonalPDF}
f_{Y|X} (y|x) = \frac{f_{X,Y} (x,y)}{f_X(x)},
\end{equation}
provided that $f_X(x) \neq 0$.
We emphasize that there is a strong similarity between the definition of the conditional PDF and PMF.
For a fixed value $X = x$, the conditional PDF $f_{Y|X} (y|x)$ is a legitimate PDF since it is nonnegative and it integrates to one.

\begin{example}
\end{example}

Let $Y$ be an event such that $\Pr (Y \in S) > 0$.
The conditional PDF of $Y$ given $\{ Y \in S \}$ is defined by
\begin{equation*}
f_{Y|S} (y)
= \left\{ \begin{array}{cc} \frac{ f_Y(y) }{\Pr (Y \in S)}, & y \in S \\
0, & \text{otherwise}. \end{array} \right.
\end{equation*}
This PDF can be used to compute the conditional probability of specific events given $Y \in S$.
Let $T$ be a measurable set, then
\begin{equation*}
\begin{split}
\Pr ( Y \in T | Y \in S)
&= \frac{ \Pr ( Y \in T \cap Y \in S) }{ \Pr ( Y \in S) } \\
&= \frac{ \int_{Y \cap T} f_Y(y) dy }{ \Pr ( Y \in S) } \\
&= \int_{T} f_{Y|S} (y) dy .
\end{split}
\end{equation*}

\begin{example}
\end{example}

The conditional expectation of a function $g(Y)$ is simply the integral of $g(Y)$ weighted by the proper conditional PDF,
\begin{align*}
E[g(Y) | X = x] &= \int_{Y(\Omega)} g(Y) f_{Y|X} (y|x) dy \\
E[g(Y) | S] &= \int_{Y(\Omega)} g(Y) f_{Y|S} (y) dy .
\end{align*}
Note again that the function
\begin{equation*}
h(x) = \Expect [Y | X=x]
\end{equation*}
defines a random variable since the conditional expectation of $Y$ may vary as a function of $X$.
A conditional expectation is, itself, a random variable.

\begin{example}
\end{example}

\subsection{Independence}

Two continuous random variables $X$ and $Y$ are \emph{independent} if their joint PDF is the product of their respective marginal PDFs,
\begin{equation*}
f_{X,Y} (x,y) = f_X (x) f_Y(y)
\end{equation*}
for every $x$ and $y$.
From \eqref{equation:ContinuousConditonalPDF}, we gather that the conditional PDF of $Y$ given $X=x$ is equal to the marginal PDF of $Y$ whenever $X$ and $Y$ are independent
\begin{equation*}
f_{Y|X} (y|x) = \frac{f_{X,Y} (x, y)}{f_X(x)}
= \frac{f_X (x) f_Y (y)}{f_X(x)} = f_Y (y),
\end{equation*}
provided that $f_X(x) \neq 0$.

Furthermore, if $X$ and $Y$ are independent random variables then the two events $\{ X \in S \}$ and $\{ Y \in T \}$ are independent,
\begin{equation*}
\begin{split}
\Pr (X \in S, Y \in T) &= \int_S \int_T f_{X,Y} (x,y) dy dx \\
&= \int_S f_X (x) dx \int_T f_Y (y) dy \\
&= \Pr (X \in S) \Pr (Y \in T).
\end{split}
\end{equation*}

\begin{example}
\end{example}


\section{Some properties of the $Q$-function}

\begin{equation*}
\int_0^{\infty} Q(x) dx = \frac{1}{\sqrt{2 \pi}} .
\end{equation*}

\begin{gather*}
Q(x) < \frac{1}{\sqrt{2 \pi}} x e^{-x^2/2} \quad x > 0 \\
Q(x) \leq \frac{1}{2} e^{-x^2/2} \quad x \geq 0 \\
Q(x) \leq \frac{1}{2} e^{-\sqrt{2/\pi} x} \quad x \geq 0
\end{gather*}

