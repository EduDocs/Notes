\chapter{Convergence, Sequences and Limit Theorems}

Some of the most astonishing results in probability are related to the properties of sequences of random variables and the convergence of empirical distributions.
From an engineering viewpoint, these results are important as they enable the efficient design of complex systems with very small probabilities of failure.
Concentration behavior facilitates true economies of scale. 


\section{Types of Convergence}

The premise on which most probabilistic convergence results lie is a sequence of random variables $X_1, X_2, \ldots$ and a limiting random variable $X$, all of which are defined on the same probability space.
Recall that a random variable is a function of the outcome of a random experiment.
The above statement stipulate that all the random variables listed above are functions of the outcome of a same experiment.

Statements that can be made about a sequence of random variables range from simple assertions to more intricate claims.
For instance, the sequence may appear to move toward a deterministic quantity or to behave increasingly akin to a certain function.
Alternatively, the CDFs of the random variables in the sequence may appear to approach a precise function.
Being able to recognize specific patterns within the sequence is key in establishing converge results.
The various statement one can make about the sequence $X_1, X_2, \ldots$ lead to the different types of convergence encountered in probability.
Below, we discuss briefly three types of convergence.

\begin{example} \label{example:GaussianLLN}
Suppose that $X_1, X_2, \ldots$ is a sequence of independent Gaussian random variables, each with mean $m$ and variance $\sigma^2$.
Define the partial sums
\begin{equation} \label{equation:GaussianEmpiricalSum}
S_n = \sum_{i=1}^n X_i ,
\end{equation}
and consider the sequence
\begin{equation} \label{equation:GaussianEmpiricalAverage}
S_1, \frac{S_2}{2}, \frac{S_3}{3}, \ldots
\end{equation}
We know that affine transformations of Gaussian random variables remain Gaussian.
Furthermore, we know that sums of jointly Gaussian random variables are also Gaussian.
Thus, $S_n / n$ possesses a Gaussian distribution with mean
\begin{equation*}
\Expect \left[ \frac{S_n}{n} \right]
= \frac{ \Expect [ S_n ] }{n}
= \frac{ \Expect [ X_1 ] + \cdots + \Expect [ X_n ] }{n}
= m
\end{equation*}
and variance
\begin{equation*}
\Var \left[ \frac{S_n}{n} \right]
= \frac{ \Var [ S_n ] }{n^2}
= \frac{ \Var [ X_1 ] + \cdots + \Var [ X_n ] }{n^2}
= \frac{ \sigma^2 }{n} .
\end{equation*}
It appears that the PDF of $S_n / n$ concentrates around $m$ as $n$ approaches infinity.
That is, the sequence in \eqref{equation:GaussianEmpiricalAverage} seems to become increasingly predictable.
\end{example}

\begin{example} \label{example:GaussianCLT}
Again, let $X_1, X_2, \ldots$ be the sequence described above, and let $S_n$ be defined according to \eqref{equation:GaussianEmpiricalSum}.
This time, we wish to characterize the properties of
\begin{equation*}
S_1 - m, \frac{S_2 - 2m}{\sqrt{2}}, \frac{S_3 - 3m}{\sqrt{3}}, \ldots
\end{equation*}
From our current discussion, we know that $(S_n -nm) / \sqrt{n}$ is a Gaussian random variables.
We can compute its mean and variance as follows
\begin{gather*}
\Expect \left[ \frac{S_n -nm}{\sqrt{n}} \right]
= \frac{ \Expect [ S_n - nm] }{n}
= 0 \\
\Var \left[ \frac{S_n - mn}{\sqrt{n}} \right]
= \frac{ \Var [ S_n - mn] }{n}
= \frac{ \Var [ S_n ] }{n}
= \sigma^2 .
\end{gather*}
No matter how large $n$ is, the random variable $(S_n - nm)/\sqrt{n}$ has a Gaussian distribution with mean zero and variance $\sigma^2$.
Intriguingly, the distributions remains invariant throughout the sequence.
\end{example}


\subsection{Convergence in Probability}

The basic concept behind the definition of \emph{convergence in probability} is that the probability that a random variable deviates from its typical behavior becomes less likely as the sequence progresses. \index{Convergence in probability}
Formally, a sequence $X_1, X_2, \ldots$ of random variables converges in probability to $X$ if for every $\epsilon > 0$,
\begin{equation*}
\lim_{n \rightarrow \infty} \Pr \left( \left| X_n - X \right| \geq \epsilon \right) = 0 .
\end{equation*}
In Example~\ref{example:GaussianLLN}, the sequence $\{ S_n / n \}$ converges in probability to $m$.


\subsection{Mean Square Convergence}

We say that a sequence $X_1, X_2, \ldots$ of random variables \emph{converges in mean square} to $X$ if \index{Mean square convergence}
\begin{equation*}
\lim_{n \rightarrow \infty} \Expect \left[ \left| X_n - X \right|^2 \right] = 0 .
\end{equation*}
That is, the second moment of the difference between $X_n$ and $X$ vanishes as $n$ goes to infinity.
Convergence in the mean square sense implies convergence in probability.

\begin{proposition} \label{proposition:ConvergenceImplication1}
Let $X_1, X_2, \ldots$ be a sequence of random variables that converge in mean square to $X$.
Then, the sequence $X_1, X_2, \ldots$ also converges to $X$ in probability.
\end{proposition}
\begin{proof}
Suppose that $\epsilon > 0$ is fixed.
The sequence $X_1, X_2, \ldots$ converges in mean square to $X$.
Thus, for $\delta > 0$, there exists an $N$ such that $n \geq N$ implies
\begin{equation*}
\lim_{n \rightarrow \infty} \Expect \left[ \left| X_n - X \right|^2 \right] < \delta .
\end{equation*}
If we apply the Chebyshev inequality to $X_n - X$, we get
\begin{equation*}
\Pr \left( \left| X_n - X \right| \geq \epsilon \right)
\leq \frac{ \Expect \left[ \left| X_n - X \right|^2 \right] }{ \epsilon^2 }
< \frac{ \delta }{ \epsilon^2 }
\end{equation*}
Since $\delta$ can be made arbitrarily small, we conclude that this sequence also converges to $X$ in probability.
\end{proof}

\subsection{Convergence in Distribution}

A sequence $X_1, X_2, \ldots$ of random variables is said to \emph{converge in distribution} to a random variable $X$ if \index{Convergence in distribution}
\begin{equation*}
\lim_{n \rightarrow \infty} F_{X_n} (x) = F_X (x)
\end{equation*}
at every point $x \in \RealNumbers$ where $F_X (\cdot)$ is continuous.
This type of convergence is also called \emph{weak convergence}.  \index{Weak convergence}

\begin{example}
Let $X_n$ be a continuous random variable that is uniformly distributed over $[0, 1/n]$.
Then, the sequence $X_1, X_2, \ldots$ converges in distribution to $0$.

In this example, $X = 0$ and
\begin{equation*}
F_X (x) = \begin{cases} 1 & x \geq 0 \\
0 & x < 0. \end{cases}
\end{equation*}
Furthermore, for every $x < 0$, we have $F_{X_n} (x) = 0$; and for every $x > 0$, we have
\begin{equation*}
\lim_{n \rightarrow \infty} F_{X_n} (x) = 1.
\end{equation*}
Hence, the sequence $X_1, X_2, \ldots$ converges in distribution to a constant.
\end{example}


\section{The Law of Large Numbers}

The law of large numbers focuses on the convergence of empirical averages.
Although, there are many versions of this law, we only state its simplest form below.
Suppose that $X_1, X_2, \ldots$ is a sequence of independent and identically distributed random variable, each with finite second moment.
Furthermore, for $n \geq 1$, define the empirical sum
\begin{equation*}
S_n = \sum_{i=1}^n X_n .
\end{equation*}
The law of large number asserts that the sequence of empirical averages,
\begin{equation*}
\frac{S_n}{n} = \frac{1}{n} \sum_{i=1}^n X_n ,
\end{equation*}
converges in probability to the mean $\Expect [X]$.

\begin{theorem}[Law of Large Numbers] \index{Law of large numbers}
Let $X_1, X_2, \ldots$ be independent and identically distributed random variables with mean $\Expect [X]$ and finite variance.
For every $\epsilon > 0$, we have
\begin{equation*}
\lim_{n \rightarrow \infty}
\Pr \left( \left| \frac{S_n}{n} - \Expect [X] \right| \geq \epsilon \right)
= \lim_{n \rightarrow \infty}
\Pr \left( \left| \frac{X_1 + \cdots + X_n}{n} - \Expect [X] \right| \geq \epsilon \right)
= 0 .
\end{equation*}
\end{theorem}
\begin{proof}
Taking the expectation of the empirical average, we obtain
\begin{equation*}
\Expect \left[ \frac{S_n}{n} \right]
= \frac{ \Expect \left[ S_n \right] }{n}
= \frac{\Expect [X_1] + \cdots + \Expect [X_n] }{n}
= \Expect [X] .
\end{equation*}
Using independence, we also have
\begin{equation*}
\Var \left[ \frac{S_n}{n} \right]
= \frac{ \Var [X_1] + \cdots + \Var [X_n] }{n^2}
= \frac{ \Var [X] }{n} .
\end{equation*}
As $n$ goes to infinity, the variance of the empirical average $S_n / n$ vanishes.
Thus, we showed that the sequence $\{ S_n / n \}$ of empirical averages converges in mean square to $\Expect [X]$ since
\begin{equation*}
\lim_{n \rightarrow \infty}
\Expect \left[ \left| \frac{S_n}{n} - \Expect [X] \right|^2 \right] 
= \lim_{n \rightarrow \infty}
\Var \left[ \frac{S_n}{n} \right] = 0 .
\end{equation*}
To get convergence in probability, we apply the Chebyshev inequality, as we did in Proposition~\ref{proposition:ConvergenceImplication1},
\begin{equation*}
\Pr \left( \left| \frac{S_n}{n} - \Expect [X] \right| \geq \epsilon \right)
\leq \frac{ \Var \left[ \frac{S_n}{n} \right] }{\epsilon^2}
= \frac{ \Var [X] }{n \epsilon^2} ,
\end{equation*}
which clearly goes to zero as $n$ approaches infinity.
\end{proof}

\begin{example}
Suppose that a die is thrown repetitively.
We are interested in the average number of times a six shows up on the top face, as the number of throws becomes very large.

Let $D_n$ be a random variable that represent the number on the $n$th roll.
Also, define the random variable $X_n = \IndicatorFcn_{\{ D_n = 6 \}}$.
Then, $X_n$ is a Bernoulli random variable with parameter $p = 1/6$, and the empirical average $S_n / n$ is equal to the number of times a six is observed divided by the total number of rolls.
By the law of large numbers, we have
\begin{equation*}
\lim_{n \rightarrow \infty}
\Pr \left( \left| \frac{S_n}{n} - \frac{1}{6} \right| \geq \epsilon \right) = 0 .
\end{equation*}
That is, as the number of rolls increases, the average number of times a six is observe converges to the probability of getting a six.
\end{example}


\subsection{Heavy-Tailed Distributions*}

There are situations where the law of large numbers does not apply.
For example, when dealing with \emph{heavy-tailed distributions}, one needs to be very careful.
In this section, we study Cauchy random variables in greater details.
First, we show that the sum of two independent Cauchy random variables is itself a Cauchy random variable.

Let $X_1$ and $X_2$ be two independent Cauchy random variables with parameter $\gamma_1$ and $\gamma_2$, respectively.
We wish to compute the PDF of $S = X_1 + X_2$.
For continuous random variable, the PDF of $S$ is given by the convolution of $f_{X_1} (\cdot)$ and $f_{X_2} (\cdot)$.
Thus, we can write
\begin{equation*}
\begin{split}
f_S (x) &= \int_{-\infty}^{\infty} f_{X_1} (\xi) f_{X_2} (x - \xi) d\xi \\
 &= \int_{-\infty}^{\infty}
\frac{\gamma_1}{\pi \left( \gamma_1^2 + \xi^2 \right)}
\frac{\gamma_2}{\pi \left( \gamma_2^2 + (x - \xi)^2 \right)} d\xi .
\end{split}
\end{equation*}
This integral is somewhat difficult to solve.
We therefore resort to complex analysis and contour integration to get a solution.
Let $C$ be a contour that goes along the real line from $-a$ to $a$, and then counterclockwise along a semicircle centered at zero.
For $a$ large enough, Cauchy's residue theorem requires that
\begin{equation} \label{equation:CauchyConvolutionContour}
\begin{split}
\oint_{C} f_{X_1} (z) f_{X_2} (x - z) dz
&= \oint_{C} 
\frac{\gamma_1}{\pi \left( \gamma_1^2 + z^2 \right)}
\frac{\gamma_2}{\pi \left( \gamma_2^2 + (x - z)^2 \right)} dz \\
&= 2 \pi i \left( \operatorname{Res} (g, i \gamma_1)
+ \operatorname{Res} (g, x + i \gamma_2) \right)
\end{split}
\end{equation}
where we have implicitly defined the function
\begin{equation*}
\begin{split}
g(z) &= \frac{\gamma_1 \gamma_2}{\pi^2 \left( \gamma_1^2 + z^2 \right)
\left( \gamma_2^2 + (z - x)^2 \right) } \\
&= \frac{\gamma_1 \gamma_2}{\pi^2
( z - i \gamma_1 ) ( z + i \gamma_1 )
( z - x + i \gamma_2 ) ( z - x - i \gamma_2 ) } .
\end{split}
\end{equation*}
Only two residues are contained within the enclosed region.
Because they are simple poles, their values are given by
\begin{align*}
\operatorname{Res} (g, i \gamma_1)
&= \lim_{z \rightarrow i \gamma_1} (z - i \gamma_1) g(z)
= \frac{\gamma_2}{2 i \pi^2
\left( ( x - i \gamma_1 )^2 + \gamma_2^2 \right) } \\
\operatorname{Res} (g, x + i \gamma_2)
&= \lim_{z \rightarrow x + i \gamma_2} (z - x - i \gamma_2) g(z)
= \frac{\gamma_1}{2 i \pi^2
\left( ( x + i \gamma_2 )^2 + \gamma_1^2 \right) } .
\end{align*}
It follows that
\begin{equation*}
\begin{split}
& 2 \pi i \left( \operatorname{Res} (g, i \gamma_1)
+ \operatorname{Res} (g, x + i \gamma_2) \right) \\
&= \frac{\gamma_2}{\pi \left( ( x - i \gamma_1 )^2 + \gamma_2^2 \right) }
+ \frac{\gamma_1}{\pi \left( ( x + i \gamma_2 )^2 + \gamma_1^2 \right) } \\
%&= \frac{\gamma_2}{\pi (x - i \gamma_1 - i \gamma_2) (x - i \gamma_1 + i \gamma_2) }
%+ \frac{\gamma_1}{\pi (x + i \gamma_2 - i \gamma_1) (x + i \gamma_2 + i \gamma_1) } \\
%&= \frac{\gamma_2 (x + i \gamma_2 + i \gamma_1) + \gamma_1 (x - i \gamma_1 - i \gamma_2)}
%{\pi (x - i \gamma_1 - i \gamma_2) (x - i \gamma_1 + i \gamma_2)
%(x + i \gamma_2 + i \gamma_1) } \\
%&= \frac{(\gamma_1 + \gamma_2) (x - i \gamma_1 + i \gamma_2)}
%{\pi (x - i \gamma_1 - i \gamma_2) (x - i \gamma_1 + i \gamma_2)
%(x + i \gamma_2 + i \gamma_1) } \\
&= \frac{(\gamma_1 + \gamma_2)}
{\pi \left( (\gamma_1 + \gamma_2)^2 + x^2 \right) }
\end{split}
\end{equation*}
The contribution of the arc in \eqref{equation:CauchyConvolutionContour} vanishes as $a \rightarrow \infty$.
We then conclude that the PDF of $S$ is equal to
\begin{equation*}
f_S (x) = \frac{(\gamma_1 + \gamma_2)}
{\pi \left( (\gamma_1 + \gamma_2)^2 + x^2 \right) } .
\end{equation*}
The sum of two independent Cauchy random variables with parameters $\gamma_1$ and $\gamma$ is itself a Cauchy random variable with parameter $\gamma_1 + \gamma_2$.

Let $X_1, X_2, \ldots$ form a sequence of independent Cauchy random variables, each with parameter $\gamma$.
Also, consider the empirical sum
\begin{equation*}
S_n = \sum_{i=1}^n X_n .
\end{equation*}
Using mathematical induction and the aforementioned fact, it is possible to show that $S_n$ is a Cauchy random variable with parameter $n \gamma$.
Furthermore, for $x \in \RealNumbers$, the PDF of the empirical average $S_n / n$ is given by
\begin{equation*}
\frac{f_{S_n} ( n x )}{\left| \frac{1}{n} \right|}
= \frac{n^2 \gamma}{\pi \left( n^2 \gamma^2 + (nx)^2 \right)}
= \frac{\gamma}{\pi \left( \gamma^2 + x^2 \right)} ,
\end{equation*}
where we have used the methodology developed in Chapter~\ref{chapter:DerivedDistributions}.
Amazingly, the empirical average of a sequence of independent Cauchy random variables, each with parameter $\gamma$, remains a Cauchy random variable with the same parameter.
Clearly, the law of large numbers does not apply to this scenario.
Note that our version of the law of large numbers requires random variables to have finite second moments, a condition that is clearly violated by the Cauchy distribution.
This explains why convergence does not take place in this situation.


\section{The Central Limit Theorem}

The \emph{central limit theorem} is a remarkable result in probability; it partly explains the prevalence of Gaussian random variables.
In some sense, it captures the behavior of large sums of small, independent random components.

\begin{theorem}[Central Limit Theorem] \index{Central limit theorem}
Let $X_1, X_2, \ldots$ be independent and identically distributed random variables, each with mean $\Expect [X]$ and variance $\sigma^2$.
The distribution of
\begin{equation*}
\frac{ S_n - n \Expect [X] }{ \sigma \sqrt{n} }
\end{equation*}
converges in distribution to a standard normal random variable as $n \rightarrow \infty$.
In particular, for any $x \in \RealNumbers$,
\begin{equation*}
\lim_{n \rightarrow \infty}
\Pr \left( \frac{ S_n - n \Expect [X] }{ \sigma \sqrt{n} } \leq x \right)
= \int_{- \infty}^x \frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2}{2}} d\xi .
\end{equation*}
\end{theorem}

\begin{proof}
Initially, we assume that $\Expect [X] = 0$ and $\sigma^2 = 1$.
Furthermore, we only study the situation where the moment generating function of $X$ exists and is finite.
Consider the log-moment generating function of $X$,
\begin{equation*}
\Lambda_X (s) = \log M_X (s)
= \log \Expect \left[ e^{sX} \right] .
\end{equation*}
The first two derivatives of $\Lambda_X (s)$ are equal to
\begin{gather*}
\frac{d \Lambda_X}{ds} (s)
= \frac{1}{M_X (s)} \frac{d M_X}{ds} (s) \\
\frac{d^2 \Lambda_X}{ds^2} (s)
= \frac{1}{M_X (s)} \left( \frac{d M_X}{ds} (s) \right)^2
+ \frac{1}{M_X (s)} \frac{d^2 M_X}{ds^2} (s) .
\end{gather*}
Collecting these results and evaluating the functions at zero, we get $\Lambda_X (0) = 0$, $\frac{d \Lambda_X}{ds} (0) = \Expect [X] = 0$, and $\frac{d^2 \Lambda_X}{ds^2} (0) = \Expect \left[ X^2 \right] = 1$.
Next, we study the log-moment generating function of $S_n / \sqrt{n}$.
Recall that the expectation of a product of independent random variables is equal to the product of their individual expectations.
Using this property, we get
\begin{equation*}
\begin{split}
\log \Expect \left[ e^{s S_n / \sqrt{n} } \right]
&= \log \Expect \left[ e^{s X_1 / \sqrt{n} }
\cdots e^{s X_n / \sqrt{n} } \right] \\
&= \log \left( M_X \left( \frac{s}{\sqrt{n}} \right)
\cdots M_X \left( \frac{s}{\sqrt{n}} \right) \right) \\
&= n \Lambda_X \left( s n^{-1/2} \right) .
\end{split}
\end{equation*}
To explore the asymptotic behavior of the sequence $\{ S_n / \sqrt{n} \}$, we take the limit of $n \Lambda_X \left( s n^{-1/2} \right)$ as $n \rightarrow \infty$.
In doing so, notice the double use of L'H\^{o}pital's rule,
\begin{equation*}
\begin{split}
\lim_{n \rightarrow \infty} \frac{1}{n^{-1}} \Lambda \left( s n^{-1/2} \right)
&= \lim_{n \rightarrow \infty} \frac{1}{n^{-2}}
\frac{d \Lambda}{ds} \left( s n^{-1/2} \right) \frac{s n^{-3/2}}{2} \\
&= \lim_{n \rightarrow \infty} \frac{s}{2 n^{-1/2}}
\frac{d \Lambda}{ds} \left( s n^{-1/2} \right) \\
&= \lim_{n \rightarrow \infty} \frac{s}{n^{-3/2}}
\frac{d^2 \Lambda}{ds^2} \left( s n^{-1/2} \right) \frac{s n^{-3/2}}{2} \\
&= \lim_{n \rightarrow \infty} \frac{s^2}{2}
\frac{d^2 \Lambda}{ds^2} \left( s n^{-1/2} \right) = \frac{s^2}{2} .
\end{split}
\end{equation*}
That is, the moment generating function of $S_n / \sqrt{n}$ converges point-wise to $e^{s^2/2}$ as $n \rightarrow \infty$.
In fact, this implies that
\begin{equation*}
\Pr \left( \frac{S_n}{\sqrt{n}} \leq x \right) \rightarrow \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{-\frac{\xi^2}{2}} d\xi
\end{equation*}
as $n \rightarrow \infty$.
In words, $S_n / \sqrt{n}$ converges in distribution to a standard normal random variable.
The more general case where $\Expect [X]$ and $\Var [X]$ are arbitrary constants can be established in an analog manner by proper scaling of the random variables $X_1, X_2, \ldots$
\end{proof}

In the last step of the proof, we stated that point-wise convergence of the moment generating functions implies convergence in distribution.
This is a sophisticated result that we quote without proof.


\subsection{Normal Approximation}

The central limit theorem can be employed to approximate the CDF of large sums.
Again, let
\begin{equation*}
S_n = X_1 + \cdots + X_n
\end{equation*}
where $X_1, X_2, \ldots$ are independent and identically distributed random variables with mean $\Expect [X]$ and variance $\sigma^2$.
When $n$ is large, the CDF of $S_n$ can be estimated by approximating
\begin{equation*}
\frac{ S_n - n \Expect [X] } { \sigma \sqrt{n} }
\end{equation*}
as a standard normal random variable.
More specifically, we have
\begin{equation*}
\begin{split}
F_{S_n} (x) 
&= \Pr (S_n \leq x)
= \Pr \left( \frac{ S_n - n \Expect [X] }{ \sigma \sqrt{n} }
\leq \frac{x - n \Expect [X]}{\sigma \sqrt{n}} \right) \\
&\approx \Phi \left( \frac{x - n \Expect [x]}{\sigma \sqrt{n}} \right) ,
\end{split}
\end{equation*}
where $\Phi (\cdot)$ is the CDF of a standard normal random variable.


%\subsection{The Hoeffding Bound}


%
%See Fine's book page~493.
%

\section*{Further Reading}

\begin{small}
\begin{enumerate}
\item Ross, S., \emph{A First Course in Probability}, 7th edition, Pearson Prentice Hall, 2006: Chapter~8.
\item Bertsekas, D. P., and Tsitsiklis, J. N., \emph{Introduction to Probability}, Athena Scientific, 2002: Sections~7.2--7.4.
\item Miller, S. L., and Childers, D. G., \emph{Probability and Random Processes with Applications to Signal Processing and Communications}, 2004: Chapter~7.
\end{enumerate}
\end{small}

