\chapter{Expectations and Bounds}

The concept of expectation, which was originally introduced in the context of discrete random variables, can be generalized to other types of random variables.
We know from our previous discussion that expectations provide an effective way to summarize the information contained in the distribution of a random variable.
As we will see, expectations are also very useful in establishing bounds on probabilities.


\section{Continuous Expectations}

The definition of expectation for continuous random variables is very similar to it discrete counterpart;
the weighted sum is replaced by a weighted integral.
For a continuous random variable $X$, the \emph{expectation} of $g(X)$ is given by \index{Expectation}
\begin{equation*}
\Expect [g(X)]
= \int_{X(\Omega)} g(\xi) f_X (\xi) d\xi .
\end{equation*}
In particular, the \emph{mean} of $X$ is equal to \index{Mean}
\begin{equation*}
\Expect [X]
= \int_{X(\Omega)} x f_X (x) dx
\end{equation*}
and its \emph{variance} becomes \index{variance}
\begin{equation*}
\Var (X) = \Expect \left[ (X - \Expect[X])^2 \right]
= \int_{X(\Omega)} (\xi - \Expect[X])^2 f_X (\xi) d\xi .
\end{equation*}
We note that the variance of continuous random variable $X$ can also be computed as $\Var(X) = \Expect \left[ X^2 \right] - \left( E[X] \right)^2$.

\begin{example}
We wish to find the mean and variance of a Gaussian random variable with parameter $m$ and $\sigma^2$.
The PDF of this random variable is therefore given by
\begin{equation*}
f_X (\xi) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(\xi - m)^2}{2\sigma^2}}
\end{equation*}
for $- \infty < \xi < \infty$.

The mean of $X$ can be obtained by a simple change of variable,
\begin{equation*}
\begin{split}
\Expect [X]
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{- \infty}^{\infty} \xi e^{-\frac{(\xi-m)^2}{2\sigma^2}} d\xi \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\left( \zeta+\frac{m}{\sigma} \right) e^{-\frac{\zeta^2}{2}} d\zeta \\
&= \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\zeta e^{-\frac{\zeta^2}{2}} d\zeta
+ \frac{\sigma}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\frac{m}{\sigma} e^{-\frac{\zeta^2}{2}} d\zeta
= m.
\end{split}
\end{equation*}
To derive the variance, we start with the following equation dictated by the normalization condition
\begin{equation*}
\int_{-\infty}^{\infty} e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} \sigma ,
\end{equation*}
We differentiate both sides of this equation with respect to $\sigma$ and get
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sigma^3}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sqrt{2 \pi} .
\end{equation*}
A straightforward rearrangement of the terms yields
\begin{equation*}
\int_{-\infty}^{\infty} \frac{(\xi-m)^2}{\sqrt{2 \pi} \sigma}
e^{- \frac{(\xi-m)^2}{2 \sigma^2}} d\xi
= \sigma^2 ,
\end{equation*}
which is the desired result,
Of course, the variance can also be obtained by solving the integral equation directly.
\end{example}


\section{Moment Generating Functions}

The \emph{moment generating function} of a random variable $X$ is defined by
\begin{equation*}
M_X (s) = \mathrm{E} \left[ e^{s X} \right] .
\end{equation*}
This function gets its name from the following property.
If the moment generating function exists in an interval around $s = 0$, then the $n$th momemnt of $X$ is given by
\begin{equation*}
\begin{split}
\frac{d^n}{ds^n} M_X (s) \Big|_{s=0}
&= \frac{d^n}{ds^n} \mathrm{E} \left[ e^{s X} \right] \Big|_{s=0}
= \mathrm{E} \left[ \frac{d^n}{ds^n} e^{s X} \right] \bigg|_{s=0} \\
&= \mathrm{E} \left[ X^n e^{s X} \right] \Big|_{s=0}
= \mathrm{E} [X^n] .
\end{split}
\end{equation*}
In words, if we differentiate the function $M_X(s)$ a total of $n$ times and then evaluate the resulting at zero, we obain the $n$th moment of $X$.
In particular, we have $M_X'(0) = \mathrm{E} [X]$ and $M_X''(0) = \mathrm{E} [X^2]$.

\begin{example}[Exponential Random Variable]
Let $X$ be an exponential random variable with parameter $\lambda$.
The moment-generating function of $X$ is given by
\begin{equation*}
M_X (s) = \int_0^{\infty} \lambda e^{-\lambda \xi} e^{s\xi} d\xi
= \int_0^{\infty} \lambda e^{-(\lambda-s) \xi} d\xi
= \frac{\lambda}{\lambda - s} .
\end{equation*}
The mean of $X$ is
\begin{equation*}
\Expect [X] = \frac{d M_X }{ds} (0)
= \left. \frac{\lambda}{(\lambda - s)^2} \right|_{s=0}
= \frac{1}{\lambda} ;
\end{equation*}
more generally, the $n$th moment of $X$ can be computed as
\begin{equation*}
\Expect [X^n] = \frac{d^n M_X }{ds^n} (0)
= \left. \frac{n! \lambda}{(\lambda - s)^{n+1}} \right|_{s=0}
= \frac{n!}{\lambda^n} .
\end{equation*}
Incidentally, we deduce from these results that the variance of $X$ is equal to $1/\lambda^2$.
\end{example}

\begin{example}[Discrete Uniform Random Variable]
Suppose $X$ is a discrete uniform random variable taking value in $X(\Omega) = \{ 1, 2, \ldots, n \}$.
Then, $p_X(k) = 1/n$ for $1 \leq k \leq n$ and
\begin{equation*}
M_X(s) = \sum_{k = 1}^n \frac{1}{n} e^{sk}
= \frac{1}{n} \sum_{k = 1}^n e^{sk}
= \frac {e^s (e^{ns} - 1)} {n (e^s - 1)} .
\end{equation*}
Using this expression, we can easily get the first two moments of $X$,
\begin{align*}
\Expect [X] &= \frac{d M_X}{ds} (0)
= \lim_{s \rightarrow 0} \frac{1}{n} \sum_{k = 1}^n k e^{sk} = \frac{n + 1}{2} \\
\Expect \left[ X^2 \right] &= \frac{d^2 M_X}{ds^2} (0)
= \lim_{s \rightarrow 0} \frac{1}{n} \sum_{k=1}^n k^2 e^{sk}
= \frac {(n + 1)(2n + 1)}{6} .
\end{align*}
Thus, the mean of $X$ is $(n+1)/2$ and its variance is $(n^2 - 1)/12$.
\end{example}

\begin{example}[Gaussian Random Variable]
Let $U$ be a standard normal random variable whose PDF is given by
\begin{equation*}
f_U (\xi) = \frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2}{2}}.
\end{equation*}
The moment generating function of $U$ is equal to
\begin{equation*}
\begin{split}
M_U (s) &= \Expect \left[ e^{sU} \right]
= \int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2}{2}} e^{s\xi} d\xi \\
&= \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2 + 2 s \xi}{2}} d\xi \\
&= e^{\frac{s^2}{2}} \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{\infty}
\frac{1}{\sqrt{2 \pi}} e^{- \frac{\xi^2 - 2 s \xi + s^2}{2}} d\xi \\
&= e^{\frac{s^2}{2}} \frac{1}{\sqrt{2 \pi}}
\int_{- \infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{- \frac{(\xi - s)^2}{2}} d\xi
= e^{\frac{s^2}{2}} .
\end{split}
\end{equation*}
The last equality follows from the normalization condition and the fact that the integrand is a Gaussian PDF.
\end{example}

Let $M_U(s)$ be the generating function associated with a random variable $U$, and consider the random variable $V = aU + b$ where $a$ and $b$ are constant.
The moment generating function of $V$ can be obtained as follows,
\begin{equation*}
M_V (s) = \Expect \left[ e^{s V} \right]
= \Expect \left[ e^{s (aX + b)} \right]
= e^{sb} \Expect \left[ e^{saX} \right]
= e^{sb} M_X (as) .
\end{equation*}
Thus, if $V$ is an affine function of $U$ then $M_V (s) = e^{sb} M_X (as)$.

\begin{example}
We can use this result of find the moment generating function of a Gaussian random variable with mean $m$ and variance $\sigma^2$.
From above, we know that the moment generating function of a standard normal random variable is $M_U (s) = e^{\frac{s^2}{2}}$.
Furthermore, recall that $V = \sigma U + m$ is a Gaussian random variable with parameter $m$ and $\sigma^2$.
The moment generating function of $V$ then becomes
\begin{equation*}
M_V (s) = e^{sm} M_U (\sigma s) = e^{m s + \frac{\sigma^2 s^2}{2}} .
\end{equation*}
\end{example}

Let $X$ and $Y$ be independent random variables.
Consider the random variable $Z = X + Y$.
The moment generating function of $Z$ is given by
\begin{equation*}
\begin{split}
M_Z (s) &= \mathrm{E} \left[ e^{sZ} \right]
= \mathrm{E} \left[ e^{s(X + Y)} \right] \\
&= \mathrm{E} \left[ e^{sX} e^{sY} \right]
= \mathrm{E} \left[ e^{sX} \right] \mathrm{E} \left[ e^{sY} \right] \\
&= M_X(s) M_Y(s) .
\end{split}
\end{equation*}

\begin{example}[Sum of Gaussian Random Variables]
Suppose $X$ and $Y$ are independent Gaussian varibles.
We denote the mean and variance of $X$ by $m_X$ and $\sigma_X^2$.
Similarly, we represent the mean and variance of $Y$ by $m_Y$ and $\sigma_Y^2$. 
We wish to show that the sum $U = X + Y$ is Gaussian with parameters $m_X + m_Y$ and $\sigma_X^2 + \sigma_Y^2$.

The moment generating functions of $X$ and $Y$ are
\begin{align*}
M_X (s) &= e^{m_X s + \frac{\sigma_X^2 s^2}{2}} \\
M_Y (s) &= e^{m_Y s + \frac{\sigma_Y^2 s^2}{2}}
\end{align*}
The moment generating function of $U = X + Y$ is equal to
\begin{equation*}
M_U (s) = M_X (s) M_Y (s)
= \exp \left( (m_X + m_Y) s + \frac{(\sigma_X^2 + \sigma_Y^2) s^2}{2} \right) ,
\end{equation*}
which implies that $U$ is a Gaussian random variable with mean $m_X + m_Y$ and variance $\sigma_X^2 + \sigma_Y^2$.
\end{example}


\section{Important Inequalities}

There are situations for which computing the exact value of a probability is impossible or impractical.
For these situations, it may be acceptable to use bounds for the value of an elusive probability.


\subsection{The Chebyshev Inequality}

The \emph{Chebyshev inequality} provides a means to compute such bounds. \index{Chebyshev inequality}
Suppose that $g (\cdot)$ is a nonnegative function and let $S$ be a measurable set.
We denote the infimum of $g (\cdot)$ over $S$ by
\begin{equation*}
i_S = \inf_{ x \in S } g (x) .
\end{equation*}
The Chebyshev inequality asserts that
\begin{equation} \label{equation:ChebyshevInequality}
i_S \Pr (X \in S)
\leq \Expect \left[ g(X) \right] .
\end{equation}
This result is remarkably powerful, still it is easy to show.
The definition of $i_S$ and the fact that $g (\cdot)$ is nonnegative imply that
\begin{equation*}
i_S \mathbf{1}_S (x) \leq g(x) \mathbf{1}_S (x) \leq g(x)
\end{equation*}
for any value of $x$.
Thus, for all realizations of $X$, we have $i_X \mathbf{1}_S (X) \leq g(X)$.
Taking the expectactions of these functions with respect to $X$, we get
\begin{equation*}
i_S \Pr (X \in S)
= \Expect \left[ i_S \mathbf{1}_S (X) \right]
\leq \Expect [g (X)] .
\end{equation*}

\begin{example}
Many engineering applications require knowledge of the form $\Pr (X \geq a)$ for some threshold $a > 0$.
We wish to derive an upper bound for $\Pr (X \geq a)$ based solely on the mean of $X$.
Using the Chebyshev inequaltiy, we define $S = [a, \infty)$.
Clearly, the infimum value of $S$ is $\inf_{x \in S} x = a$.
Thus, applying \eqref{equation:ChebyshevInequality}, we obtain
\begin{equation*}
\Pr (X \geq a) \leq \frac{ \Expect [X] }{ a } .
\end{equation*}
\end{example}

We note that the Chebyshev inequality can be applied to both discrete and continuous random variables alike.
Furthermore, in some cases, the inequality can be tight.

\begin{example}
Supose that $0 < b \leq a^2$.
Also, assume that $S = \{ x | |x| \geq a^2 \}$.
Let $X$ be a discrete random variable with PMF
\begin{equation*}
p_X (x) = \left\{ \begin{array}{ll} 1 - \frac{b^2}{a^2}, & x = 0 \\
\frac{b^2}{a^2}, & x = a \\
0, & \text{otherwise.} \end{array} \right.
\end{equation*}
Then, $\Expect \left[ X^2 \right] = b^2$ and $\Pr (X \in S) = b^2 / a^2$.
In this case, the Chebyshev inequality is tight.
\end{example}

\subsection{The Markov Inequality}

The \emph{Markov inequality} is a special case of the Chebyshev inequality that employs that relies on the second moment of a random variable. \index{Markov inequality}
Consider the function  $g(x) = x^2$ and let $S = \{ x | x^2 \geq b \}$ where $b$ is a positive constant.
We wish to find a bound on the probability that $|X|$ exceeds $b$.
Using the Chebyshev inequality, we have $i_S = \inf_{x \in S} x^2 = b^2$ and, consequently, we have
\begin{equation*}
b^2 \Pr (X \in S) \leq \Expect [X^2] .
\end{equation*}
Because $b$ is a positive real number, we can rewrite this equation as
\begin{equation*}
\Pr (|X| \geq b) = \Pr (X \in S) \leq \frac{\Expect [X^2]}{b^2} .
\end{equation*}


\newpage

\begin{example}[The Cantelli Inequality]
Suppose that $X$ is random variable with mean $m$ and variance $\sigma^2$.
We wish to show that
\begin{equation*}
\Pr ( X - m \geq \alpha ) \leq \frac{\sigma^2}{\sigma^2 + \alpha^2},
\end{equation*}
where $\alpha \geq 0$.
\end{example}


\subsection{The Chernoff Bound}

\subsection{The Hoeffding Bound}

See Fine's book page~493.

\subsection{Jensen's Inequality}

Suppose that $g$ is a convex function, that is
\begin{equation*}
p g(x) + (1-p) g(y) \geq g(px + (1-p)y)
\end{equation*}
for all $p \in (0,1)$ and all real numbers $x$ and $y$.
Then, we get
\begin{equation*}
\Expect [g(X)] \geq g \left( \Expect [X] \right) ,
\end{equation*}
provided both expectation exist.


\section*{Further Reading}

\begin{small}\begin{enumerate}\item Ross, S., \emph{A First Course in Probability}, 7th edition, Pearson Prentice Hall,2
006: Sections~5.2, 7.7, 8.2.
\item Bertsekas, D.P., and Tsitsiklis, J.N., \emph{Introduction to Probability}, Athena Sc
ientific, 2002: Section~3.1, 4.1, 7.1.
\end{enumerate}
\end{small}

