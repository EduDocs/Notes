\chapter{Classical Estimation}


From a statistics point of view, \emph{probabilities} are used to predict the outcomes of a random experiment.
For a specific distribution $\mu$ on the observation space $\mathcal{X}$, we can therefore express the probability of set $S \subset \mathcal{X}$ as
\begin{equation*}
\Pr (X \in S) = \mu (S) = \int_S d\mu (x) .
\end{equation*}
Often, in inference problems, the distribution $\mu$ comes from a collection of probability measures parametrized by $\theta$.
Under such circumstances, we can make this dependence on $\theta$ explicit and write
\begin{equation*}
\Pr (X \in S ; \theta) = \mu_{\theta} (S) = \int_S d\mu_{\theta}(x) .
\end{equation*}
This notation, although somewhat cumbersome, will prove useful in posing pertinent inference problems.

When the support of random variable $X$ is a countable set, we can express the probability of individual elements as
\begin{equation*}
\Pr (X = x_i; \theta) = \mu_{\theta} (x_i) = p_{X} (x_i ; \theta) .
\end{equation*}
Random variable $X$ is then said to be a \emph{discrete random variable}.
Furthermore, if $X$ is a \emph{continuous random variable}, then $d\mu_{\theta} (x) = f_X (x ; \theta) dx$, and we recover the familiar equation
\begin{equation*}
\Pr (X \in S ; \theta) = \mu_{\theta} (S) = \int_{S} f_X (x; \theta) dx .
\end{equation*}
In these notes, we reserve the term continuous random variable for scenarios where the cumulative distribution function is differentiable with derivative $f_X (x; \theta)$.

As mentioned above, $\theta$ is an abstract parameter that specifies the probability distribution associated with $X$.
When $\theta$ is known, then so is the governing probability distribution $\mu_{\theta}$.
As such, events such as $\{ X \in S \}$ can be computed unambiguously.
In the special situation where $X$ is discrete, we can think of $\mu_{\theta}$ as a map form $\mathcal{X}$ to $\RealNumbers$,
\begin{equation*}
\mu_{\theta} : x_i \mapsto p_X (x_i; \theta) .
\end{equation*}
Similarly, when $X$ is a continuous random variable, we get
\begin{equation*}
\frac{d \mu_{\theta}}{dx} : x \mapsto f(x; \theta) .
\end{equation*}
Having to resort to probability mass functions and probability density functions may appear overly restrictive at first.
After all, there can be random variables that are neither discrete nor continuous.
However, several results in estimation require strong regularity conditions over collection of parametrized distributions.
It is consequently not too constraining and common in practice to assume that the probability measure $\mu_{\theta}$ corresponds either to a discrete or a continuous random variable.


\section{Statistical Inference}

In many situations, $\theta$ is not known a priori.
Rather, it is the mathematical object we wish to make inferences about.
That is, based on the outcome of a random experiment, we wish to gain information about $\theta$.
This process is especially meaningful when the family of distribution $\{ \mu_{\theta} \}$ is somewhat smooth with respect to parameter  $\theta$.

Given a specific outcome $x$, a map that plays a fundamental role in making inferences about $\theta$ is the \emph{likelihood function} $L(\theta; x)$.
When distributions in $\{ \mu_{\theta} \}$ are discrete, then the likelihood function assumes the form
\begin{equation*}
L(\cdot ; x) : \theta \mapsto p_X(x ; \theta) .
\end{equation*}
When the distribution are continuous, then the likelihood function becomes
\begin{equation*}
L(\cdot ; x) : \theta \mapsto f_X(x ; \theta) .
\end{equation*}
In both cases, these maps are functions from the parameter set $\mathcal{U}$ to the real numbers.
Notice the apparent disconnect between the discrete and continuous cases.
This separation is hard resolved mathematically, as it stems from the distinction between the countable and the uncountable.
Still, these two versions of the likelihood function often play a similar role conceptually.
Keeping this in mind, we seek to develop a unified framework for these two scenarios; details and proof will often be provided for one of the two possible aforementioned cases, with a natural bias for parametrized family of continuous distributions.

While $\mu_{\theta}$ underlies the probability of events under parameter $\theta$ and $L (\theta; x)$ provides information about parameter $\theta$ based on observation $x$, when seen as functions from $\mathcal{U} \times \mathcal{X} \mapsto \RealNumbers$, we get
\begin{equation*}
L(\theta; x_i) = p_X (x_i; \theta) \quad \text{or} \quad L(\theta; x) = f_X (x; \theta) .
\end{equation*}
In other words, likelihoods and probabilities are perspectives on functions over the caratesian product $\mathcal{U} \times \mathcal{X}$.
It is valuable to learn the distrinction between these two terms, as the nomenclature enable a more precise discussion of statistical problems and solutions.
It is also important to understand the relation between likelihoods and probabilities, as proofs may require a change in perspective and a corresponding shift in notation.


\section{Score Function}

The \emph{score function} plays a fundamental role in inference.
It is defined as the gradient of the logarithm of the likelihood function,
\begin{equation*}
\frac{\partial}{\partial \theta} \log L(\theta ; x)
= \frac{1}{L (\theta; x)} \frac{\partial}{\partial \theta} L (\theta; x) .
\end{equation*}
Note that the score is a function of both $\theta$ and observation $x$.
Also, its very definition assumes that the function $L(\theta;, x)$ is smooth enough over $\mathcal{U} \times \mathcal{X}$ for the gradient to exist.
Under proper regularity conditions, the expected value of the score vanishes.
Suppose that $X$ is a continuous random variable with density $f_{X} (x; \theta)$.
Then, we have
\begin{equation*}
\begin{split}
&\mathrm{E}_{\mu_{\theta}} \left[ \frac{\partial \log L}{\partial \theta} (\theta ; X) \right]
= \mathrm{E}_{\mu_{\theta}} \left[ \frac{1}{L (\theta; X)} \frac{\partial L}{\partial \theta} (\theta; X) \right] \\
&= \int \frac{1}{L (\theta; x)} \frac{\partial L}{\partial \theta} (\theta; x) f_{X} (x; \theta) dx
= \int \frac{\partial f_X}{\partial \theta} (x; \theta) dx \\
&= \frac{\partial}{\partial \theta} \int f_X (x; \theta) dx
= \frac{\partial}{\partial \theta} 1 = 0 .
\end{split}
\end{equation*}
We have used the fact that $L (\theta; x) = f_X (x; \theta)$.
The regularity conditions are necessary to interchange the order of integration and differentiation.
An interesting point about this property is the following property: the empirical average of the score function evaluated with true parameter $\theta$ tends to zero as the number of independent samples approaches infinity.


\section{Fisher Information}

A second quantity that is of paramount importance in statistics is the \emph{Fisher Information}.
It is defined as the variance of the score function.
Since the score has mean zero, we have
\begin{equation*}
\begin{split}
I_X (\theta)
&= \mathrm{E}_{\mu_{\theta}} \left[ \left( \frac{\partial \log L}{\partial \theta} (\theta; X) \right)^2 \right] \\
&= \mathrm{E}_{\mu_{\theta}} \left[ \left( \frac{\partial \log f_X}{\partial \theta} (X; \theta) \right)^2 \right] .
\end{split}
\end{equation*}
The expectation above is with respect to $\mu_{\theta}$, the measure on $X$.

When $\log f_X (x; \theta)$ is twice differentiable with respect to $\theta$ and under regularity condition
\begin{equation*}
\int \frac{\partial^2 f_X}{\partial \theta^2} (x; \theta) dx
= \frac{\partial^2}{\partial \theta^2} \int f_X (x; \theta) dx = 0 ,
\end{equation*}
we can rewrite the Fisher information as
\begin{equation} \label{equation:FisherInformation2}
I_X (\theta)
= - \mathrm{E}_{\mu_{\theta}} \left[ \frac{\partial^2 \log f_X}{\partial \theta^2} (X; \theta) \right] .
\end{equation}
In some sense, the Fisher Information is a measure of sharpness.
A smooth curve with shallow maximum will feature a low negative expected second moment, thereby providing little information.
A sharp curve with high peaks will have a high negative expected second derivative, thereby offering more information.

The Fisher information presents two important properties of information measures.
First, for two independent experiments, the combined Fisher information $I_{(X,Y)} (\theta)$ is the sum of the information contained in each individual experiment,
\begin{equation*}
I_{(X,Y)} (\theta) = I_{X} (\theta) + I_{Y} (\theta) .
\end{equation*}
Second, Fisher information is subject to the data processing inequality.
If $T = g(X)$ is a statistic, then $I_T (\theta) \leq I_X (\theta)$.


\subsection{Fisher Information Matrix}

Looking at \eqref{equation:FisherInformation2}, it is possible to extend the concept of Fisher information to vector parameters.
Suppose $\theta = (\theta_1, \ldots, \theta_n)$, the Fisher information matrix of $\theta$ is defined component-wise by
\begin{equation} \label{equation:FisherInformationMatrix}
\left[ I_X (\theta) \right]_{ij}
= - \mathrm{E}_{\mu_{\theta}} \left[ \frac{\partial^2 \log L}{\partial \theta_i \partial \theta_j} (\theta; X) \right] .
\end{equation}
The Fisher information matrix plays an important role in the Cramer-Rao lower bound applied to vector parameters.


\section{Cram\'{e}r-Rao Lower Bound}


First, we recall the Cauchy-Schwarz inequality
\begin{equation*}
\left| \int g h d\mu \right| \leq \| g \|_2 \| h \|_2 .
\end{equation*}
When $\| h \|_2 > 0$, we can use this inequality to provide a lower bound on $\| g \|_2^2$,
\begin{equation} \label{equation:CauchySchwarzUB}
\| g \|_2^2 \geq \frac{ \left| \int g h d\mu \right|^2 }{ \| h \|_2^2} .
\end{equation}
In the present case, we wish to find a lower bound on the variance of an unbiased estimator,
\begin{equation*}
\mathrm{E}_{\mu_{\theta}} \left[ \left( \hat{\theta} (X) - \theta \right)^2 \right]
= \left\| \hat{\theta} (X) - \theta \right\|_2^2 .
\end{equation*}
Thus, naturally, we have $g(x) = \hat{\theta}(x) - \theta$.
The genius of the Cramer-Rao lower bound lies in the selection of function $h (\cdot)$, the score function
\begin{equation*}
h(x) = \frac{\partial \log f_X}{\partial \theta} (x; \theta)
= \frac{1}{f_X (x; \theta)} \frac{\partial f_X}{\partial \theta} (x; \theta) .
\end{equation*}
For this particular choice, we get
\begin{equation*}
\begin{split}
\| h \|_2^2
&= \mathrm{E}_{\mu_{\theta}} \left[ \left( \frac{\partial \log f_X}{\partial \theta} (X; \theta) \right)^2 \right] \\
&= - \mathrm{E}_{\mu_{\theta}} \left[ \frac{\partial^2 \log f}{\partial \theta^2} (X; \theta) \right] ,
\end{split}
\end{equation*}
where the second inequality holds under proper regularity conditions.
We recognize $\| h \|_2^2$ as the Fisher information $I_X(\theta)$.
The last step of the proof is to evaluate the numerator on the right-hand side of \eqref{equation:CauchySchwarzUB},
\begin{equation*}
\begin{split}
\int g h d\mu
&= \int \left( \hat{\theta}(x) - \theta \right)
\frac{1}{f_X (x; \theta)} \frac{\partial f_X}{\partial \theta} (x; \theta)
f_X (x; \theta) dx \\
&= \int \left( \hat{\theta}(x) - \theta \right)
\frac{\partial f_X}{\partial \theta} (x; \theta) dx \\
&= \int \hat{\theta}(x) \frac{\partial f_X}{\partial \theta} (x; \theta) dx
- \theta \int \frac{\partial f_X}{\partial \theta} (x; \theta) dx \\
&= \frac{\partial}{\partial \theta} \int \hat{\theta}(x) f_X (x; \theta) dx
- \theta \frac{\partial}{\partial \theta} \int f_X (x; \theta) dx \\
&= \frac{\partial}{\partial \theta} \theta - \theta \frac{\partial}{\partial \theta} 1
= 1 .
\end{split}
\end{equation*}
Collecting these results, we obtain the scalar version of the Cramer-Rao lower bound,
\begin{equation*}
\mathrm{E}_{\mu_{\theta}} \left[ \left( \hat{\theta} (X) - \theta \right)^2 \right]
\geq \frac{1}{I_X (\theta)} .
\end{equation*}


\begin{example}
Let $\theta$ be an unknown scalar parameter that represent the amplitude of a signal.
Observations about this signal are made in the following fashion,
\begin{equation*}
X_n = \theta + W_n \quad n = 0, 1, \ldots, N-1
\end{equation*}
where $\{ W_n \}$ forms a sequence of independent zero-mean Gaussian random variables, each with variance $\sigma^2$.
We wish to find a lower bound on the variance of an unbiased estimator.
\end{example}


\subsection{Vector Parameters}

In certain problems of statistical inference, it is natural and mathematically desirable to parametrize the set of probability measures $\{ \mu_{\theta} \}$ using a vector, $\theta = (\theta_1, \ldots, \theta_n)$.
Two basic considerations come into play when building the parameter space $\mathcal{U}$.
First, the parametrization should capture the regularity or smoothness of the parameters we wish to estimate.
Second, the objective criterion should be easily expressible in terms of $\theta$.
Keeping this in mind, we extend the Cramer-Rao lower bound to the case where $\theta$ is a vector.
Again, we restrict our analysis to unbiased estimator and we seek to obtain a lower bound on the variance of every component in $\theta = (\theta_1, \ldots, \theta_2)$.

\begin{theorem}
Let $\{ f_X (x; \theta) \}$ be a collection of continuous probability distributions indexed by vector $\theta = (\theta_1, \ldots, \theta_n) \in \mathcal{U}$.
Under suitable regularity conditions, the variance of any unbiased estimator $\hat{\theta} (\cdot)$ must satisfy
\begin{equation*}
\mathrm{E}_{\mu_{\theta}} \left[ \left( \hat{\theta}_i (X) - \theta \right)^2 \right]
\geq \left[ I_X^{-1} (\theta) \right]_{ii}
\end{equation*}
where $I_X(\theta)$ is the Fisher information matrix associated with vector $\theta$.
\end{theorem}


\newpage
$Z = X + Y$
\begin{equation*}
\begin{split}
I_Z (\theta)
&= \mathrm{E}_{\mu_{\theta}} \left[ \left( \frac{\partial \log f_Z}{\partial \theta} (Z; \theta) \right)^2 \right] \\
&= \int_{\RealNumbers} \left( \frac{1}{f_Z (z; \theta)} \frac{\partial f_Z}{\partial \theta} (z; \theta) \right)^2 f_Z (z; \theta) dz
\end{split}
\end{equation*}

\begin{equation*}
f_Z (z; \theta) = \int_{\RealNumbers} f_X (\xi; \theta) f_Y (z - \xi; \theta) d\xi
\end{equation*}

\begin{theorem}
Let $X$ and $Y$ be independent continuous random variables.
Assume that $f_X(\cdot; \theta)$ and $f_Y(\cdot; \theta)$ are differential probability density functions that are strictly positive over $\RealNumbers$.
\end{theorem}

\begin{equation*}
\begin{split}
\frac{\partial}{\partial \theta} f_Z (z; \theta)
&= \frac{\partial}{\partial \theta} \int_{\RealNumbers} f_X (\xi; \theta) f_Y (z - \xi; \theta) d\xi \\
&= \int_{\RealNumbers} \frac{\partial}{\partial \theta}
\left( f_X (\xi; \theta) f_Y (z - \xi; \theta) \right) d\xi \\
&= \int_{\RealNumbers}
\frac{\partial f_X}{\partial \theta} (\xi; \theta) f_Y (z - \xi; \theta)
+ f_X (\xi; \theta) \frac{\partial f_Y}{\partial \theta} (z - \xi; \theta) d\xi \\
\end{split}
\end{equation*}


\newpage

\subsection{Probability and Information}

Consider a sample space $(\Omega, \mathcal{F})$ with a given probability measure.
We wish to develop a function $i(\cdot)$ that captures the amount of information contained in an event $E \in \mathcal{F}$.
This function should operate on $\Pr(E)$, and it should be additive over independent events.
Specifically, if $E_1$ and $E_2$ are independent, then
\begin{equation} \label{equation:InformationAdditivity}
i \left( \Pr (E_1 \cap E_2) \right)
= i \left( \Pr (E_1) \right) + i \left( \Pr (E_2) \right) .
\end{equation}
Recall that, for independent events, $\Pr(E_1 \cap E_2)$ is equal to the product of $\Pr(E_1)$ and $\Pr(E_2)$,
\begin{equation} \label{equation:IndependentEventsAdditiveProbabilities}
\Pr (E_1 \cap E_2) = \Pr (E_1) \Pr (E_1) .
\end{equation}
Together, \eqref{equation:InformationAdditivity} and \eqref{equation:IndependentEventsAdditiveProbabilities} demand that $i(\cdot)$ be a homomorphism between semigroup $((0,1], \times)$ and semigroup $( [0, \infty), +)$. 
This single requirement drastically narrows the potential candidates for $i(\cdot)$.
We stress that these two semigroups are embedded, respectively, in the multiplicative group of positive real numbers and the additive group of real numbers.
It is well-known that logarithmic functions are the only continuous isomorphisms from $(\RealNumbers^+, \times)$ to $(\RealNumbers, +)$.
It then appears that functions of the form
\begin{equation*}
i \left( \Pr (E) \right) = a \log_b \left( \Pr (E) \right)
\end{equation*}
arise naturally as possible information measures for events.
Multiplicative constant $a$ and base $b$ of the logarithm act as arbitrary scaling factors,
\begin{equation*}
a \log_b ( x ) = a \frac{ \log ( x ) }{ \log ( b) }
= \frac{a}{ \log (b) } \log ( x )
\end{equation*}
By taking $a = -1$ and $b > 1$, we get positive values for the amount of information associated with non-trivial events.

The objective of this brief discussion is to provide partial insight on the role of logarithmic functions as measures of information.
One subtlety remains in providing a definition for $i(\cdot)$, the case where $\Pr (E) = 0$.
This scenario is handled by stating that zero-probability events contain no information.

\begin{definition}[Probabilistic Information] \label{definition:ProbabilisticInformation}
Let $(\Omega, \mathcal{F}, P)$ be a probability space.
The probabilisitc information of event $E \in \mathcal{F}$ is equal to $i(\Pr(E))$, where function $i: [0, \infty) \mapsto \RealNumbers$ is defined by
\begin{equation} \label{equation:InformationFunction}
i(s) = \begin{cases} - \log s, & s > 0 \\
0, & s = 0 . \end{cases}
\end{equation}
\end{definition}

It is straightforward to generalized the concept of probabilistic information from probability spaces to discrete and continuous random variables.


\subsection{Information Mass Function}

Suppose $X$ is a discrete random variable on $(\Omega, \mathcal{F}, P)$.
The information mass function of $X$ is a function that measure the information content associated with the possible values of $X$.
For any $x_k \in X(\Omega)$, we have
\begin{equation*}
i_X (x_k) = i \left( \Pr (X = x_k) \right) = i \left( \Pr \left( \{ \omega : X(\omega) = x_k \} \right) \right) .
\end{equation*}
For all other elements in the codomain of $X$, we have $\Pr (X = x) = 0$ and hence $i_X (x) = i (0) = 0$.

\begin{definition}[IMF]
Let $X$ be a discrete random variable with probability mass function $p_X (\cdot)$.
The information of $X$ is defined by
\begin{equation*}
i_X (x_k) = i \left( p_X (x_k) \right) = - \log p_X (x_k),
\end{equation*}
where $x_k \in X(\Omega)$.
More generally, we have $i_X (x) = i \left( p_X (x) \right)$.
\end{definition}

Based on this definition, we can easily express several quantities found in information theory
Let $\mu$ be the probability distribution on the codomain of $X$ induced by measure $P$.
The entropy of discrete random variable $X$ becomes
\begin{equation*}
H [X] = \mathrm{E}_{\mu} \left[ i_X (X) \right] = - \sum_{x \in X(\Omega)} p_X (x) \log p_X (x) .
\end{equation*}
The Kullback-Leibler divergence of $Y$ from $X$ can be expressed as
\begin{equation*}
\begin{split}
D [X \| Y] &= \mathrm{E}_{\mu} \left[ i_Y (X) - i_X (X) \right] \\
&= \sum_{x \in X(\Omega)} p_X (x) \log \frac{p_X (x)}{p_Y (x)} .
\end{split}
\end{equation*}
Finally, if $\upsilon$ denotes the probability distribution on the codomain of $X \times Y$ induced by measure $P$, then the mutual information of discrete random variables $X$ and $Y$ reduces to
\begin{equation*}
\begin{split}
I [X ; Y] &= \mathrm{E}_{\upsilon} \left[ i_X (X) + i_Y (Y) - i_{(X,Y)} (X,Y) \right] \\
&= \sum_{x \in X(\Omega)} \sum_{y \in Y(\Omega)}
p_{X,Y} (x, y) \log \frac{p_{X,Y} (x, y)}{p_X (x) p_Y (y)} .
\end{split}
\end{equation*}
Of course, these quantities are inter-related, and they discussed at length in the literature.
Still, a probabilistic information viewpoint provides intuition as to why the logarithmic function unifies these various criteria.


\subsection{Information Density Function}

Extending the concept of information to continuous random variables is a straightforward task.

\begin{definition}[IDF]
Let $X$ be a continuous random variable with density $f_X (\cdot)$.
The information density function of $X$ is defined by
\begin{equation*}
i_X (x) = i \left( f_X (x) \right) .
\end{equation*}
\end{definition}

This definition can be motivated by a Riemann-type asymptotic argument.
Suppose $X$ is a real-valued random variable with continuous density $f_X (\cdot)$.
Let $\delta > 0$ be fixed.
By the mean-value theorem, there exists $x_k \in [ k \delta , (k+1) \delta )$ such that
\begin{equation*}
f_X (x_k) \delta = \int_{k \delta}^{(k+1) \delta} f_X (\xi) d\xi .
\end{equation*}
Denote by $X_{\delta}$ the discrete random variable created by choosing one $x_k$ for every interval $[ k \delta , (k+1) \delta )$, and subsequently defining the probability mass function of $X_{\delta}$ by
\begin{equation*}
p_{X_{\delta}} (x_k) = \Pr (X_{\delta} = x_k) = f_X (x_k) \delta .
\end{equation*}
The entropy associated with $X_{\delta}$ is given by
\begin{equation*}
\begin{split}
&- \sum_{x_k} p_{X_{\delta}} (x_k) i_{X_{\delta}} (x_k)
= - \sum_{x_k} f_X (x_k) \delta i \left( \delta f_X (x_k) \right) \\
&= - \delta \log \delta
- \sum_{x_k} f_X (x_k) \delta i \left( f_X (x_k) \right) .
\end{split}
\end{equation*}
Taking limits, as $\delta \rightarrow 0$, we recover the standard differential entropy
\begin{equation*}
H [X] = \mathrm{E}_{\mu} \left[ i_X (X) \right]
= \mathrm{E}_{\mu} \left[ i (f_X(X)) \right] .
\end{equation*}
We note that, using this definition of information density, the Kullback-Leibler divergence of $Y$ from $X$ has exactly the same form in the continuous case as it does in the discrete setting,
\begin{equation*}
D[X \| Y] = \mathrm{E}_{\mu} \left[ i_Y(X) - i_X (X) \right] .
%= \int_{X(\Omega)} f_X (x) \log \frac{ f_X(x) }{ f_Y (x) } dx
\end{equation*}


\section{Information and Geometry}

The entropy provides a measure of uncertainty associated with a random variable, and it is obtained by taking expectation.
This quantity and its derivates are very pertinent in the contexts of data compression and reliable communication.
Still, the entropy of random variable $X$ fails to capture the geometry of space associated with the codomain of $X$.
If $X$ is discrete, then its entropy can be computed based solely on $\{ p_X (\cdot) \}$;
it does not dependend on the labels $\{ x_k \}$.
In particular, assume that $X$ takes on value in $\{ 1, \ldots, n \}$ and let $\sigma (\cdot)$ be a permutation on these same integers; then $H [X] = H [\sigma(X)]$.
More generally, if $X$ is a discrete random variable and $g(\cdot)$ is an injective function, then $H[X] = H[g(X)]$.
Clearly, the entropy is completely disconnected from $X(\Omega)$.

A similar observation can be made about continuous random variables, although it is harder to characterize mathematically.
Let $\lambda$ denote the Lebesgue measure on $\RealNumbers$.
If two probability density functions $f_X (\cdot)$ and $f_Y (\cdot)$ are such that
\begin{equation*}
\lambda ( \{ x : f_X (x) > \tau \} )
= \lambda ( \{ y : f_X (y) > \tau \} )
\end{equation*}
for all $\tau \geq 0$, then the random variables $X$ and $Y$ have the same differential entropy.
In particular, the differential entropy is invariant to translations and reflections.

One way to relate the information of random variable $X$ to its codomain is through variations.
Suppose that $i_X (\cdot)$ is the information density of a continuous random variable.
If $i_X (\cdot)$ is diffentiable, then we can define the score of $X$ as follows.

\begin{definition}[Score] \label{definition:ScoreX}
Let $X$ be a continuous random variable with information density function $i_X (\cdot)$.
If $i_X(\cdot)$ is differentiable at point $x \in X(\Omega)$, then the score of $X$ is defined as the derivative of $i_X(\cdot)$ evaluated at $x$.
If, in addition, $f_X (x) > 0$, then the score becomes
\begin{equation} \label{equation:ScoreX}
V_X (x) = \frac{d i_X}{dx} (x) = \frac{d \log f_X}{dx} (x) = \frac{1}{f_X (x)} \frac{d f_X}{dx} (x) .
\end{equation}
\end{definition}

When the score of $X$ exists almost everywhere, is can be employed to analyze properties of the information density.
For example, we can compute the variation of $i_X (\cdot)$ as
\begin{equation*}
V (f_X) = \int_{\RealNumbers} \left| \frac{d i_X}{dx} (x) \right| dx
\end{equation*}
or its weighted variation
\begin{equation*}
V_{\mu} (f_X) = \mathrm{E}_{\mu} \left[ \left| \frac{d i_X}{dx} (X) \right| \right] .
\end{equation*}
A close look at this second expression reveals that $V_{\mu} (f_X)$ is the $L_1$-norm of the score function with respect to $\mu$.
Not so surprisingly, the $L_2$-norm of the score function also plays a fundamental role in statistics and information theory.
It forms a basis for the paramount Fisher information.

\begin{definition}[Fisher information] \label{definition:FisherInformationX}
Let $X$ be a continuous random variable with differentiable density $i_X (\cdot)$.
The Fisher information of $X$ is defined by
\begin{equation} \label{equation:FisherInformationX}
\begin{split}
J_X &= \mathrm{E}_{\mu} \left[ \left( \frac{d i_X}{dx} (X) \right)^2 \right]
= \mathrm{E}_{\mu} \left[ \left( \frac{d \log f_X}{dx} (X) \right)^2 \right] \\
&= \mathrm{E}_{\mu} \left[ \left( \frac{1}{f_X (x)} \frac{d f_X}{dx} (X) \right)^2 \right] .
\end{split}
\end{equation}
Alternatively, we can write $J_X = \mathrm{E}_{\mu} \left[ \left| V_X (X) \right|^2 \right]$, which clearly exposes the relation between the Fisher information and the $L_2$-norm of $V_X (\cdot)$ with respect to distribution $\mu$.
\end{definition}



\newpage

\section{Questions}

\begin{enumerate}

\item Consider the probabilistic information of Definition~\ref{definition:ProbabilisticInformation}.
\begin{enumerate}
\item Show that the information function defined in \eqref{equation:InformationFunction} is additive.
\item Let $X$ and $Y$ be independent discrete random variables with probability mass functions $p_X(\cdot)$ and $p_Y (\cdot)$, respectively.
Let $\upsilon$ be the probability distribution on $X(\Omega) \times Y(\Omega)$ induced by $(X,Y)$.
Using part (a), prove that $H[(X,Y)] = H [X] + H[Y]$. 
\item Again, let $X$ and $Y$ be discrete random variables with probability mass functions $p_X(\cdot)$ and $p_Y (\cdot)$, respectively.
Show that
\begin{equation*}
\mathrm{E}_{\mu} [i_Y (X)] \geq \mathrm{E}_{\mu} [i_X (X)]
\end{equation*}
where $\mu$ is the probability distribution on $X(\Omega)$ induced by $X$.
\end{enumerate}

\item In this question, we consider the score function defined in \eqref{equation:ScoreX} and the Fisher information of \eqref{equation:FisherInformationX}.
Suppose $X$ and $Y$ are independent continuous random variables with differentiable density.
Futhermore, assume that $f_X(\cdot)$ and $f_Y(\cdot)$ are non-zero everywhere.
\begin{enumerate}
\item Show that the expected value of the score function of $X$ is equal to zero, $\mathrm{E}_{\mu} [V_X(X)] = 0$, where $\mu$ is the distribution induced by $X$.
\item Let $Z = X + Y$ and recall that
\begin{equation*}
f_Z (z) = \int_{\RealNumbers} f_X (\xi) f_Y (z -\xi) d\xi .
\end{equation*}
Under the aforementioned assumptions, show that for a specific $z$
\begin{equation*}
\tilde{f}_z (\xi) = \frac{f_X (\xi) f_Y (z -\xi)}{f_Z(z)}
\end{equation*}
is a probability density function; moreover, $\tilde{f}_z (\xi) = f_{X | Z} (x | z)$.
\item Show that
\begin{equation*}
V_Z (z) = \frac{\frac{d f_Z}{dz}(z)}{f_Z(z)}
= \mathrm{E} \left[ \left. \frac{\frac{d f_X}{dx}(X)}{f_X(X)} \right| Z = z \right]
= \mathrm{E} \left[ V_X(X) | Z = z \right] ,
\end{equation*}
where the expectation is with respect to $f_{X|Z} (\cdot|z)$.
\item By symmetry, we can argue from part (b) that
\begin{equation*}
V_Z(z) = \mathrm{E} \left[ V_Y(Y) | Z = z \right] .
\end{equation*}
Collecting these two results, we can then write
\begin{equation*}
\mathrm{E} \left[ a V_X(X) + b V_Y(Y) | Z = z \right] = (a + b) V_Z (z) .
\end{equation*}
Note that the conditional probability measure above does not admit a conditional density.
Using this fact and nested conditional expectation, show that
\begin{equation*}
(a + b)^2 J_Z \leq a^2 J_X + b^2 J_Y .
\end{equation*}
\item Pick values for $a$ and $b$ to show that
\begin{equation*}
\frac{1}{J_Z} \geq \frac{1}{J_X} + \frac{1}{J_Y} .
\end{equation*}
\end{enumerate}


\item Consider two probability mass functions, $p_X (\cdot)$ and $p_Y (\cdot)$, defined over the integers $\{ 1, \ldots, m \}$.
Assume that these PMFs are strictly positive over their shared domain.
We observe a sequence of independent and identically distributed random variable $Z_1, Z_2, \ldots$ and we wish to determine whether elements in this sequence are generated using $p_X (\cdot)$ or $p_Y (\cdot)$ (the only two possibilities).
For convenience, we assume that
\begin{equation*}
\mathrm{E}_{p_X} [Z] < \tau < \mathrm{E}_{p_Y} [Z] .
\end{equation*}
We restrict our attention to log-likelihood ratio tests where the empirical average
\begin{equation} \label{equation:AverageLogLikelihoodRatio}
A_n = \frac{1}{n} \sum_{k=1}^n \log \frac{p_Y (Z_k)}{p_X (Z_k)} 
\end{equation}
is compared to threshold value $\tau$.

\begin{enumerate}
\item Express the log-likelihood ratio of \eqref{equation:AverageLogLikelihoodRatio} in terms of the information mass functions $i_X (\cdot)$ and $i_Y (\cdot)$.
\item Argue that the empirical average \eqref{equation:AverageLogLikelihoodRatio} converges in probability to a constant.
Find the value of this constant when the underlying distribution is $p_X(\cdot)$.
\item For the rest of this problem, let $p_X (\cdot)$ be a binomial distribution with parameter $(p, 4)$ where $p < 0.5$; and let $p_Y(\cdot)$ be a binomial distribution with parameter $(1-p, 4)$.
Find the distribution of $n A_n$ under $p_X (\cdot)$.
\item Derive a Chernoff bound for $\Pr (A_n > \tau)$ given that the underlying probability is $p_X (\cdot)$.
\item Find the large deviation principle governing the sequence.
\end{enumerate}

\end{enumerate}


The notions of information discussed so far capture the uncertainty associated with random
only capture the are based on 

In the discrete case, the the information mass function is 


\begin{equation*}
\mathrm{E}_{\mu} \left[ \log \frac{p_X(k)}{p_Y(k)} \right]
\end{equation*}

\begin{itemize}
\item Stein's lemma
\item inner product and Fisher information
\end{itemize}
