\chapter{Principles of Data Reduction}

A function $T(\cdot)$ on $\mathbf{X}$ defines a form of processing or data reduction.
An experimenter who uses the statistic $T(\mathbf{x})$ rather than the raw observation $\mathbf{x}$, will treat as equal two samples $\mathbf{x}$ and $\mathbf{y}$ that satisfy $T(\mathbf{x}) = T(\mathbf{y})$, even though the actual sample values may be different in some ways.


In this chapter, we are especially interested in methods of data redution that do not discard important information about the unknown parameter $\theta$, and yet successfully discard irrelevant information as far as gaining knowledge about $\theta$ is concrened.


\section{The Sufficiency Principle}


 A \emph{sufficient statistic} for a parameter $\theta$ is a statistic that captures all the information about $\theta$ contained in the sample.

\begin{definition}[Sufficient Statistics]
A statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$.
\end{definition}

To use this definition to verify that a statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$, we must verify that for any fixed values of $\mathbf{x}$ and $t$, the conditional probability $P_{\theta} (\mathbf{X} = \mathbf{x} | T (\mathbf{X}) = t)$ is the same for all values of $\theta$.
We note that this probability is zero for all values of $\theta$ if $T(\mathbf{x}) \neq t$.
So, we must verify only that $P_{\theta} (\mathbf{X} = \mathbf{x} | T (\mathbf{X}) = T(\mathbf{x}))$ does not depend on $\theta$.
Since $\{ \mathbf{X} = \mathbf{x} \}$ is a subset of $\{ T(\mathbf{X}) = T(\mathbf{x}) \}$,
\begin{equation*}
\begin{split}
P_{\theta} (\mathbf{X} = \mathbf{x} | T (\mathbf{X}) = T(\mathbf{x}))
&= \frac{ P_{\theta} (\mathbf{X} = \mathbf{x}, T (\mathbf{X}) = T(\mathbf{x})) }{ P_{\theta} (T (\mathbf{X}) = T(\mathbf{x})) } \\
&= \frac{ P_{\theta} (\mathbf{X} = \mathbf{x}) }{ P_{\theta} (T (\mathbf{X}) = T(\mathbf{x})) } \\
&= \frac{ p (\mathbf{x} | \theta) }{ q(T (\mathbf{x}) | \theta) }
\end{split}
\end{equation*}
where $p(\mathbf{x}|\theta)$ is the joint PMF of the sample $\mathbf{X}$ and $q(t|\theta)$ is the PMF of $T(\mathbf{X})$.
Thus, $T(\mathbf{X})$ is a sufficient statistics for $\theta$ is and only if, for every $\mathbf{x}$, the above ratio of PMFs is constant as a function of $\theta$.

\begin{theorem}
If $p(\mathbf{x}|\theta)$ is the joint PDF or PMF of $\mathbf{X}$ and $q(t|\theta)$ is the PDF or PMF of $T(\mathbf{X})$, then $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if, for every $\mathbf{x}$ in the sample space, the ratio $p(\mathbf{x}|\theta)/q(T(\mathbf{X})|\theta)$ is constant as a function of $\theta$.
\end{theorem}

It may be difficult to use the definition of a sufficient statistic to find a sufficient statistic for a particular model.
To use the definition, we must guess a statistic $T(\mathbf{X})$ to be sufficient, find the PDF or PDF of $T(\mathbf{X})$, and check that the ratio of PDFs or PMFs does not depend on $\theta$.
The first step requires a good deal of intuition and the second sometimes requires some tedious analysis.
Fortunately, it may be possible is some cases to find a sufficent statistic by simple inspection.

\begin{theorem}
Let $f(\mathbf{x}|\theta)$ denote the joint PDF or PMF of a sample $\mathbf{X}$.
A statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if and only if there exist functions $g(t|\theta)$ and $h(\mathbf{x})$ such that, for all sample points $\mathbf{x}$ and all parameter points $\theta$,
\begin{equation*}
f(\mathbf{x}|\theta) = g(T(\mathbf{x})|\theta) h(\mathbf{x}) .
\end{equation*}
\end{theorem}
\begin{proof}
Suppose $T(\mathbf{X})$ is a sufficient statistic.
Choose $g(t|\theta) = P_{\theta} (T(\mathbf{X}) = t)$ and $h(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} | T(\mathbf{X}) = T(\mathbf{x}))$.
Because $T(\mathbf{X})$ is sufficient, the conditional probability defining $g(\mathbf{x})$ does not depend on $\theta$.
Thus, this choice of $h(\mathbf{x})$ and $g(t|\theta)$ is legitimate, and for this choice, we have
\begin{equation*}
\begin{split}
f(\mathbf{x}|\theta) &= P_{\theta} (\mathbf{X} = \mathbf{x}) \\
&= P_{\theta} (\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = T(\mathbf{x})) \\
&= P_{\theta} (T(\mathbf{X}) = T(\mathbf{x}))
P (\mathbf{X} = \mathbf{x} | T(\mathbf{X}) = T(\mathbf{x})) \\
&= g(T(\mathbf{x})|\theta) h(\mathbf{x}) .
\end{split}
\end{equation*}
So, the desired factorization has been exhibited.
We also see from the last two lines above that
\begin{equation*}
P_{\theta} (T(\mathbf{X}) = T(\mathbf{x}))
= g(T(\mathbf{x})|\theta) h(\mathbf{x}) .
\end{equation*}
That is, $g(T(\mathbf{x}|\theta) h(\mathbf{x})$ is the PMF of $T(\mathbf{X})$.

Now, assume that the factorization exists.
Let $q(t|\theta)$ be the PMF of $T(\mathbf{X})$.
To show that $T(\mathbf{X})$ is sufficient, we examine the ratio $f(\mathbf{x}|\theta)/q(T(\mathbf{X})|\theta)$.
Define $A_{T(\mathbf{x})} = \{ \mathbf{y} : T(\mathbf{y}) = T(\mathbf{x}) \}$.
Then
\begin{equation*}
\begin{split}
\frac{ f(\mathbf{x}|\theta) }{ q(T(\mathbf{x})|\theta) }
&= \frac{ g(T(\mathbf{x})|\theta) h(\mathbf{x}) }{ q(T(\mathbf{x})|\theta) } \\
&= \frac{ g(T(\mathbf{x})|\theta) h(\mathbf{x}) }{ \sum_{A_{T(\mathbf{x})}} g(T(\mathbf{y})|\theta) h(\mathbf{y}) } \\
&= \frac{ g(T(\mathbf{x})|\theta) h(\mathbf{x}) }{ g(T(\mathbf{x})|\theta) \sum_{A_{T(\mathbf{x})}} h(\mathbf{y}) } \\
&= \frac{ h(\mathbf{x}) }{ \sum_{A_{T(\mathbf{x})}} h(\mathbf{y}) } .
\end{split}
\end{equation*}
Since the ratio does not depend on $\theta$, we conclude that $T(\mathbf{X})$ is a sufficient statistic for $\theta$.
\end{proof}

