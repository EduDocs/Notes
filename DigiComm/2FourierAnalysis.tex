\chapter{Fourier Analysis and Sampling}
\label{chapter:FourierAnalysisSampling}

\emph{Fourier analysis} refers to a collection of tools that can be applied to express a function in terms of complex sinusoids, called basis elements, of different frequencies.
The result of the decomposition is the amplitude and the phase to be imparted to each basis element in the reconstruction.
This decomposition is termed the \emph{frequency domain representation} of the original signal.

Fourier analysis is extremely useful in engineering, with a myriad of applications.
Part of its appeal lies in the fact that basis elements are characteristic functions of linear time-invariant systems.
This property, which may seem nebulous at this point, is instrumental in solving many challenging problems, and makes Fourier analysis a powerful methodology for the design of communication systems.
We assume that the reader is familiar with basic Fourier analysis, and only review details that are pertinent to our treatment of communication systems.
This is not intended to be a comprehensive treatment of harmonic analysis.


\section{Fourier Series}
\label{section:FourierSeries}

Fourier series can be employed to express, as weighted sums of sinusoidal components, either periodic functions or functions that are time-limited.
Suppose that $s(t)$ is a function that is nonzero only for $-\frac{T}{2} \leq t \leq \frac{T}{2}$, and is square integrable
\begin{equation*}
\| s(t) \|^2 = \int_{\mathbb{R}} |s(t)|^2 dt
= \int_{-\frac{T}{2}}^{\frac{T}{2}} |s(t)|^2 dt < \infty .
\end{equation*}
Then $s(t)$ possesses a \emph{Fourier series representation}, which is defined by
\begin{equation} \label{equation:FourierSeries1}
s(t) = \begin{cases} \sum_{k=-\infty}^{\infty}
\hat{s}_k e^{2 \pi i \frac{k}{T} t}, & |t| \leq \frac{T}{2} \\
0, & \text{otherwise} \end{cases}
\end{equation}
where the Fourier series coefficients $\{ \hat{s}_k : k \in \mathbb{Z} \}$ are given by
\begin{equation*}
\hat{s}_k = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}
s(t) e^{-2 \pi i \frac{k}{T} t} dt .
\end{equation*}
We can use the standard rectangular function $\mathrm{rect}(\cdot)$, defined by
\begin{equation} \label{equation:RectangularFunction}
\mathrm{rect} (t) = \begin{cases} 1, & |t| < 0.5 \\
0, & \text{otherwise} \end{cases}
\end{equation}
to simplify \eqref{equation:FourierSeries1}, and rewrite the Fourier representation of $s(t)$ as
\begin{equation} \label{equation:FourierSeries2}
s(t) = \sum_{k=-\infty}^{\infty}
\hat{s}_k e^{2 \pi i \frac{k}{T} t}
\mathrm{rect} \left( \frac{t}{T} \right) .
\end{equation}
From a vector space perspective, \eqref{equation:FourierSeries2} asserts that $s(t)$ can be expressed as a linear combination of basis elements $\{ \theta_k (t) : k \in \mathbb{Z} \}$, where
\begin{equation*}
\theta_k (t) = e^{2 \pi i \frac{k}{T} t} \mathrm{rect} \left(\frac{t}{T} \right) .
\end{equation*}
Furthermore, note that the collection of functions $\{ \theta_k (t) : k \in \mathbb{Z} \}$ forms an orthogonal set under the standard inner product; that is,
\begin{equation*}
\begin{split}
\left\langle \theta_k(t), \theta_n(t) \right\rangle
&= \int_{-\infty}^{\infty} \theta_k(t) \theta_n^*(t) dt
= \int_{-\frac{T}{2}}^{\frac{T}{2}} e^{2 \pi i \frac{k}{T} t}
e^{- 2 \pi i \frac{n}{T} t} dt \\
&= \int_{-\frac{T}{2}}^{\frac{T}{2}} e^{2 \pi i \frac{(k-n)}{T} t} dt
= 0
\end{split}
\end{equation*}
for all $k \neq n$.
An interesting and important aspect of Fourier series is that time-limited functions can be characterized using a discrete set of coefficients.
This fact provides insight into the sampling theorem, which we will review shortly.


\section{Fourier Transforms}

The \emph{Fourier transform} applies to functions that are not necessarily time-limited.
Assume that $x(t)$ is a signal that is square integrable,
\begin{equation} \label{equation:L2Condition}
\| x(t) \|^2 = \int_{\mathbb{R}} | x(t) |^2 dt < \infty .
\end{equation}
Then, we can express $x(t)$ using its frequency domain representation.
The Fourier transform of $x(t)$, which we denote by $\hat{x}(f)$ or $\mathcal{F} [x(t)]$, is defined by
\begin{equation} \label{equation:FourierTransform}
\hat{x}(f) = \mathcal{F} [x(t)]
= \int_{\mathbb{R}} x(t) e^{-2 \pi i f t} dt .
\end{equation}
The original function can subsequently be expressed in terms of its decomposition,
\begin{equation} \label{equation:InverseFourierTransform}
x(t) = \int_{\mathbb{R}} \hat{x}(f) e^{2 \pi i f t} df .
\end{equation}
We sometimes denote the inverse Fourier transform of $\hat{x}(f)$ as $\mathcal{F}^{-1} [\hat{x}(f)]$.
It is interesting to point out the duality between the Fourier transform and its inverse, $\mathcal{F} \left[ \hat{x} (t) \right] = x(-f)$.
This relation is rooted in the striking similarity between \eqref{equation:FourierTransform} and \eqref{equation:InverseFourierTransform}.

\begin{example}[Rectangular Pulse]
The rectangular pulse $\mathrm{rect} (\cdot)$, defined in \eqref{equation:RectangularFunction}, can be used to constrain various signals in time or frequency.
Note that $\| \mathrm{rect} (t) \|^2 = 1 < \infty$, which guarantees that Fourier analysis can be applied to this function.
The Fourier transform of $\mathrm{rect} (t)$ can be computed as follows,
\begin{equation*}
\begin{split}
\mathcal{F} \left[ \mathrm{rect} (t) \right]
&= \int_{\mathbb{R}} \mathrm{rect} (t) e^{- 2 \pi i f t} dt
= \int_{-\frac{1}{2}}^{\frac{1}{2}} e^{- 2 \pi i f t} dt \\
&= \frac{1}{\pi f} \left( \frac{e^{\pi i f} - e^{- \pi i f}}{2i} \right)
= \frac{ \sin \pi f }{\pi f} \\
&= \mathrm{sinc}(f) .
\end{split}
\end{equation*}
Thus, the Fourier transform of $\mathrm{rect}(\cdot)$ is the famous $\mathrm{sinc}(\cdot)$ function, which plays a fundamental role in the sampling and reconstruction of information signals.
\end{example}

When condition~\eqref{equation:L2Condition} is not satisfied, it may be hazardous to use Fourier analysis and frequency domain representations.
Strictly speaking, the Fourier transform of a function may not exist if it behaves wildly.
Casually taking the Fourier transforms of arbitrary signals should be avoided.
Having said that, there will be instances where we discuss the Fourier transforms of functions that do not fulfill \eqref{equation:L2Condition};
one such example appears below.
We adopt this somewhat cavalier attitude because experience allows us to avoid pitfalls, and Fourier relaxation leads to great engineering insight.
The downside of this approach is that the reader is left with the burden of deciding whether a signal has a proper spectral representation, or if the definition of the Fourier transform is being applied loosely.

Starting with signal $x(t)$, we can write
\begin{equation} \label{equation:NestedFourierRepresentation}
\begin{split}
x(t) &= \int_{\mathbb{R}} \hat{x}(f) e^{2 \pi i f t} df
= \int_{\mathbb{R}} \left[ \int_{\mathbb{R}} x(\tau) e^{-2 \pi i f \tau} d\tau \right] e^{2 \pi i f t} df \\
&= \int_{\mathbb{R}} \left[ \int_{\mathbb{R}} e^{2 \pi i f (t - \tau)} df \right] x(\tau) d\tau ,
\end{split}
\end{equation}
where the second equality follows from \eqref{equation:FourierTransform} and the third equality is obtained by changing the order of integration.
Recall that we can use a $\delta$-function to write (informally)
\begin{equation*}
x(t) = \int_{\mathbb{R}} \delta (t - \tau) x(\tau) d\tau .
\end{equation*}
Since \eqref{equation:NestedFourierRepresentation} holds for any time~$t$, we gather that
\begin{equation*}
\delta (t) = \int_{\mathbb{R}} e^{2 \pi i f t} df ,
\end{equation*}
and hence the (cavalier) Fourier transform of the $\delta$-function is $\mathcal{F} [ \delta(t) ] = 1$.


\subsection{Periodic Signals}
\label{section:PeriodicSignals}

We can develop (cavalier) Fourier transform representations for periodic signals as well, thereby providing a unified treatment of periodic and aperiodic functions. 
Indeed, we can construct the Fourier transform of a periodic signal directly from its Fourier series representation.
Let $x(t)$ be a signal with Fourier transform $\hat{x}(f) = \delta (f - f_0)$.
To recover the signal $x(t)$, we can apply the inverse Fourier transform
\begin{equation*}
x(t) = \mathcal{F}^{-1} [ \delta (f - f_0) ]
=\int_{\mathbb{R}} \delta (f - f_0) e^{2 \pi i f t} df
= e^{2 \pi i f_0 t}.
\end{equation*}
More generally, if $\hat{x}(f)$ is a linear combination of impulses equally spaced in frequency
\begin{equation} \label{equation:PeriodicFrequency}
\hat{x}(f) = \sum_{k = -\infty}^{\infty} \hat{s}_k \delta (f - k f_0) ,
\end{equation}
then its inverse Fourier transform becomes
\begin{equation} \label{equation:PeriodicTime}
x(t) = \sum_{k = -\infty}^{\infty} \hat{s}_k e^{2 \pi i k f_0 t} .
\end{equation}
Note that \eqref{equation:PeriodicTime} corresponds to the Fourier series representation of a periodic signal.
Thus, the Fourier transform of a periodic signal with Fourier series coefficients $\{ \hat{s}_k : k \in \mathbb{Z} \}$ can be interpreted as a train of impulses in the frequency domain.

A signal that will be useful in our analysis of sampling is the impulse train
\begin{equation*}
x(t) = \sum_{k = -\infty}^{\infty} \delta(t - kT) .
\end{equation*}
This is a special case of a periodic function, with period $T$.
We can therefore apply a methodology similar to the one derived above to compute its Fourier transform.
The Fourier series coefficients for the impulse train are obtained as
\begin{equation*}
\hat{s}_k = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}
x(t) e^{- 2 \pi i \frac{k}{T} t} dt
= \frac{1}{T} .
\end{equation*}
Using \eqref{equation:PeriodicFrequency}, we get
\begin{equation} \label{equation:ImpulseTrainFrequency}
\hat{x}(f)
= \frac{1}{T} \sum_{k = -\infty}^{\infty} \delta \left( f - \frac{k}{T} \right) .
\end{equation}
Surprisingly, an impulse train in the time domain can be regarded as an impulse train in the frequency domain.
A second representation for $x(t)$ is given by \eqref{equation:PeriodicTime},
\begin{equation} \label{equation:ImpulseTrainTime}
x(t) = \sum_{k = -\infty}^{\infty} \delta(t - kT)
= \frac{1}{T} \sum_{k = -\infty}^{\infty} e^{2 \pi i \frac{k}{T} t} .
\end{equation}
Which representation to use depends on the problem at hand.


\subsection{Spectral Density}
\label{subsection:SpectralDensity}

The energy content of a deterministic signal $x(t)$ is given by \eqref{equation:L2Condition}.
If the energy content of $x(t)$ is finite, i.e.\ $\| x(t) \|^2 < \infty$, then we can define its \emph{time autocorrelation function} by
\begin{equation*}
R_x(\tau) = \int_{\mathbb{R}} x(t)x^*(t - \tau) dt .
\end{equation*}
Using this notation, we can write the energy content of $x(t)$ as $R_x(0)$.

\begin{definition}
The \emph{energy spectral density} of $x(t)$, denoted by $\mathcal{G}_x (f)$, is the Fourier transform of its time autocorrelation function,
\begin{equation*}
\mathcal{G}_x(f) = \mathcal{F} [ R_x (\tau) ] = | \hat{x}(f) |^2.
\end{equation*}
\end{definition}

Intuitively, the energy spectral density captures the frequency content of a signal and helps identify how its energy is distributed across frequencies.
The \emph{spectral bandwidth} of Fourier transform $\hat{x}(f)$ is the smallest value of $W$ such that $\mathcal{G}_x(f) = 0$ for all $|f| > W$.
A signal $x(t)$ is \emph{bandwidth-limited} to $W$ if it can be obtained as the inverse Fourier tranform of a function $\hat{x}(f)$, where $\hat{x}(f)$ is identically zero for all $|f| > W$.


\subsection{Linear Time-Invariant Filters}
\label{subsection:LinearTimeInvariantFilters}

The importance of the Fourier transform comes, partly, from its ability to capture the effects of linear time-invariant filters on deterministic signals.
Suppose that the input to a linear time-invariant filter is $x(t)$, then its output is given by
\begin{equation*}
y(t) = x(t) \ast h(t),
\end{equation*}
where $h(t)$ is the impulse response of the linear filter and $\ast$ denotes the convolution operator.
If we use $\hat{h}(f)$ to represent the Fourier transform of impulse response $h(t)$, then the output signal in the frequency domain becomes
\begin{equation*}
\hat{y}(f) = \hat{x}(f) \hat{h}(f) .
\end{equation*}
That is, convolution in the time domain becomes multiplication in the frequency domain, a much simpler operation.
The output signal can then be recovered by taking the inverse Fourier transform of $\hat{y}(f)$,
\begin{equation*}
y(t) = \mathcal{F}^{-1} [ \hat{y}(f) ] = \mathcal{F}^{-1} [ \hat{x}(f) \hat{h}(f) ] .
\end{equation*}


\section{Sampling Deterministic Signals}

The sampling theorem is one of the most significant results in communications.
Many digital communication systems rely on the validity of this theorem and on the design insights it provides for proper operation.
The basic idea behind the sampling theorem can be summarized in a few words.
If a signal $x(t)$ is bandwidth-limited to $W$, then this signal can be reconstructed from a collection of samples so long as the samples are taken at periodic intervals of $T \leq \frac{1}{2W}$.
A formal version of the sampling theorem appears below.

\begin{theorem}[Sampling Theorem] \label{theorem:SamplingTheorem}
Let signal $x(t)$ be a bandwidth-limited function with bandwidth $W$.
If $x(t)$ is sampled at times $\{ nT : n \in \mathbb{Z} \}$ where $T \leq \frac{1}{2W}$, then it is possible to reconstruct the original signal $x(t)$ from its sampled points $\{ x(nT) : n \in \mathbb{Z} \}$.
Specifically, if $T \leq \frac{1}{2W}$ then
\begin{equation} \label{equation:SamplingReconstructionFormula}
x(t) = \sum_{n = -\infty}^{\infty}
x(nT) \mathrm{sinc} \left( \frac{t}{T}-n \right) .
\end{equation}
\end{theorem}
\begin{proof}
The signal $x(t)$ is bandwidth-limited with bandwidth $W$.
It follows that $x(t)$ is the inverse Fourier transform of a function $\hat{x}(f)$, where $\hat{x}(f) = 0$ for all frequencies such that $|f| > W$.
For convenience, we define $F = \frac{1}{T}$ and we stress that $W \leq \frac{1}{2T} = \frac{F}{2}$.
Thus, $\hat{x}(f) = 0$ whenever $|f| > \frac{F}{2}$.
We can apply the theory of Fourier series introduced in Section~\ref{section:FourierSeries} to express $\hat{x}(f)$ as
\begin{equation*}
\hat{x}(f) = \sum_{k=-\infty}^{\infty} s_k e^{2 \pi i \frac{k}{F} f}
\mathrm{rect} \left( \frac{f}{F} \right)
\end{equation*}
where the coefficients $\{ s_k : k \in \mathbb{Z} \}$ are equal to
\begin{equation*}
s_k
= \frac{1}{F} \int_{-\frac{F}{2}}^{\frac{F}{2}}
\hat{x}(f) e^{- 2 \pi i \frac{k}{F} f} df .
\end{equation*}
Special care should be taken when reading these equations because we are applying Fourier series analysis to a function in the frequency domain.
This can get confusing.

We can then write $x(t)$ in terms of basis elements,
\begin{equation*}
\begin{split}
x(t) &= \mathcal{F}^{-1} \left[ \hat{x}(f) \right]
= \mathcal{F}^{-1} \left[ \sum_{k=-\infty}^{\infty} s_k e^{2 \pi i \frac{k}{F} f}
\mathrm{rect} \left( \frac{f}{F} \right) \right] \\
&= \sum_{k=-\infty}^{\infty} s_k \mathcal{F}^{-1} \left[ e^{2 \pi i \frac{k}{F} f}
\mathrm{rect} \left( \frac{f}{F} \right) \right] \\
%&= \sum_{k=-\infty}^{\infty} s_k
%\int_{-\frac{F}{2}}^{\frac{F}{2}} e^{2 \pi i \frac{k}{F} f} e^{2 \pi i f t} df \\
%&= \sum_{k=-\infty}^{\infty} s_k
%\int_{-\frac{F}{2}}^{\frac{F}{2}} e^{2 \pi i \left( t + \frac{k}{F} \right) f} df \\
%&= \sum_{k=-\infty}^{\infty} s_k
%\frac{e^{\pi i \left( t + \frac{k}{F} \right) F} - e^{-\pi i \left( t + \frac{k}{F} \right) F}}{2 \pi i \left( t + \frac{k}{F} \right)} \\
%&= \sum_{k=-\infty}^{\infty} s_k
%\frac{\sin \left( \pi \left( t + \frac{k}{F} \right) F \right)}{\pi \left( t + \frac{k}{F} \right)} \\
%&= \sum_{k=-\infty}^{\infty} s_k F \mathrm{sinc} \left( F t + k \right) \\
&= \sum_{k=-\infty}^{\infty} \frac{s_k}{T} \mathrm{sinc} \left( \frac{t}{T} + k \right) .
\end{split}
\end{equation*}
Above, we have successively used the scaling and time-shift properties of the Fourier transform.
We can obtain the values of $\{ s_k : k \in \mathbb{Z} \}$ explicitly by exploiting the characteristics of the $\mathrm{sinc} (\cdot)$ function,
\begin{equation*}
x(nT)
= \sum_{k=-\infty}^{\infty}
\frac{s_k}{T} \mathrm{sinc} \left( \frac{nT}{T} + k \right)
= \sum_{k=-\infty}^{\infty}
\frac{s_k}{T} \mathrm{sinc} ( n + k ) = \frac{s_{-n}}{T} .
\end{equation*}
Thus, we have $s_{k} = T x(-kT)$ and formula \eqref{equation:SamplingReconstructionFormula} follows.
The sampling rate corresponding to $T = \frac{1}{2W}$ is the minimum rate at which perfect reconstruction is possible.
It is called the \emph{Nyquist rate}.
\end{proof}

We can gain better intuition about sampling using the Fourier transform representation for periodic signals developed in Section~\ref{section:PeriodicSignals}.
Let $x_{\mathrm{s}}(t)$ denote the result of sampling $x(t)$ by impulses at times $\{ nT : n \in \mathbb{Z} \}$,
\begin{equation*}
x_{\mathrm{s}}(t) = x(t) \sum_{n=-\infty}^{\infty} \delta(t-nT)
= \sum_{n=-\infty}^{\infty} x(nT) \delta(t-nT).
\end{equation*}
Looking at the sampled signal in the frequency domain, we get
\begin{equation*}\begin{split}
\hat{x}_{\mathrm{s}}(f) &= \hat{x}(f) \ast \mathcal{F} \left[ \sum_{n=-\infty}^{\infty} \delta(t-nT) \right] \\
&= \hat{x}(f) \ast \frac{1}{T} \sum_{n=-\infty}^{\infty} \delta \left( f-\frac{n}{T} \right) \\
&= \frac{1}{T} \sum_{n=-\infty}^{\infty} \hat{x} \left( f - \frac{n}{T} \right) ,\end{split}\end{equation*}
where we have used \eqref{equation:ImpulseTrainFrequency} to express the Fourier transform of an impulse train.
When the sampling rate is fast enough, the translated copies of $\hat{x}(f)$ contained in the transform $\hat{x}_{\mathrm{s}}(f)$ do not overlap, and the original signal can be recovered using an ideal lowpass filter.
However, when the sampling period~$T$ is too small, the various copies of $\hat{x}(f)$ overlap and the content of the original is partially destroyed.
This is know as \emph{aliasing}.

\begin{figure}[htbp]
\begin{center}
\epsfig{file=Figures/sampling,width=8cm}
\caption{The sampling and reconstruction of a bandwidth-limited signal.
When the sampling rate exceeds twice the bandwidth of the original signal, this signal can be reconstituted from its sampled values.}
\label{figure:Sampling}
\end{center}
\end{figure}
A succession of power spectral densities can be found on Figure~\ref{figure:Sampling}.
The top component shows the power spectral density of the original signal.
The density of the sampled signal appears below.
Finally, the reconstruction operation where a lowpass filter is employed to recovered the original function is illustrated at the bottom of the figure.
In contrast, Figure~\ref{figure:Aliasing} exhibits a case where the sampling frequency is too low.
\begin{figure}[htbp]
\begin{center}
\epsfig{file=Figures/aliasing,width=8cm}
\caption{A low sampling frequency leads to aliasing, thereby preventing reconstruction of the original signal.}
\label{figure:Aliasing}
\end{center}
\end{figure}
Aliasing in the frequency domain prevents the original signal from being retrieved.

The illusion of a moving image in video is achieved by displaying a rapid succession of still pictures over time.
Films are typically shot at a rate of twenty-four frames per second, whereas the minimum frame rate required to create the appearance of a moving image is about fifteen frames per second.
The human eye acts as a lowpass filter and transforms the succession of images into a live video.
High-speed cameras are used to record slow-motion playback movies.
As a consequence, they must run at much higher frame-rates than normal cameras.


\section{Stochastic Signals}
\label{section:StocahsticSignalsFAS}

A \emph{stochastic process} (or \emph{random process}) is an extension of the concept of \emph{random variable} to the situation where the values of a signal are not known beforehand.
Mathematically, a stochastic process can be viewed in two different ways.
First, the process can be thought of as an instantiation of a random experiment where the outcome is selected from a collection of time functions.
Alternatively, a stochastic process can be viewed as a collection of random variables indexed by time.
If the index set corresponds to the real numbers, then the process is a \emph{continuous-time random process}.
Whereas if the index set is discrete, then the random process is a \emph{discrete-time random process}.
The viewpoint where a stochastic process is regarded as a collection of random variables tends to prevail in the study of digital communications.
\begin{figure}[htbp]
\begin{center}
\begin{psfrags}
\psfrag{A}[c]{Amplitude}
\psfrag{t}[c]{Time}
\psfrag{t0}[c]{$t_0$}
\psfrag{t1}[c]{$t_1$}
\epsfig{file=Figures/process,width=7cm}
\end{psfrags}
\caption{Two distinct abstractions of a random process.
It can be viewed as the output of an experiment where a function is selected at random.
Alternatively, a random process may be taken as a set of random variables indexed by time.}
\label{figure:RandomProcess}
\end{center}
\end{figure}

Random processes are frequently employed in the design of communication systems.
For example, they can be used to model the data originating from a source, channel variations, noise and interference.
Their importance will become evident as we progress through these notes.
In general, it is difficult to provide a complete mathematical description for a random process.
For now, we restrict our attention to \emph{stationary} and \emph{ergodic} random processes.

\begin{definition}[Stationarity]
A random process $X(t)$ is wide-sense stationary if its \emph{mean}
\begin{equation*}
m_X(t) = \mathrm{E} [X(t)]
\end{equation*}
is independent of time, and its \emph{autocorrelation function} defined by
\begin{equation*}
R_X(t_1, t_2) = \mathrm{E} [X(t_1) X^*(t_2)]
\end{equation*}
only depends on the difference between $t_1$ and $t_2$.
With a slight abuse of notation, we can denote the mean and autocorrelation of a stationary process respectively by $m_X$ and $R_X(\tau)$, where $\tau = t_1 - t_2$.
\end{definition}

\begin{definition}[Ergodicity]
The \emph{ergodic theorems} assert that, under certain conditions, the time average of a function along all the possible trajectories of a random process exists and is equal to its ensemble average,
\begin{equation*}
\lim_{T \rightarrow \infty} \frac{1}{T} \int_{- \frac{T}{2}}^{\frac{T}{2}} g(X(t)) dt
= \mathrm{E}[g(X(t))] .
\end{equation*}
When a stochastic process fulfills these conditions, it is called \emph{ergodic}.
\end{definition}

\begin{figure}[htbp]
\begin{center}
\begin{psfrags}
\psfrag{A}[c]{Amplitude}
\psfrag{t}[c]{Time}
\psfrag{t0}[c]{$t_0$}
\psfrag{m}[c]{$m_X$}
\psfrag{e}[c]{$\lim_{T \rightarrow \infty} \frac{1}{T} \int_{- \frac{T}{2}}^{\frac{T}{2}} X(t) dt$}
\epsfig{file=Figures/ergodic,width=8cm}
\end{psfrags}
\caption{For ergodic processes, the time average of a function along a trajectory is equal to the ensemble average.}
\label{figure:ErgodicProcess}
\end{center}
\end{figure}

One of the important characteristics of an ergodic process is that it suffices to look at one realization of the process to infer many of its statistical attributes.
Ergodicity is a very strong property, and it is hard to test and validate.
Rather, it is frequently taken as a premise in the design of communication systems.
For instance, most information sources are assumed to be stationary and ergodic.
Such a postulate appears reasonable, especially given the many successful communication systems implemented to date.


\subsection{Power Spectral Density}

The \emph{power spectral density} of a stochastic signal is an extension to the spectral density discussed in Section~\ref{subsection:SpectralDensity}.
The definition of the power spectral density is somewhat more intricate due to the more complex nature of random signals.
In particular, it must account for uncertainty in the process.

Let $X(t)$ be a wide-sense stationary and ergodic random process, with $R_X(0) < \infty$.
The Fourier transform of a specific realization of $x(t)$ may not exist, as it need not fulfill condition \eqref{equation:L2Condition}.
However, a truncated version of $x(t)$ possesses a Fourier transform.
Consider the truncated version of $x(t)$ given by
\begin{equation*}
x_T(t) = x(t) \mathrm{rect} \left( \frac{t}{T} \right) ,
\end{equation*}
and its Fourier transform
\begin{equation*}
\hat{x}_T(f) = \mathcal{F} \left[ x(t) \mathrm{rect} \left( \frac{t}{T} \right) \right] .
\end{equation*}
The power spectral density of $X(t)$ represents the amount of power per hertz of bandwidth present in the signal at various frequencies, and it is defined by
\begin{equation*}
\mathcal{S}_X(f) = \lim_{T \rightarrow \infty} \frac{1}{T} \mathrm{E} \left[ |\hat{x}_T(f)|^2 \right] .
\end{equation*}
Note how the truncated signal is used to overcome the difficulty of dealing with infinite-energy signals.
This is a common and valuable trick.
As we will soon see, the power spectral density plays an instrumental role in the sampling theorem for random signals.
First, we provide a means to compute $\mathcal{S}_X(f)$ from its statistical attributes.

\begin{theorem}[Wiener-Khinchin]
The power spectral density $\mathcal{S}_X (f)$ of a wide-sense stationary random process $X(t)$ is equal to the Fourier transform of its autocorrelation function, $\mathcal{S}_X (f) = \mathcal{F} [R_X (\tau)]$.
\end{theorem}
\begin{proof}
For a wide-sense stationary process, we have
\begin{equation*}
\begin{split}
\mathcal{S}_X(f) &= \lim_{T \rightarrow \infty} \frac{1}{T} \mathrm{E} \left[ |\hat{x}_T(f)|^2 \right]
= \lim_{T \rightarrow \infty} \frac{1}{T} \mathrm{E} \left[ \hat{x}_T(f) \hat{x}_T^*(f) \right] \\
&= \lim_{T \rightarrow \infty} \frac{1}{T} \mathrm{E} \left[
\int_{-\frac{T}{2}}^{\frac{T}{2}} X(t_1) e^{-2 \pi i f t_1} dt_1
\int_{-\frac{T}{2}}^{\frac{T}{2}} X^*(t_2) e^{2 \pi i f t_2} dt_2 \right] \\
&= \lim_{T \rightarrow \infty} \frac{1}{T}
\int_{-\frac{T}{2}}^{\frac{T}{2}} \int_{-\frac{T}{2}}^{\frac{T}{2}}
\mathrm{E} \left[ X(t_1) X^*(t_2) \right]
e^{-2 \pi i f (t_1-t_2)} dt_1 dt_2 \\
&= \int_{\mathbb{R}} R_X (\tau) e^{-2 \pi i f\tau} d\tau
= \mathcal{F} [ R_X (\tau) ] .
\end{split}
\end{equation*}
The fourth equality is obtained by interchanging the expectation and the integrals, while the sixth equality follows from a change of variables and the fact that $X(t)$ is wide-sense stationary.
To guarantee that the former operation is legitimate, $\tau R_X(\tau)$ must remain finite for all $\tau$.
\end{proof}


\subsection{Filtering Stochastic Processes}

We discussed in Section~\ref{subsection:LinearTimeInvariantFilters} how the Fourier transform can simplify the analysis of the effects of linear time-invariant filters on deterministic signals.
In this section, we consider the operation of such filters in the context of random processes.

\begin{theorem}
If a wide-sense stationary process $X(t)$ with mean $m_X$ and autocorrelation function $R_X(\tau)$ is passed through a linear time-invariant filter with impulse response $h(t)$, then the output process $Y(t)$ has mean
\begin{equation*}
m_Y = m_X \int_{\mathbb{R}} h(t) dt
\end{equation*}
and its autocorrelation is equal to
\begin{equation*}
R_Y (\tau) = R_X(\tau) \ast h(\tau) \ast h^*(-\tau) .
\end{equation*}
\end{theorem}
\begin{proof}
The output process at time~$t$ is given by $Y(t) = \int_{\mathbb{R}} X(t - \xi) h(\xi) d\xi$.
We can therefore obtain the expectation of $Y(t)$ as follows,
\begin{equation*}
\begin{split}
m_Y (t) &= \mathrm{E} \left[ \int_{\mathbb{R}} X(t - \xi) h(\xi) d\xi \right]
= \int_{\mathbb{R}} \mathrm{E} \left[ X(t - \xi) \right] h(\xi) d\xi \\
&= m_X \int_{\mathbb{R}} h(\xi) d\xi.
\end{split}
\end{equation*}
We emphasize that $m_Y$ is independent of time.

To derive the autocorrelation function for $Y(t)$, we first compute the cross-correlation between $X(t)$ and $Y(t)$,
\begin{equation} \label{equation:CrossCorrelationLTI}
\begin{split}
\mathrm{E} [X(t_1) Y^*(t_2) ]
&= \mathrm{E} \left[ X(t_1) \int_{\mathbb{R}} X^*(\xi) h^*(t_2 - \xi) d\xi \right] \\
&= \int_{\mathbb{R}} \mathrm{E} \left[ X(t_1) X^*(\xi) \right] h^*(t_2 - \xi) d\xi \\
&= \int_{\mathbb{R}} R_X(t_1 - \xi) h^*(t_2 - \xi) d\xi \\
%&= \int_{\mathbb{R}} R_X(\tau - \xi) h^*(- \xi) d\xi \\
&= R_X(\tau) \ast h^*(-\tau) .
\end{split}
\end{equation}
This shows that the cross-correlation between $X(t)$ and $Y(t)$ depends only on $\tau$; we can therefore express it as $R_{XY}(\tau)$.
We are now ready to compute the autocorrelation function for $Y(t)$.
\begin{equation} \label{equation:AutoCorrelationLTI}
\begin{split}
\mathrm{E} [Y(t_1) Y^*(t_2) ]
&= \mathrm{E} \left[ \int_{\mathbb{R}} X(\xi) h(t_1 - \xi) d\xi Y^*(t_2) \right] \\
&= \int_{\mathbb{R}} \mathrm{E} \left[ X(\xi) Y^*(t_2) \right] h(t_1 - \xi) d\xi \\
&= \int_{\mathbb{R}} R_{XY}(\xi - t_2) h(t_1 - \xi) d\xi \\
%&= \int_{\mathbb{R}} R_{XY}(\xi) h(\tau - \xi) d\xi \\
&= R_{XY}(\tau) \ast h(\tau) .
\end{split}
\end{equation}
Substituting $R_{XY} (\tau)$ by the equivalent expression $R_X(\tau) \ast h^*(-\tau)$ from \eqref{equation:CrossCorrelationLTI}, we get the desired result.
We observe that the autocorrelation of the process $Y(t)$ only depends on the difference between $t_1$ and $t_2$, and hence $Y(t)$ is also wide-sense stationary.
\end{proof}

Obtaining an expression for the autocorrelation function corresponding to the output of a linear time-invariant filter allows us to characterize the power spectral density of the output process.
In terms of the frequency representation, we get $m_Y = m_X \hat{h}(0)$ and
\begin{equation*}
\begin{split}
\mathcal{S}_Y (f) &= \mathcal{F} [ R_Y (\tau) ] \\
&= \mathcal{F} \left[ R_X (\tau) \ast h(\tau) \ast h^*(-\tau) \right] \\
&= \mathcal{S}_X(f) | \hat{h}(f) |^2 .
\end{split}
\end{equation*}
A linear time-invariant filter can be employed to shape the spectrum of a stochastic process, and to constrain its bandwidth.
This is an important result, as linear filters can be used to reduce the bandwidth of a random signal before sampling or to reconstruct a random signal from its samples.


\section{Sampling Bandlimited Processes}

We know from Theorem~\ref{theorem:SamplingTheorem} that a bandwidth-limited signal can be perfectly reconstructed from its samples provided that the sampling rate exceeds twice the bandwidth of the original signal.
At this point, one may wonder whether it is possible to extend the sampling theorem to bandwidth-limited stochastic processes.
This question is answered in the affirmative below.

\begin{theorem} \label{theorem:SamplingRandomSignals}
Suppose that $X(t)$ is a wide-sense stationary bandwidth-limited process with bandwidth $W$ and power spectral density $\mathcal{S}_X (f)$.
Let $\tilde{X}(t)$ be an approximation for $X(t)$ built from the sampled values $\{ X(nT) : n \in \mathbb{Z} \}$,
\begin{equation*}
\tilde{X}(t) = \sum_{n=-\infty}^{\infty} X(nT) \mathrm{sinc} (2 W (t - nT)) ,
\end{equation*}
where $T = \frac{1}{2W}$ denotes the sampling interval.
Then the mean-squared error between the original random process and the reconstructed version vanishes,
\begin{equation} \label{equation:SamplingMSE}
\left\| X(t) - \tilde{X}(t) \right\|^2
= \mathrm{E} \left[ \left| X(t) - \sum_{n=-\infty}^{\infty}
X(nT) \mathrm{sinc} (2 W (t - nT)) \right|^2 \right] = 0 .
\end{equation}
The expectation in \eqref{equation:SamplingMSE} is over all possible realizations of $X(t)$.
\end{theorem}
\begin{proof}
To establish this result, we expand the mean-squared error of \eqref{equation:SamplingMSE},
\begin{equation*}
\begin{split}
&\left\| X(t) - \tilde{X}(t) \right\|^2
= \mathrm{E} \left[ \left| X(t) - \sum_{n=-\infty}^{\infty} X(nT)
\mathrm{sinc}(2 W (t - nT)) \right|^2 \right] \\
&= R_X(0) - \sum_{n=-\infty}^{\infty} [ R_X(t-nT) + R_X^*(t-nT) ]
\mathrm{sinc}(2 W (t - nT)) \\
&+ \sum_{n=-\infty}^{\infty} \sum_{m=-\infty}^{\infty} R_X((m-n)T)
\mathrm{sinc}(2 W (t - mT)) \mathrm{sinc}(2 W (t - nT)) .
\end{split}
\end{equation*}
The double summation above can be rewritten as
\begin{equation*}
\begin{split}
&\sum_{n=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} R_X(kT)
\mathrm{sinc}(2 W (t - kT - nT)) \mathrm{sinc}(2 W (t - nT)) \\
&\sum_{n=-\infty}^{\infty} \left( \sum_{k=-\infty}^{\infty} R_X(kT)
\mathrm{sinc}(2 W (t - kT - nT)) \right) \mathrm{sinc}(2 W (t - nT)) \\
&= \sum_{n=-\infty}^{\infty} R_X(t - nT) \mathrm{sinc}(2 W (t - nT)) ,
\end{split}
\end{equation*}
where the last equality follows from the sampling theorem for deterministic signals (Theorem~\ref{theorem:SamplingTheorem}).
Putting these results together, we get
\begin{equation*}
\left\| X(t) - \tilde{X}(t) \right\|^2
= R_X(0) - \sum_{n=-\infty}^{\infty} R_X^*(t-nT) \mathrm{sinc}(2 W (t - nT)) .
\end{equation*}
Applying Theorem~\ref{theorem:SamplingTheorem} one more time and noticing that $R_X(0) = R_X^*(0)$, we obtain $\| X(t) - \tilde{X}(t) \|^2 = 0$, as desired.
\end{proof}

Theorem~\ref{theorem:SamplingRandomSignals} is important because it confirms that the design insights gained from analyzing deterministic signals hold for random signals as well.


\section{Bandpass Signals and Processes}

One possible application of sampling is to take a continuous-time signal and to transform it into a discrete-time signal.
For instance, this operation gives the information coming out of a source a format more suitable for digital communications.
This prime application of sampling served as the original motivation for our study of the subject.
A second possible application of sampling is the processing of received waveforms at the output of communication channels.
In digital communications, the data often assumes the form of an analog carrier signal modulated by a digital bit stream.
Mathematically, this situation is captured by the equation
\begin{equation*}
y(t) = x(t) \cos (2 \pi f_{\mathrm{c}} t) .
\end{equation*}
The signal $y(t)$ is a special form of a \emph{bandpass signal}.
Its Fourier transform $\hat{y}(f)$ is non-zero only for frequencies contained in a small neighborhood of carrier frequency $f_{\mathrm{c}}$.
That is, $\hat{y}(f) = 0$ for all frequencies such that $|f - f_{\mathrm{c}}| \geq W$.
To apply the sampling tools derived above to the information bearing signal $x(t)$, we need to shift the corresponding spectrum to the origin.

The Fourier transform of $y(t)$ is given by
\begin{equation*}
\hat{y}(f) = \frac{1}{2} \hat{x}(f+f_{\mathrm{c}}) + \frac{1}{2} \hat{x}(f - f_{\mathrm{c}}) .
\end{equation*}
Our strategy is to first eliminate $\frac{1}{2} \hat{x}(f + f_{\mathrm{c}})$ from $\hat{y}(f)$, and then to scale and shift $\frac{1}{2} \hat{x}(f + f_{\mathrm{c}})$ back to the origin.
Define the \emph{step function} by
\begin{equation*}
\mathrm{step} (t) = \frac{1}{2} + \frac{1}{2} \mathrm{sign}(t). 
\end{equation*}
Taking the (cavalier) Fourier transform of $\mathrm{step}(t)$, we get
\begin{equation*}
\begin{split}
{\mathcal{F}} [\mathrm{step} (t)]
&= {\mathcal{F}} \left[ \frac{1}{2} + \frac{1}{2} \mathrm{sign}(t) \right] \\
&= \frac{1}{2} \delta (f)
- \frac{1}{2} \int_{-\infty}^0 e^{-2 \pi i ft} dt
+ \frac{1}{2} \int_0^{\infty} e^{-2 \pi i ft} dt \\
&= \frac{1}{2} \delta(f)  + \frac{1}{2 \pi i f}.
\end{split}
\end{equation*}
Using the duality property of the Fourier transform, we get
\begin{equation*}
\mathcal{F}^{-1} [\mathrm{step}(f)] = \frac{1}{2} \delta(t) + \frac{i}{2 \pi t} .
\end{equation*}
And, by construction, we obtain $\hat{x}(f - f_{\mathrm{c}}) = 2 \mathrm{step}(f) \hat{y}(f)$.
We can therefore recover the original lowpass signal $x(t)$ using the frequency-shift property of the Fourier transform,
\begin{equation*}
x(t)
= \left[ y(t) \ast \left( \delta (t) + \frac{i}{\pi t} \right) \right]
e^{- 2 \pi i f_{\mathrm{c}} t}
= \left[ y(t) + i \left( y(t) \ast \frac{1}{\pi t} \right) \right] e^{- 2 \pi i f_{\mathrm{c}} t} .
\end{equation*}
The second component of this signal,
\begin{equation*}
y(t) \ast \frac{1}{\pi t} ,
\end{equation*}
is called the \emph{Hilbert transform} of $y(t)$.
Once $x(t)$ is brought back to baseband, the standard sampling theorem applies and a discrete-time version of the signal can be produced.

% Bandpass processes
