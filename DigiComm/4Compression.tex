\chapter{Data Compression}

Having explored the process of turning an analog information signal into a digital sequence using sampling and quantization, we begin our treatment of digital information.
From here on, we assume that information sources are digital in nature.
That is, they produce a succession of symbols, each drawn from a \emph{discrete alphabet}.
The two main goals of the current chapter are to find an adequate measure of information for digital systems and to describe compression algorithms that can be employed to represent the said information in a succinct manner.
\emph{Data compression}, also known as \emph{source coding}, is important because it reduces the consumption of expensive resources such as hard disk space or transmission bandwidth.
Alternatively, it can be applied to lower the cost of communication, reduce latency or improve the quality of the received messages.

This document provides an introduction to \emph{lossless compression} algorithms, whereby the original message can be recovered perfectly from the compressed data.
This is in contrast with \emph{lossy data compression}, which can achieve better compression ratio at the expense of introducing some distortion in the message.
In the latter case, part of the information may be lost and the original data need not be recovered perfectly, although the reconstructed message may be quite close to the original one.
For instance, the JPEG algorithm can be employed as a lossy compression scheme to reduce the size of a digital photograph.
In lossless data compression, two strategies are employed to reduce the expected length of a message.
Highly probable symbols are assigned short descriptions and less likely symbols are encoded using longer binary representations.
Second, the statistical redundancy contained in the input signal over time is removed, leading to a more concise description of the digital data.
Data compression algorithms are explained more thoroughly below.

As we will see, finding a pertinent measure of information is essential in assessing the performance and limitations of compression algorithms.
While the general notion of information may be quite broad, it has a precise definition in the context of digital communication systems.
To describe this specific meaning, we need to develop a rigorous mathematical model for digital information sources.


\section{Discrete Memoryless Sources}

As mentioned above, a digital source produces a sequence of symbols drawn from a countable alphabet.
It can accordingly be modeled as a discrete-time random process.
Because of their indeterminate nature, random signals and stochastic processes can be quite hard to characterize.
In Section~\ref{section:StocahsticSignalsFAS}, we discussed two desirable attributes of random signals, namely stationary and ergodicity.
Yet, we purposely avoided giving an explicit definition for a random process.
A detailed treatment of the subject requires advanced concepts from probability theory, a topic that interested readers may wish to pursue on their own.
For the sake of simplicity, we focus on elementary information sources in this document.
These sources are collectively known as discrete memoryless sources.
They are easy to analyze and can be described unambiguously in a straightforward manner.
Furthermore, discrete memoryless sources provide valuable insights into the design of efficient compression algorithms.

\begin{definition}
A \emph{discrete memoryless source} is a digital information source that produces a sequence of independent and identically distributed symbols over time.
Mathematically, it consists of an alphabet $\mathcal{X}$ and a probability mass function $p_X(\cdot)$ such that, at any time~$t$, the probability that the source outputs symbol $x \in \mathcal{X}$ is equal to $p_X(x)$, irrespective of the past and future.
\end{definition}

For discrete memoryless sources, it suffices to define the probability mass function of individual symbols to completely characterize the statistical properties of the corresponding random signal.
The higher-order statistics need not be specified explicitly, as they can be obtained from
\begin{equation} \label{equation:JointDistributionMemorylessSource}
\Pr (X_{t_1} = x_{t_1}, \ldots, X_{t_n} = x_{t_n})
= p_X(x_{t_1})  \cdots p_X(x_{t_n})
\end{equation}
where $x_{t_1}, \ldots, x_{t_n} \in \mathcal{X}$.
In \eqref{equation:JointDistributionMemorylessSource}, the random variable $X_{t_i}$ denotes the output of the source at time~$t_i$.
We provide two examples of memoryless sources below to further illustrate their form.

\begin{example}[Binary Source] \label{example:BinarySource}
The simplest possible information source is a discrete memoryless source where $p_X(\cdot)$ is the probability mass function of a Bernoulli random variable,
\begin{equation*}
p_X(x) = \begin{cases} (1 - p), & x = 0 \\
p, & x = 1 \end{cases}
\end{equation*}
with $p \in [0,1]$.
This source can be employed, for instance, to model the successive flipping of a biased coin, where heads is obtained with probability $p$ and tails is obtained with probability $1 - p$.
\end{example}

\begin{example}
To construct a slightly more elaborate example, consider a collection of experiments where a fair coin is flipped repetitively until heads is observed.
The outcome of each experiment is reported as a source output.
The source alphabet in this case is $\mathcal{X} = \{1, 2, \ldots \}$, the positive integers, and the marginal probability mass function associated with individual outcomes becomes
\begin{equation*}
p_X (x) = \frac{1}{2^x}, \quad x = 1, 2, \ldots
\end{equation*}
Thus, the distribution of the source output at time~$t$ is a geometric random variable with parameter~$\frac{1}{2}$.
\end{example}

It is straightforward to show that all discrete memoryless sources are both stationary and ergodic.
The fact that their outcomes are independent over time makes them convenient for analysis, leading to simple interpretations.
However, it should also be pointed out that many realistic sources are more complicated than memoryless sources.
In particular, their outputs may be correlated over time, which can have a major impact on information rates.
Handling complex sources requires heavier mathematical machinery, and is beyond the scope of this document.
The results obtained with more involve sources are, nonetheless, similar in nature to the ones presented below.

Having constructed a suitable abstraction for digital sources, we turn to the subject of digital information.
From an intuitive point of view, the data rate of a discrete memoryless source should be equal to the amount of information it produces at every time instant.
In other words, the amount of information created by a discrete memoryless source at time~$t$ should be computable based on $\mathcal{X}$ and $p_X(\cdot)$ exclusively.
This is indeed the case.
Before we can make this statement precise, we need a precise mathematical characterization of information.
We address this issue by introducing entropy, a concept closely related to the notion of information.


\section{Entropy}

The \emph{entropy} can be viewed as a measure of uncertainty in a random variable.
In the context of digital communications, it serves as a measure of information in that it provides a lower bound on the expected number of bits required to describe the output of a discrete memoryless source.
This lower bound is tight and can be approached using different schemes, as we will see shortly.

\begin{definition}[Entropy]
Let $X$ be a discrete random variable drawn from alphabet $\mathcal{X}$ according to probability mass function $p_X(\cdot)$.
The entropy of $X$, denoted $H[X]$, is defined by
\begin{equation} \label{equation:Entropy}
H[X] = - \sum_{x \in \mathcal{X}} p_X (x) \log_2 ( p_X(x) ) .
\end{equation}
Under this definition, entropy is described in bits.
When writing $H[X]$, we use the convention
\begin{equation*}
0 \cdot \log_2 \left( \frac{1}{0} \right)
= \lim_{\epsilon \rightarrow 0} \epsilon \log_2 \left( \frac{1}{\epsilon} \right)
= 0 .
\end{equation*}
Alternatively, the entropy of $X$ can be interpreted as an expected value,
\begin{equation*}
H[X] = \mathrm{E} \left[ \log_2 \left( \frac{1}{p_X(X)} \right) \right] .
\end{equation*}
\end{definition}

The entropy as defined in \eqref{equation:Entropy} has interesting properties.
The value $H[X]$ does not depend on the actual symbols themselves, it only depends on the probability mass function of the possible outcomes.
For instance, in Example~\ref{example:EntropyFairCoin}, the entropy of $X$ remains the same whether we represent the flipping of a coin by a single bit or through a string of letters.
More generally, the way we choose to designate the possible outcomes of a random experiment has no impact over the entropy of the corresponding source, only the relative probabilities of the symbols matter.

\begin{example} \label{example:EntropyFairCoin}
Let $X$ be an abstract representation of the flipping of a (possibly biased) coin.
The probability mass function of $X$ is then equal to
\begin{equation*}
p_X(x) = \begin{cases} (1-p), & x = 0 \\
p, & x = 1 \end{cases}
\end{equation*}
with zero denoting tails and one for heads.
We can compute the entropy of $X$ as follows,
\begin{equation*}
H[X] = - (1-p) \log_2 (1-p)
- p \log_2 (p) .
\end{equation*}
If the coin is fair, $p = 1/2$, then the entropy of $X$ becomes one  bit.
Hence, the minimum expected number of bits needed to describe the outcome of a fair coin toss is one.
This seems quite reasonable.
\end{example}

The entropy of pair of two independent random variables is the sum of the individual entropy.
Suppose that $X$ is a vector random variable given by $X = (U, V)$, where $U$ and $V$ are independent.
Then, we can write
\begin{equation*}
p_X(x) = p_X((u, v)) = p_{U} (u) p_{V} (v)
\end{equation*}
and the entropy of $X$ is given by
\begin{equation*}
\begin{split}
H[X] &= - \sum_{ x \in \mathcal{X} } p_X(x) \log_2 ( p_X(x) ) \\
&= - \sum_{(u, v) \in \mathcal{U} \times \mathcal{V}}
p_X((u, v)) \log_2 ( p_X((u, v)) ) \\
&= - \sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{V}}
p_{U} (u) p_{V} (v) \log_2 ( p_{U} (u) p_{V} (v) ) \\
&= - \sum_{u \in \mathcal{U}}
p_{U} (u) \log_2 ( p_{U} (u) )
- \sum_{v \in \mathcal{V}}
p_{V} (v) \log_2 ( p_{V} (v) ) \\
&= H[U] + H[V] .
\end{split}
\end{equation*}
This corresponds to our intuitive understanding; the amount of information contained in two unrelated events should be the sum of the information contained in each individual event.

It is important to recognize that $H[X]$ is computed based on the probability mass function $p_X(\cdot)$, not a function of the random variable $X$ itself.
As such, $H[X]$ is a deterministic quantity and does not depend on the actual realization of $X$.
Furthermore, we note that $H[X]$ is continuous in the weights of the distribution $p_X(\cdot)$.
A small change in the distribution of $X$ only results in a small variation in its entropy.
It is therefore possible to construct accurate entropy estimates based on empirical measurements of the source outputs.


\section{Variable-Length Compression Codes}

A code is a rule for converting a symbol (or a group of symbols) into a string of bits called a codeword.
Mathematically, an encoder is a mapping $c : \mathcal{X} \mapsto \mathcal{C}$ from the input alphabet $\mathcal{X}$ to the collection of possible codewords $\mathcal{C}$.
The goal of a compression code is, of course, to provide a more concise representation of the information signal.
In lossless compression, the function $c$ must be injective over the support of $X$.
Without this one-to-one relationship, decoding errors are bound to happen.
Encoding schemes can be partitioned into two categories based on the structure of their codebooks.
If the codewords all share the same bit length, then the corresponding code is called a \emph{fixed-length code}.
This section focuses on codes in the second category, variable-length codes, which are often used in lossless data compression.

As the name suggests, a \emph{variable-length code} is an encoding function that maps source symbols to a variable number of bits.
This is a beneficial feature for many compression schemes as the greater flexibility sometimes leads to better compression ratio.
The motivation behind variable-length encoding is the intuition that data compression can be achieved by assigning short bit strings to likely symbols, and necessarily longer bit strings to less probable ones.
In dealing with variable-length codes, it is essential to recognize that they are inherently more tricky than fixed-length ones.
With variable-length coding, it may be impossible to know where codewords begin in a compressed binary file without looking at the content of the file.
This is in stark contrast with fixed-length codes where codewords are positioned at regular intervals.
To ensure that the binary output of a variable-length encoder can be recovered unambiguously, the code needs specific properties.

Variable-length codes can be nested in order of decreasing generality as non-singular, uniquely decodable and instantaneous.
A code is \emph{non-singular} if each source symbol is mapped to a different bit string.
That is, the mapping $c$ from $\mathcal{X}$ to $\mathcal{C}$ is one-to-one.
Rather, if two symbols map to the same codeword, then it is intuitively clear that the original message cannot be recovered unambiguously.
A code is said to be \emph{uniquely decodable} if its extensions are non-singular.
The extension of a code $c$ is obtained by concatenating its codewords when $c$ is applied to a multitude of symbols.
Given a string of source symbols $x_1, x_2, \ldots, x_n$, the extension of $c$ produces the output bit string
\begin{equation*}
c(x_1) c(x_2) \cdots c(x_n) .
\end{equation*}
An extension of $c$ is a proper encoding schemes because it takes a group of symbols as its argument and produces a string of bits as its output.

It is important to recognize that successive codewords in a message are communicated as a continuing sequence of bits.
There is no separation marker or frame between adjacent codewords, no commas or spaces.
The decoder, given a starting point, must infer the boundaries of every codeword from the data.
This process is called \emph{parsing}.
The third and final property of variable-length encoding is related to parsing.
A code is \emph{instantaneous}, or \emph{prefix-free}, if no codeword in $\mathcal{C}$ is a prefix of a any other encoded symbol in $\mathcal{C}$.
This property guarantees that received symbols can be decoded immediately after their entire codeword is received or read.

\begin{example}
Suppose that a source produces three possible symbols, $\mathcal{X} = \{ x_1, x_2, x_3 \}$.
We consider four encoding functions ($c_1, c_2, c_3, c_4$), each with different properties.
The encoding schemes are defined as follows.
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Symbol & \multicolumn{4}{c|}{Codeword} \\
\hline
$x$ & $c_1(x)$ & $c_2(x)$ & $c_3(x)$ & $c_4(x)$ \\
\hline
$x_1$ & 0 & 0 & 0 & 0 \\
$x_2$ & 1 & 1 & 01 & 10 \\
$x_3$ & 0 & 01 & 11 & 11 \\
\hline
\end{tabular}
\end{center}
The first scheme is not injective as it maps different source symbols to the same codeword, $c_1(x_1) = c_1(x_3)$.
Thus, individual codewords cannot be decoded with certainty.
The second code is one-to-one; however, it is not uniquely decodable.
The encoded message 01 can be generated by either input string $x_1 x_2$ or input symbol $x_3$.
Clearly, the compressed message cannot be decoded unambiguously.
The third code, $c_3(\cdot)$ is uniquely decodable, but not instantaneous.
After receiving a zero, it not immediately clear whether $x_1$ produced this output or if this zero consists of the first half of codeword $c(x_2)$.
While $c_4(\cdot)$ is a prefix code where every symbol can be decoded immediately after reading the corresponding bits.
\end{example}

The measure of a good prefix code is the expected length of its encoded symbols.
Suppose that a discrete memoryless source $(\mathcal{X}, p_X)$ is given along with a code $c$.
We denote the length in bits of codeword $c(x)$ by $l_c(x)$.
The expected number of bits produced by the source at each time instant is given by
\begin{equation} \label{equation:CodeRateCompression}
\mathrm{E} [ l_c (X) ] = \sum_{x \in \mathcal{X}} p_X(x) l_c(x) .
\end{equation}
We emphasize that the expected length is a function of both the statistics of the source and the structure of compression code employed.
Under the assumption that the source outcomes are independent and identically distributed over time, $\mathrm{E} [ l_c (X)]$ also represents the average data rate produced by the source.


\subsection{Kraft Inequality}

When building a compression code, it is obvious from \eqref{equation:CodeRateCompression} that assigning short codewords is better than long codewords.
Yet, it is clear that we cannot describe every symbol using a very small number of bits, for otherwise the prefix condition will be violated.
The collection of possible length assignments for a prefix-free code is characterized by the following inequality.

\begin{theorem}[Kraft Inequality]
Let $\mathcal{X}$ be a finite alphabet.
Any binary prefix-free code $c : \mathcal{X} \mapsto \mathcal{C}$ satisfies the inequality
\begin{equation} \label{equation:KraftInequality}
\sum_{x \in \mathcal{X}} 2^{-l_c(x)} \leq 1.
\end{equation}
where $l_c(x)$ is the bit length of codeword $c(x)$.
Conversely, if we first assign the codeword lengths such that \eqref{equation:KraftInequality} is satisfied, then there exists an instantaneous code with these codeword lengths.
\end{theorem}
\begin{proof}
We wish to give necessary and sufficient conditions about the existence of a prefix-free code with a specific length assignment.
We employ simple combinatorial arguments to a binary tree structure to establish this result.
Let $L_c = \max_{x \in \mathcal{X}} \ell_c (x)$ be the length of the longest codeword.
Code $c: \mathcal{X} \mapsto \mathcal{C}$ can be defined using a binary tree of depth $L_c$, where branches from every node correspond either to zero or one.
Each codeword consists of a unique path from the root to a leaf at depth $\ell_c(x)$, following its binary string expansion.
The prefix condition ensures that no codeword is a descendant of any other codeword in the binary tree.
For the codewords in the tree, let $S_x$ be the set of descendants that $c(x)$ would have in a full binary tree of depth $L_c$.
The sets $S_x$ are disjoint because of the prefix-free nature of the code, and $|S_x| = 2^{L_c - \ell_c(x)}$.
Since the total number of nodes at depth $L_c$ is $2^{L_c}$, we have
\begin{equation*}
\left| \bigcup_{x \in \mathcal{X}} S_x \right|
= \sum_{x \in \mathcal{X}} |S_x|
= \sum_{x \in \mathcal{X}} 2^{L_c - \ell_c(x)}
\leq 2^{L_c} .
\end{equation*}
By dividing both sides by $2^{L_c}$, we conclude that \eqref{equation:KraftInequality} holds.
That is, a binary prefix-free code $c$ over finite alphabet $\mathcal{X}$ satisfies the Kraft inequality.

Conversely, suppose that we have a code assignment such that \eqref{equation:KraftInequality} is satisfied.
Without loss of generality, we assume that the codeword lengths $\ell_c(x_i)$ are increasing in~$i$,
\begin{equation*}
\ell_c(x_1) \leq \ell_c(x_2) \leq \ell_c(x_3) \leq \cdots
\end{equation*}
We can construct a prefix code with matching codeword lengths by pruning subtrees from a  full binary tree of depth $L_c$.
First, choose any node from the full tree at depth $\ell_1$ and remove all of its descendants.
This removes $2^{L_c -\ell_1}$ leafs from the original binary tree.
Next, select any available node from the resulting tree at depth $\ell_2$, and remove all of its descendants.
This time, an additional $2^{L_c - \ell_2}$ leafs are taken away from the original binary tree.
Continue this procedure with the other codeword lengths.
After $m$ iterations, the total number of leafs removed from the original binary tree is equal to
\begin{equation*}
\sum_{i=1}^m 2^{L_c - \ell_i} = 2^{L_c} \sum_{i=1}^m 2^{-\ell_i} .
\end{equation*}
Since the Kraft inequality holds for the codeword length assignment, this implies that all the codewords can be placed at different positions on the binary graph.
Then, following the binary structure of the graph, the binary string of the codes can be inferred from the graph.
\end{proof}

\begin{example}[Code on a Tree]
Suppose that we intend to construct a prefix code for $\mathcal{X} = \{ x_1, \ldots, x_5 \}$, with code lengths
\begin{xalignat*}{3}
l_c(x_1) &= l_c(x_2) = 2 & l_c(x_3) &= l_c(x_4) = 3 & l_c(x_5) &= 2.
\end{xalignat*}
First, we check the Kraft inequality to make sure that such an assignment is feasible,
\begin{equation*}
\sum_{i = 1}^5 2^{-l_c(x_1)}
= \frac{1}{4} + \frac{1}{4} + \frac{1}{8} + \frac{1}{8} + \frac{1}{4} = 1 .
\end{equation*}
The inequality is fulfilled, we can therefore use a binary tree construction to design the desired instantaneous code.
The process is illustrated in Figure~\ref{figure:BinaryTreeCode}, and the resulting code is shown below.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Source Symbol & Codeword & Source Symbol & Codeword \\
\hline
$x_1$ & 00 & $x_4$ & 101 \\
$x_2$ & 01 & $x_5$ & 11 \\
$x_3$ & 100 & & \\
\hline
\end{tabular}
\end{center}
Since the Kraft inequality is met with equality, we know that it is impossible to get a better code by shortening one of the codewords.
\begin{figure}[htbp]
\begin{center}
\begin{psfrags}
\psfrag{0}[c]{$0$}
\psfrag{1}[c]{$1$}
\psfrag{00}[l]{$00$}
\psfrag{01}[l]{$01$}
\psfrag{100}[l]{$100$}
\psfrag{101}[l]{$101$}
\psfrag{11}[l]{$11$}
\epsfig{file=Figures/binarytreecode,width=7cm}
\end{psfrags}
\caption{Construction of a prefix code with a binary tree.}
\label{figure:BinaryTreeCode}
\end{center}
\end{figure}
\end{example}


\subsection{Entropy Bounds on Prefix-Free Codes}

Now that we know how to build instantaneous, we consider the problem of finding good prefix-free codes.
Recall from \eqref{equation:CodeRateCompression} that our objective is to find a code that with the smallest possible expected length.
The codeword length assignment is subject to the Kraft inequality.
Putting these two observations together, we can formulate this optimization problem as follows
\begin{equation*}
\min_{\ell(x)} \sum_{x \in \mathcal{X}} p_X(x) \ell(x)
\end{equation*}
subject to
\begin{equation*}
\sum_{x \mathcal{X}} 2^{- \ell(x)} \leq 1 .
\end{equation*}
Note that, for a code to exist, the function $\ell(x)$ must map symbols to positive integers.
It turns out that this problem is mathematically difficult.

To gain insight into the problem, we relax the integer constrain on $\ell(x)$.
This added flexibility will provide a lower bound on $\mathrm{E} [\ell(x)]$; having more choices can only lead to better results.
We use the method of Lagrange multipliers to solve the latter version of the problem.
The objective function, with Lagrange multiplier $\lambda$, becomes
\begin{equation*}
\sum_{x \in \mathcal{X}} p_X(x) \ell(x)
+ \lambda \left( \sum_{x \mathcal{X}} 2^{- \ell(x)} - 1 \right) .
\end{equation*}
Note that this function is twice differentiable in $\ell(x)$.
Taking the derivative with respect to $\ell(x)$ and setting it to zero, we get
\begin{equation*}
p_X(x) - \lambda \ln(2) 2^{- \ell(x)}  = 0 .
\end{equation*}
Thus, $2^{- \ell^*(x)} = p_X(x) / (\lambda \ln (2))$.
Taking the partial derivative with respect to $\lambda$ yields $\sum_{x \mathcal{X}} 2^{- \ell^*(x)} = 1$, which in turn implies $\lambda = 1 / \ln (2)$.
Putting these results together, we get that the optimal values are
\begin{equation*}
\ell^*(x) = - \log_2 (p_X(x))
\end{equation*}
and, by construction, we get
\begin{equation*}
\mathrm{E} [\ell_c(x)] \geq - \sum_{x \in \mathcal{X}} p_X(x) \log_2 (p_X(x))
= H[X]
\end{equation*}
for any prefix-code~$c$.
We have just shown that the entropy is a lower bound on the expected length of a prefix-free code.

It is equally easy to obtain an upper bound.
Observe that $\lceil - \log_2 (p_X(x)) \rceil$ is an integer, with
\begin{equation*}
- \log_2 (p_X(x))
\leq \lceil - \log_2 (p_X(x)) \rceil
\leq - \log_2 (p_X(x)) + 1 .
\end{equation*}
The Kraft inequality asserts that we can build a code $c : \mathcal{X} \mapsto \mathcal{C}$ such that $\ell_c(x) = \lceil - \log_2 (p_X(x)) \rceil$, because
\begin{equation*}
\sum_{x \in \mathcal{X}} 2^{\ell_c(x)}
\leq \sum_{x \in \mathcal{X}} 2^{\ell^*(x)} = 1 .
\end{equation*}
Thus, we gather that there exists a code~$c$ such that
\begin{equation*}
\mathrm{E} [\ell_c(x)] \leq H[X] + 1 .
\end{equation*}
We formalize these results in the form of a theorem.

\begin{theorem} \label{theorem:EntropyBoundsPrefixCodes}
Consider a discrete memoryless source $(\mathcal{X}, p_X(\cdot))$ over a finite alphabet.
If symbols are encoded individually using an optimal prefix-free code $c : \mathcal{X} \mapsto \mathcal{C}$, then the expected length of the codewords satisfies
\begin{equation*}
H[X] \leq \mathrm{E} [\ell_c(x)] \leq H[X] + 1 .
\end{equation*}
\end{theorem}


\subsection{Huffman Code}

Theorem~\ref{theorem:EntropyBoundsPrefixCodes} discusses properties of an optimal prefix-code.
However, it does not provide an algorithmic methodology to construct such a code.
This is addressed by \emph{Huffman coding}, a variable-length encoding algorithm used for lossless data compression.
Not too surprisingly, the underlying strategy in this scheme is to assign short strings of bits to likely symbols, and necessarily longer ones to less probable source outputs.
The encoding is specifically crafted so that the code table forms a prefix-free code.
Huffman coding is the most efficient compression mapping for individual source symbols.
The expected length of the compressed data achieved with this technique will be no greater than the expected message length of any other prefix-free code that operates on individual source symbols.



Although Huffman coding is optimal for a symbol-by-symbol encoding with a known input probability mass function, it can be outperformed when these two conditions are not known.
For instance, if the input distribution $p_x(\cdot)$ is not known, then it must be inferred from the available data prior to applying Huffman coding.
Small errors in the estimated probability mass function can then lead to inefficiency, which in turn renders Huffman coding suboptimal.
More importantly, a more efficient way to encode data is to consider blocks of source symbols and to encode them jointly.
Although more complicated, this process leads to better performance and typically leads to expected message lengths that are lower than that of a typical symbol-by-symbol Huffman code.


\section{Fixed-Length Compression Codes}

Such codes are easy to encode and decode, yielding unambiguous messages.
The minimum number of binary strings in lossless fixed-length encoding is $\left\lceil \log_2 ( | \mathcal{X} | ) \right\rceil$, where $| \mathcal{X} |$ is the size of the source alphabet and $\lceil \cdot \rceil$ is the ceiling function, which returns the smallest integer greater than or equal to its argument.
This type of codes cannot be used to compress data by assigning short descriptions to most frequent symbols and longer descriptions to the less likely ones.
Data compression in fixed-length coding methods is only possible for large blocks of data, and any compression beyond the logarithm of the total number of possibilities comes with a finite, though perhaps small, probability of decoding failure.


\section{Lempel-Ziv Algorithm}

The entropy is equal to the minimum number of bits per source symbol required to map the source output into binary digits in such a way that the source symbols may be reconstructed from the encoded sequence.
